{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homepage","text":""},{"location":"#documentation-for-ekeel-apps","title":"Documentation for <code>EKEEL</code> apps","text":"<p>For deep overview of the system look here or click -&gt;  <code>Show System Overview</code> </p> <p></p> <p>For annotation protocol specifications look here or click -&gt;  <code>Show Annotation Protocol</code> </p> <p></p>"},{"location":"#annotator-tool","title":"<code>Annotator Tool</code>","text":"<p>The Annotator Tool is a web application designed to facilitate the annotation of videos. It involves several key actors: A Vanilla Javascript frontend, a Python Flask backend, a MongoDB database, and a python Whisper Transcriber service running on the same machine in the backend. The tool allows users to interact with the frontend to perform various tasks such as opening the website, registering, annotating videos, comparing annotations and make automatic annotations. The transcriber service periodically processes untranscribed videos and updates the database. </p>"},{"location":"#sequence-diagrams","title":"<code>Sequence Diagrams</code>","text":"<p>Follows sequence diagrams of typical function calls:   <code>Hide Sequence Diagrams</code> </p>"},{"location":"#homepage-access","title":"<code>Homepage Access</code>","text":"<p>Interaction between a user, the frontend, and the backend when a user opens a website. The user sends a request to the frontend to open the website. The frontend then sends a GET request to the backend to retrieve the main page data. The backend processes this request by calling a function in the Flask application and then sends the response back to the frontend. The frontend processes the response and renders the main page for the user.</p> <p></p>"},{"location":"#registration-process","title":"<code>Registration Process</code>","text":"<p>The main actors are a user, the frontend (JS-Frontend), the backend (Flask-Backend), and the database (MongoDB). The process starts with the user clicking the \"Sign up\" button, prompting the frontend to send a GET request to the backend to load the registration page. The backend processes this request and the frontend renders the registration form. The user fills out the form and submits it, causing the frontend to send a POST request to the backend. The backend inserts the user's data into an unverified users collection in MongoDB and, upon receiving a response from MongoDB, sends a response back to the frontend, which renders a confirmation code page. The user then submits the confirmation code, and the frontend sends another POST request to the backend. If the code is correct, the backend deletes the user from the unverified users collection and inserts them into the verified users collection in MongoDB, then sends a response back to the frontend, which renders a success message. If the code is incorrect, the backend updates the unverified user's data in MongoDB and sends a response back to the frontend, which displays an error message. This sequence outlines the interactions between the user, frontend, backend, and database during the registration process.</p> <p></p>"},{"location":"#video-insertion","title":"<code>Video Insertion</code>","text":"<p>The main actors are: A user, the frontend (JS-Frontend), the backend (Flask-Backend), the database (MongoDB), and the transcriber service. The process begins with the user clicking the \"Manual Annotator\" button, which triggers a series of interactions. The frontend sends a GET request to the backend to load the video selection page, while the transcriber service periodically wakes up every 60 seconds to query the database for untranscribed videos. The backend processes the request and queries the database to retrieve video data, then sends the response back to the frontend, which renders the video selection page. The user then inserts a URL, prompting the frontend to send a POST request to the backend to add the video. The backend downloads the video, performs automatic transcription, and extracts terms and thumbnails, then inserts the video data into the database. The transcriber service transcribes the video and updates the database with the transcription data. Concurrently, the backend creates an interactable transcript in HTML, queries the database for additional data such as concept maps, definitions, annotation status, and vocabulary, and adds concepts and relations to the payload. Finally, the backend sends the response back to the frontend, which renders the MOOC annotator page. This sequence outlines the interactions between the user, frontend, backend, database, and transcriber service during the video annotation process.</p> <p></p>"},{"location":"#performing-annotation","title":"<code>Performing Annotation</code>","text":"<p>The process begins with the user logging in and opening the Manual Annotator, which prompts the backend to send the video selection page to the frontend for rendering. The user then clicks on a video to annotate it, causing the frontend to send a POST request to the backend. The backend processes this request by initializing the VideoAnalyzer and querying MongoDB for video data. After receiving the data, the backend creates an interactable transcript in HTML and retrieves additional information such as concept maps, definitions, annotation status, and vocabulary from MongoDB. This data is then added to the payload and sent back to the frontend, which renders the MOOC annotator page. During the annotation process, whenever the user adds a description to a concept and clicks \"Add Description,\" the frontend updates the local description and network scripts, then sends a POST request to the backend to upload the annotated graph. The backend processes this request by inserting the graph into MongoDB and responds to the frontend with the success or failure status, which the frontend handles accordingly. This sequence outlines the interactions between the user, frontend, backend, and database during the video annotation process.</p> <p></p>"},{"location":"#homepage-view","title":"Homepage View","text":"<p>The homepage view of the Annotator application provides an overview of the available features and functionalities. It serves as the entry point for users to navigate to different sections of the application.</p> <p></p>"},{"location":"#video-selection-view","title":"Video Selection View","text":"<p>The video selection view allows users to browse and select videos for annotation. This view displays a list of available videos, along with relevant metadata such as titles, descriptions, and thumbnails. Users can choose a video to start the annotation process.</p> <p></p>"},{"location":"#annotator-view","title":"Annotator View","text":"<p>The annotator view is where users perform the actual annotation of the selected video. This view provides tools and controls for annotating the video, such as adding concepts and synonyms, adding and editing descriptions, bounding box of concepts, relations, downloading the Graph in JSON format and the transcript enriched with the annotations. Users can play, pause, and navigate through the video while making annotations.</p> <p></p>"},{"location":"#slides-extraction","title":"<code>Slides Extraction</code>","text":"<p>The full document can be found here or click -&gt;  <code>Show Slide Segmentation Protocol</code> </p> <p></p> <p>Or it's summarized here:</p> <p>The system analyzes videos to identify and segment slide-based content using machine learning and OpenCV. Initially, the video undergoes a coarse analysis to determine if it contains a significant percentage of slides. A pre-trained model classifies images as \"slidish,\" and this classification is validated using OpenCV.</p> <p>If the video meets the \"slide threshold,\" further analysis is conducted. The segmentation process involves extracting keyframes based on color histograms and analyzing text to determine slide titles and content. Titles are identified through statistical analysis of text height and position, and concepts are extracted using phrasemachine. Each segment is compacted by merging similar or overlapping text and validated through a double-checking mechanism.</p> <p>The platform uses Python and relies on several core classes. The ImageClassifier handles face and text detection, as well as color conversions. The LocalVideo class manages video loading, frame extraction, and resizing using OpenCV. The VideoSpeedManager adds logic for efficient frame extraction. The TimedAndFramedText dataclass stores text, bounding box positions, and video frame ranges for slide segments. The VideoAnalyzer processes video transcription, keyframe segmentation, and slide classification, identifying slides and validating segments.</p> <p>The system includes a process scheduler that automatically segments videos, saving results to a database. This enables the reconstruction of slides and timeframes for further analysis or playback. Concepts and definitions are heuristically extracted based on the appearance of terms in transcripts and slide durations. This comprehensive process ensures accurate segmentation and analysis of educational videos containing slides.</p> <p>Info</p> <p>In the current implementation, slides extraction has been disabled to avoid overloading the server and should be reimplemented using novel NLP models like LLaVA</p>"},{"location":"#augmentator-tool","title":"<code>Augmentator Tool</code>","text":"<p>The Augmentator Tool is a specialized web application designed to enhance educational videos through automatic concept detection and knowledge graph generation.  It analyzes video content, extracts key concepts and their relationships, and presents them in an interactive visual format. The tool allows educators and learners to navigate complex educational content through temporal concept maps, making it easier to understand concept dependencies and progression throughout the video.</p> <p>The tool supports multiple languages and integrates with YouTube platform for research purposes, making it versatile for different educational settings and content types. Its primary goal is to transform linear video content into an interactive learning experience that highlights conceptual understanding and knowledge structure.</p> <p>The system processes both video transcripts and visual elements to identify important educational concepts, their temporal occurrences, and their interconnections. </p> <p>Users can interact with the generated knowledge graphs, explore concept definitions at specific timestamps, and visualize how different concepts relate to each other within the educational context. </p> <p>Note</p> <p>Currently not working for deprecation issues</p>"},{"location":"codebase/CODE_INDEX/","title":"CODE INDEX","text":"<ul> <li>EKEELVideoAnnotation<ul> <li>burst<ul> <li>extractor</li> <li>kleinberg</li> <li>prototype</li> <li>results_processor</li> <li>weight</li> </ul> </li> <li>config</li> <li>connector</li> <li>database<ul> <li>mongo</li> </ul> </li> <li>embedding<ul> <li>cluster</li> </ul> </li> <li>env</li> <li>forms<ul> <li>form</li> <li>mail</li> <li>user</li> </ul> </li> <li>main</li> <li>media<ul> <li>audio</li> <li>image</li> <li>segmentation</li> <li>video</li> </ul> </li> <li>metrics<ul> <li>agreement</li> <li>analysis</li> <li>metrics</li> </ul> </li> <li>misc_unused<ul> <li>Method_01</li> <li>Method_02</li> <li>Method_03</li> <li>Method_04</li> <li>skos_synonyms_query</li> <li>summary</li> <li>wikipe</li> </ul> </li> <li>models<ul> <li>xgboost_adapter</li> </ul> </li> <li>ontology<ul> <li>rdf_graph</li> </ul> </li> <li>services<ul> <li>NLP_API</li> </ul> </li> <li>text_processor<ul> <li>conll</li> <li>locales</li> <li>synonyms</li> <li>words</li> </ul> </li> <li>transcribe</li> <li>utils<ul> <li>itertools</li> <li>structures</li> </ul> </li> </ul> </li> <li>EKEELVideoAugmentation<ul> <li>src<ul> <li>flask-server<ul> <li>data</li> <li>environment</li> <li>handle_data</li> <li>main</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"codebase/EKEELVideoAnnotation/config/","title":"config","text":""},{"location":"codebase/EKEELVideoAnnotation/config/#config","title":"Config","text":""},{"location":"codebase/EKEELVideoAnnotation/config/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.config--flask-application-configuration","title":"Flask Application Configuration","text":"<p>Configures Flask application with reverse proxy support and authentication settings.</p> <p>Attributes:</p> Name Type Description <code>app</code> <code>Flask</code> <p>Main Flask application instance</p> <code>login</code> <code>LoginManager</code> <p>Flask-Login manager instance</p> Warning <p>Should not be moved or links to the htmls may break</p>"},{"location":"codebase/EKEELVideoAnnotation/config/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.config.ReverseProxied","title":"<code>ReverseProxied</code>","text":"<p>WSGI middleware for handling reverse proxy headers and URL rewriting.</p> <p>This class modifies the WSGI environment to support running the application behind a reverse proxy, handling script names, URL schemes, and server names.</p> <p>Attributes:</p> Name Type Description <code>app</code> <code>Flask</code> <p>The Flask application instance to wrap</p> <code>script_name</code> <code>str or None</code> <p>Optional URL prefix for the application</p> <code>scheme</code> <code>str or None</code> <p>Optional URL scheme (http/https)</p> <code>server</code> <code>str or None</code> <p>Optional server name</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Process the WSGI environment and handle reverse proxy headers</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/config.py</code> <pre><code>class ReverseProxied:\n    \"\"\"\n    WSGI middleware for handling reverse proxy headers and URL rewriting.\n\n    This class modifies the WSGI environment to support running the application\n    behind a reverse proxy, handling script names, URL schemes, and server names.\n\n    Attributes\n    ----------\n    app : flask.Flask\n        The Flask application instance to wrap\n    script_name : str or None\n        Optional URL prefix for the application\n    scheme : str or None\n        Optional URL scheme (http/https)\n    server : str or None\n        Optional server name\n\n    Methods\n    -------\n    __call__(environ, start_response)\n        Process the WSGI environment and handle reverse proxy headers\n    \"\"\"\n\n    def __init__(self, app, script_name=None, scheme=None, server=None):\n        \"\"\"\n        Initialize the reverse proxy middleware.\n\n        Parameters\n        ----------\n        app : flask.Flask\n            Flask application instance\n        script_name : str, optional\n            URL prefix for the application\n        scheme : str, optional\n            URL scheme (http/https)\n        server : str, optional\n            Server name\n        \"\"\"\n        self.app = app\n        self.script_name = script_name\n        self.scheme = scheme\n        self.server = server\n\n    def __call__(self, environ, start_response):\n        \"\"\"\n        Process WSGI environment for reverse proxy support.\n\n        Parameters\n        ----------\n        environ : dict\n            WSGI environment dictionary\n        start_response : callable\n            WSGI start_response callable\n\n        Returns\n        -------\n        callable\n            Result of calling the wrapped WSGI application\n        \"\"\"\n        script_name = environ.get('HTTP_X_SCRIPT_NAME', '') or self.script_name\n        if script_name:\n            environ['SCRIPT_NAME'] = script_name\n            path_info = environ['PATH_INFO']\n            if path_info.startswith(script_name):\n                environ['PATH_INFO'] = path_info[len(script_name):]\n        scheme = environ.get('HTTP_X_SCHEME', '') or self.scheme\n        if scheme:\n            environ['wsgi.url_scheme'] = scheme\n        server = environ.get('HTTP_X_FORWARDED_SERVER', '') or self.server\n        if server:\n            environ['HTTP_HOST'] = server\n        return self.app(environ, start_response)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/config/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.config.ReverseProxied.__call__","title":"<code>__call__(environ, start_response)</code>","text":"<p>Process WSGI environment for reverse proxy support.</p> <p>Parameters:</p> Name Type Description Default <code>environ</code> <code>dict</code> <p>WSGI environment dictionary</p> required <code>start_response</code> <code>callable</code> <p>WSGI start_response callable</p> required <p>Returns:</p> Type Description <code>callable</code> <p>Result of calling the wrapped WSGI application</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/config.py</code> <pre><code>def __call__(self, environ, start_response):\n    \"\"\"\n    Process WSGI environment for reverse proxy support.\n\n    Parameters\n    ----------\n    environ : dict\n        WSGI environment dictionary\n    start_response : callable\n        WSGI start_response callable\n\n    Returns\n    -------\n    callable\n        Result of calling the wrapped WSGI application\n    \"\"\"\n    script_name = environ.get('HTTP_X_SCRIPT_NAME', '') or self.script_name\n    if script_name:\n        environ['SCRIPT_NAME'] = script_name\n        path_info = environ['PATH_INFO']\n        if path_info.startswith(script_name):\n            environ['PATH_INFO'] = path_info[len(script_name):]\n    scheme = environ.get('HTTP_X_SCHEME', '') or self.scheme\n    if scheme:\n        environ['wsgi.url_scheme'] = scheme\n    server = environ.get('HTTP_X_FORWARDED_SERVER', '') or self.server\n    if server:\n        environ['HTTP_HOST'] = server\n    return self.app(environ, start_response)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/config/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.config.ReverseProxied.__init__","title":"<code>__init__(app, script_name=None, scheme=None, server=None)</code>","text":"<p>Initialize the reverse proxy middleware.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Flask</code> <p>Flask application instance</p> required <code>script_name</code> <code>str</code> <p>URL prefix for the application</p> <code>None</code> <code>scheme</code> <code>str</code> <p>URL scheme (http/https)</p> <code>None</code> <code>server</code> <code>str</code> <p>Server name</p> <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/config.py</code> <pre><code>def __init__(self, app, script_name=None, scheme=None, server=None):\n    \"\"\"\n    Initialize the reverse proxy middleware.\n\n    Parameters\n    ----------\n    app : flask.Flask\n        Flask application instance\n    script_name : str, optional\n        URL prefix for the application\n    scheme : str, optional\n        URL scheme (http/https)\n    server : str, optional\n        Server name\n    \"\"\"\n    self.app = app\n    self.script_name = script_name\n    self.scheme = scheme\n    self.server = server\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/connector/","title":"connector","text":""},{"location":"codebase/EKEELVideoAnnotation/connector/#connector","title":"Connector","text":""},{"location":"codebase/EKEELVideoAnnotation/connector/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.connector--ekeel-video-annotation-connector","title":"EKEEL Video Annotation Connector","text":"<p>This module serves as the entry point for gunicorn server deployment. It configures system paths for gunicorn.</p>"},{"location":"codebase/EKEELVideoAnnotation/env/","title":"env","text":""},{"location":"codebase/EKEELVideoAnnotation/env/#env","title":"Env","text":"<p>Environment configuration module.</p> <p>Loads sensitive configuration variables from secrets.env file for MongoDB, email and security settings.</p> <p>Attributes:</p> Name Type Description <code>MONGO_CLUSTER_USERNAME</code> <code>str</code> <p>Username for MongoDB cluster access</p> <code>MONGO_CLUSTER_PASSWORD</code> <code>str</code> <p>Password for MongoDB cluster access</p> <code>EMAIL_ACCOUNT</code> <code>str</code> <p>Email account for sending notifications</p> <code>EMAIL_PASSWORD</code> <code>str</code> <p>Password for email account</p> <code>APP_SECRET_KEY</code> <code>str</code> <p>Secret key for application security</p> <code>APP_SECURITY_PASSWORD_SALT</code> <code>str</code> <p>Salt value for password hashing</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If secrets.env file is missing in EVA_apps/sharedSecrets directory</p>"},{"location":"codebase/EKEELVideoAnnotation/main/","title":"main","text":""},{"location":"codebase/EKEELVideoAnnotation/main/#main","title":"Main","text":"<p>EKEEL Video Annotation Server Backend</p> <p>This module implements a Flask web server for video annotation and analysis. It provides functionality for:</p> <ul> <li> <p>User Management:</p> <ul> <li>Registration and authentication</li> <li>Profile management</li> </ul> </li> <li> <p>Video Processing:</p> <ul> <li>YouTube video processing</li> <li>Transcript extraction</li> <li>Video segmentation</li> </ul> </li> <li> <p>Annotation Features:</p> <ul> <li>Manual concept mapping</li> <li>Definition creation</li> <li>Vocabulary management</li> </ul> </li> <li> <p>Analysis Tools:</p> <ul> <li>Burst analysis (automatic/semi-automatic)</li> <li>Inter-annotator agreement</li> <li>Gold standard creation</li> <li>Linguistic analysis</li> </ul> </li> <li> <p>Data Management:</p> <ul> <li>MongoDB storage integration</li> <li>JSON-LD export format</li> <li>SKOS vocabulary support</li> </ul> </li> </ul>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.analysis","title":"<code>analysis()</code>","text":"<p>Handle various types of annotation analysis requests.</p> <p>This endpoint processes different types of analysis on video annotations: - Data summary: Statistical analysis of concept maps and definitions - Agreement: Compare annotations between two annotators - Linguistic: Analyze linguistic properties of annotations - Fleiss: Calculate inter-annotator agreement using Fleiss' kappa</p> <p>Parameters:</p> Name Type Description Default <code>analysis_type</code> <code>str</code> <p>Type of analysis to perform: - 'data_summary': Statistical summary - 'agreement': Inter-annotator comparison - 'linguistic': Linguistic analysis - 'fleiss': Fleiss' kappa calculation</p> required <code>video</code> <code>str</code> <p>Video identifier for analysis</p> required <code>annotator</code> <code>str</code> <p>Annotator ID for data_summary and linguistic analysis</p> required <code>annotator1</code> <code>str</code> <p>First annotator ID for agreement analysis</p> required <code>annotator2</code> <code>str</code> <p>Second annotator ID for agreement analysis</p> required <p>Returns:</p> Type Description <code>str</code> <p>Rendered HTML template with analysis results: - On GET: analysis_selection.html with video choices - On POST: analysis_results.html with computed results</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/analysis', methods=['GET', 'POST'])\n@login_required\ndef analysis():\n    \"\"\"\n    Handle various types of annotation analysis requests.\n\n    This endpoint processes different types of analysis on video annotations:\n    - Data summary: Statistical analysis of concept maps and definitions\n    - Agreement: Compare annotations between two annotators\n    - Linguistic: Analyze linguistic properties of annotations\n    - Fleiss: Calculate inter-annotator agreement using Fleiss' kappa\n\n    Parameters\n    ----------\n    analysis_type : str\n        Type of analysis to perform:\n        - 'data_summary': Statistical summary\n        - 'agreement': Inter-annotator comparison\n        - 'linguistic': Linguistic analysis\n        - 'fleiss': Fleiss' kappa calculation\n    video : str\n        Video identifier for analysis\n    annotator : str, optional\n        Annotator ID for data_summary and linguistic analysis\n    annotator1 : str, optional\n        First annotator ID for agreement analysis\n    annotator2 : str, optional\n        Second annotator ID for agreement analysis\n\n    Returns\n    -------\n    str\n        Rendered HTML template with analysis results:\n        - On GET: analysis_selection.html with video choices\n        - On POST: analysis_results.html with computed results\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::analysis() ******\")\n\n    video_choices = mongo.get_graphs_info()\n\n    if request.method == 'POST':\n        analysis_type = request.form[\"analysis_type\"]\n\n        if analysis_type == \"data_summary\":\n            video_id = request.form[\"video\"]\n            annotator_id = request.form[\"annotator\"]\n\n            concept_map = mongo.get_concept_map(annotator_id, video_id)\n            definitions = mongo.get_definitions(annotator_id, video_id)\n\n            results = compute_data_summary(video_id,concept_map, definitions)\n\n            if annotator_id != \"Burst_Analysis\":\n                user = mongo.get_user(annotator_id)\n                annotator = user[\"name\"] + \" \" + user[\"surname\"]\n\n            else:\n                annotator = \"Burst\"\n\n            return render_template('analysis_results.html', results=results, annotator=annotator, title=video_choices[video_id][\"title\"])\n\n        elif analysis_type == \"agreement\":\n            video_id = request.form[\"video\"]\n            annotator1_id = request.form[\"annotator1\"]\n            annotator2_id = request.form[\"annotator2\"]\n\n            concept_map1 = mongo.get_concept_map(annotator1_id, video_id)\n            concept_map2 = mongo.get_concept_map(annotator2_id, video_id)\n\n            results = compute_agreement(concept_map1, concept_map2)\n\n            if annotator1_id != \"Burst_Analysis\":\n                u1 = mongo.get_user(annotator1_id)\n                results[\"annotator1\"] = u1[\"name\"] + \" \" + u1[\"surname\"]\n            else:\n                results[\"annotator1\"] = \"Burst\"\n\n            if annotator2_id != \"Burst_Analysis\":\n                u2 = mongo.get_user(annotator2_id)\n                results[\"annotator2\"] = u2[\"name\"] + \" \" + u2[\"surname\"]\n            else:\n                results[\"annotator2\"] = \"Burst\"\n\n\n            return render_template('analysis_results.html', results=results, title=video_choices[video_id][\"title\"])\n\n        elif analysis_type == \"linguistic\":\n            video_id = request.form[\"video\"]\n            annotator_id = request.form[\"annotator\"]\n\n            results = linguistic_analysis(annotator_id, video_id)\n\n            return render_template('analysis_results.html', results=results, title=video_choices[video_id][\"title\"])\n\n\n        elif analysis_type == \"fleiss\":\n            video_id = request.form[\"video\"]\n\n            results = fleiss(video_id)\n\n            return render_template('analysis_results.html', results=results, analysis_type=analysis_type,\n                                   title=video_choices[video_id][\"title\"])\n\n    videos = []\n    for vid_id in video_choices.keys():\n        video = video_choices[vid_id]\n        video[\"video_id\"] = vid_id\n        videos.append(video)\n\n    return render_template('analysis_selection.html',  videos=videos) #form=form,\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.burst","title":"<code>burst()</code>","text":"<p>Process burst analysis requests for video annotations.</p> <p>This endpoint handles burst analysis of videos, which can be either: - Semi-automatic: Extracts transcript and processes text for annotations - Automatic: Extracts keywords only</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>YouTube video ID from form</p> required <code>type</code> <code>str</code> <p>Analysis type from form: - 'semi': Semi-automatic analysis with transcript - 'auto': Automatic analysis with keywords only</p> required <p>Returns:</p> Type Description <code>str</code> <p>On GET:      Rendered 'burst.html' template with:     - form : BurstForm         Form for burst analysis parameters     - videos : list         Available videos for analysis On POST:     Rendered 'burst_results.html' template with:     - result : list         Processed subtitles (semi-automatic only)     - video_id : str         YouTube video identifier     - language : str         Detected video language     - concepts : list         Extracted keywords     - title : str         Video title     - lemmatized_subtitles : list         Processed subtitle text (semi-automatic only)     - type : str         Analysis type performed</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/burst', methods=['GET', 'POST'])\n@login_required\ndef burst():\n    \"\"\"\n    Process burst analysis requests for video annotations.\n\n    This endpoint handles burst analysis of videos, which can be either:\n    - Semi-automatic: Extracts transcript and processes text for annotations\n    - Automatic: Extracts keywords only\n\n    Parameters\n    ----------\n    url : str\n        YouTube video ID from form\n    type : str\n        Analysis type from form:\n        - 'semi': Semi-automatic analysis with transcript\n        - 'auto': Automatic analysis with keywords only\n\n    Returns\n    -------\n    str\n        On GET: \n            Rendered 'burst.html' template with:\n            - form : BurstForm\n                Form for burst analysis parameters\n            - videos : list\n                Available videos for analysis\n        On POST:\n            Rendered 'burst_results.html' template with:\n            - result : list\n                Processed subtitles (semi-automatic only)\n            - video_id : str\n                YouTube video identifier\n            - language : str\n                Detected video language\n            - concepts : list\n                Extracted keywords\n            - title : str\n                Video title\n            - lemmatized_subtitles : list\n                Processed subtitle text (semi-automatic only)\n            - type : str\n                Analysis type performed\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::burst() ******\")\n\n    #form = addVideoForm()\n    form = BurstForm()\n    videos = mongo.get_videos([\"video_id\",\"title\", \"creator\"])\n\n    if form.validate_on_submit():\n\n        video_id = form.url.data\n        video = VideoAnalyzer(f\"https://youtu.be/{video_id}\",{\"language\",\"transcript_data\"})\n        #text = SemanticText(get_text(video_id), video.identify_language())      \n        #conll_sentences = conll_gen(video_id, text)\n        title, keywords = get_real_keywords(video_id,annotator_id = current_user.mongodb_id)\n\n        # semi-automatic extraction\n        if form.type.data == \"semi\":\n\n            video.request_transcript()\n            subtitles = video.data[\"transcript_data\"][\"text\"]\n            #if video.data[\"transcript_data\"][\"is_whisper_transcribed\"]:\n            #all_lemmas = set(video._get_words_lemma().values())\n            lemmatized_subtitles = html_interactable_transcript_word_level(subtitles)\n            #else:\n            #    lemmatized_subtitles, all_lemmas = html_interactable_transcript_legacy(subtitles,video.data[\"language\"], concepts=keywords)\n\n            return render_template('burst_results.html', result=subtitles, video_id=video_id, language=video.data[\"language\"], concepts=keywords,\n                                   title=title, lemmatized_subtitles=lemmatized_subtitles, type=\"semi\")\n\n        return render_template('burst_results.html', result=[], video_id=video_id,language=video.data[\"language\"], concepts=keywords, title=title,\n                                lemmatized_subtitles=[], type=form.type.data)\n\n    return render_template('burst.html', form=form, videos=videos)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.burst_launch","title":"<code>burst_launch()</code>","text":"<p>Process and store burst analysis results with optional synonym expansion.</p> <p>This endpoint handles the processing of burst analysis results, storing them in the database, and computing comparison metrics with existing annotations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>JSON request data containing: - id : str     Video identifier - concepts : list     List of extracted concepts - conceptVocabulary : dict     Mapping of concepts to synonyms - syn_burst : bool     Whether to include synonym expansion - burst_type : str     Type of burst analysis ('auto' or 'semi')</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON response containing: - concepts : list     Processed concept list - concept_map : dict     Generated concept relationships - definitions : dict     Concept definitions - data_summary : dict     Statistical summary of results - downloadable_jsonld_graph : dict     JSON-LD formatted graph data - agreement : dict     Comparison metrics including:     - name : str         Annotator name     - K : float         Agreement score     - VEO : float         Vector embedding overlap     - GED : float         Graph edit distance     - pageRank : float         PageRank similarity     - LO : float         Learning outcome score     - PN : float         Prerequisite network score - can_be_refined : bool     Whether video supports refinement</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/burst_launch', methods=[\"GET\", \"POST\"])\ndef burst_launch():\n    \"\"\"\n    Process and store burst analysis results with optional synonym expansion.\n\n    This endpoint handles the processing of burst analysis results, storing them\n    in the database, and computing comparison metrics with existing annotations.\n\n    Parameters\n    ----------\n    data : dict\n        JSON request data containing:\n        - id : str\n            Video identifier\n        - concepts : list\n            List of extracted concepts\n        - conceptVocabulary : dict\n            Mapping of concepts to synonyms\n        - syn_burst : bool\n            Whether to include synonym expansion\n        - burst_type : str\n            Type of burst analysis ('auto' or 'semi')\n\n    Returns\n    -------\n    dict\n        JSON response containing:\n        - concepts : list\n            Processed concept list\n        - concept_map : dict\n            Generated concept relationships\n        - definitions : dict\n            Concept definitions\n        - data_summary : dict\n            Statistical summary of results\n        - downloadable_jsonld_graph : dict\n            JSON-LD formatted graph data\n        - agreement : dict\n            Comparison metrics including:\n            - name : str\n                Annotator name\n            - K : float\n                Agreement score\n            - VEO : float\n                Vector embedding overlap\n            - GED : float\n                Graph edit distance\n            - pageRank : float\n                PageRank similarity\n            - LO : float\n                Learning outcome score\n            - PN : float\n                Prerequisite network score\n        - can_be_refined : bool\n            Whether video supports refinement\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: main.py::burst_launch() ******\")\n    data = request.json\n\n    video_id = data[\"id\"]\n    concepts = data[\"concepts\"]\n    concept_vocabulary = data[\"conceptVocabulary\"]\n    syn_burst = data[\"syn_burst\"]\n    burst_type = data[\"burst_type\"]    \n\n    # select burst type\n    if syn_burst:\n        print(\"Starting Burst \" + burst_type + \" with synonyms\")\n        concept_map,definitions = burst_extraction_with_synonyms(video_id, concepts, concept_vocabulary)\n    else:\n        print(\"Starting Burst \" + burst_type)\n        concept_map,definitions = burst_extraction(video_id,concepts)\n    if burst_type == \"semi\":\n        user = current_user.complete_name.replace(\" \",\"_\")+\"_Burst_Analysis\"\n        name = current_user.complete_name\n        email = current_user.email\n    else:\n        user = \"Burst_Analysis\"\n        name = user\n        email = user\n    burst_graph = mongo.get_graph(user,video_id)\n\n    # saving burst_graph on db if not already present\n    if burst_graph is None:\n        print(\"Saving Burst Graph on DB...\")\n        _,burst_graph = create_burst_graph(video_id,definitions,concept_map)\n        local_vocabulary = create_local_vocabulary(video_id,concept_vocabulary)\n        skos_concepts = local_vocabulary[\"skos:member\"]\n        downloadable_jsonld_graph = {\"@context\":burst_graph[\"@context\"],\"@graph\":burst_graph[\"@graph\"].copy()+[local_vocabulary]}\n        burst_graph[\"@graph\"].extend([{\"id\":concept[\"id\"],\"type\":concept[\"type\"]} for concept in skos_concepts])\n        mongo.insert_graph({ \"video_id\":video_id,\n                                \"annotator_id\":user,\n                                \"annotator_name\":name,\n                                \"email\":email,\n                                \"graph\": burst_graph,\n                                \"conceptVocabulary\": {\"@context\": burst_graph[\"@context\"], \n                                                      \"@graph\": skos_concepts}})\n    else:\n        graph = sorted(burst_graph[\"@graph\"],key=lambda x: int(x[\"id\"][3:]) if str(x[\"id\"][3:]).isnumeric() else 1042)\n        for i,node in reversed(list(enumerate(graph))):\n            if not str(node[\"id\"]).startswith(\"concept_\"):\n                break\n            else:\n                graph.pop(i)\n        downloadable_jsonld_graph = {\"@context\":burst_graph[\"@context\"],\"@graph\":graph+[create_local_vocabulary(video_id,concept_vocabulary)]}\n\n    data_summary = compute_data_summary(video_id,concept_map,definitions)\n\n    # checks whether video has been segmented and if it is classifies ad slide video or not in order to enable refinement\n    video = VideoAnalyzer(\"https://www.youtube.com/watch?v=\"+video_id,{\"video_data\"})\n    can_be_refined = video.is_slide_video() and \"slide_titles\" in video.data[\"video_data\"].keys()\n\n    json = {\n        \"concepts\": concepts,\n        \"concept_map\": concept_map,\n        \"definitions\": definitions,\n        \"data_summary\": data_summary,\n        \"downloadable_jsonld_graph\": downloadable_jsonld_graph,\n        \"agreement\": None,\n        \"can_be_refined\": can_be_refined\n    }\n\n    graphs = mongo.get_graphs_info(video_id)\n    if graphs is not None:\n        #first_annotator = graphs[\"annotators\"][0]['id']\n        #concept_map_annotator = db_mongo.get_concept_map(first_annotator, video_id)\n\n        annotators = graphs[\"annotators\"]\n        # [NOTE] used me as annotator instead of annotators[0] for testing keywords\n        my_id = current_user.mongodb_id\n        indx_annotator = 0\n        for i,annot in enumerate(annotators):\n            if annot['id']==my_id:\n                indx_annotator = i\n                break\n        indx_annotator = 0\n        annotator = graphs[\"annotators\"][indx_annotator]['id']\n        concept_map_annotator = mongo.get_concept_map(annotator, video_id)\n\n        veo, pageRank, LO, PN, ged_sim = calculate_metrics(concept_map, concept_map_annotator, concepts)\n\n        json[\"agreement\"] = {\n            \"name\":graphs[\"annotators\"][indx_annotator][\"name\"].replace(\"_\",\" \"),\n            \"K\": compute_agreement(concept_map, concept_map_annotator)[\"agreement\"],\n            \"VEO\": veo,\n            \"GED\": ged_sim,\n            \"pageRank\": round(pageRank, 3),\n            \"LO\": round(LO, 3),\n            \"PN\": round(PN, 3)\n        }\n\n    return json\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.confirm_code","title":"<code>confirm_code()</code>","text":"<p>Handle the confirmation code process for new users.</p> <p>Returns:</p> Type Description <code>str</code> <p>The rendered HTML content of the confirmation code page if the form is not submitted/valid.</p> <code>Response</code> <p>A redirect response to the login page if the confirmation code is valid.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/confirm_code', methods=['POST', 'GET'])\ndef confirm_code():\n    \"\"\"\n    Handle the confirmation code process for new users.\n\n    Returns\n    -------\n    str\n        The rendered HTML content of the confirmation code page if the form is not submitted/valid.\n    werkzeug.wrappers.Response\n        A redirect response to the login page if the confirmation code is valid.\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::confirm_code() ******\")\n\n    form = ConfirmCodeForm()\n    email = json.loads(request.args['mail'])\n\n    if form.validate_on_submit():\n\n        code = form.code.data\n\n        user = unverified_users.find_one({\"email\": email})\n\n        if bcrypt.checkpw(code.encode('utf-8'), user[\"code_on_creation_hash\"].encode('utf-8')):\n\n            new_user = {\n                'name': user[\"name\"],\n                'surname': user[\"surname\"],\n                'email': user[\"email\"],\n                'password_hash': user[\"password_hash\"],\n                'video_history_list': []\n            }\n            # users.insert_one(new_user)\n            unverified_users.delete_one({\"email\": email})\n            users.insert_one(new_user)\n\n            flash('Thanks! Email confirmed, you can now log in', 'success')\n\n        else:\n            tries = user[\"nb_try_code_on_creation\"] + 1\n            new = {\"$set\": {\"nb_try_code_on_creation\": tries}}\n            unverified_users.update_one({\"email\": email}, new)\n\n\n    return render_template('user/confirm_code.html', form=form)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.confirm_email","title":"<code>confirm_email(token)</code>","text":"<p>Handle the email confirmation process for new users.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The confirmation token sent to the user's email.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The rendered HTML content of the index page after confirming the email.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/confirm/&lt;token&gt;')\ndef confirm_email(token):\n    \"\"\"\n    Handle the email confirmation process for new users.\n\n    Parameters\n    ----------\n    token : str\n        The confirmation token sent to the user's email.\n\n    Returns\n    -------\n    str\n        The rendered HTML content of the index page after confirming the email.\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::confirm_email() ******\")\n\n    try:\n        print(token)\n        email = confirm_token(token)\n\n        u = unverified_users.find_one({\"email\": email})\n        new_user = {\n            'name': u[\"name\"],\n            'surname': u[\"surname\"],\n            'email': u[\"email\"],\n            'password_hash': u[\"password_hash\"],\n            'video_history_list': []\n        }\n        # users.insert_one(new_user)\n        unverified_users.delete_one({\"email\": email})\n        users.insert_one(new_user)\n\n        us = User(email)\n        login_user(us)\n\n        flash('Thanks! Email confirmed', 'success')\n    except:\n        flash('The confirmation link is invalid or has expired.', 'danger')\n\n    return render_template('index.html')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.delete_annotation","title":"<code>delete_annotation()</code>","text":"<p>Delete user-specific annotations for a video.</p> <p>This endpoint removes all annotation data associated with a specific video and user combination from the database.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Unique identifier of the video whose annotations should be deleted</p> required <code>user</code> <code>dict</code> <p>Dictionary containing user information: - id : str     MongoDB user identifier - name : str      User's full name</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON response containing: - done : bool     True if deletion successful, False if error occurred</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route(\"/delete_annotation\", methods=[\"GET\",\"POST\"])\ndef delete_annotation():\n    \"\"\"\n    Delete user-specific annotations for a video.\n\n    This endpoint removes all annotation data associated with a specific video\n    and user combination from the database.\n\n    Parameters\n    ----------\n    video_id : str\n        Unique identifier of the video whose annotations should be deleted\n    user : dict\n        Dictionary containing user information:\n        - id : str\n            MongoDB user identifier\n        - name : str \n            User's full name\n\n    Returns\n    -------\n    dict\n        JSON response containing:\n        - done : bool\n            True if deletion successful, False if error occurred\n    \"\"\"\n    video_id = request.json[\"video_id\"]\n    user = {\"id\": current_user.mongodb_id,\n            \"name\": current_user.complete_name}\n    mongo.remove_annotations_data(video_id, user)\n\n    return {\"done\":True}\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.delete_video","title":"<code>delete_video()</code>","text":"<p>Delete a video and its associated data from the database.</p> <p>This endpoint handles video deletion requests. It removes the video entry and all associated annotations from MongoDB storage.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Unique identifier of the video to delete, passed in request JSON</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON response containing: - done : bool     True if deletion successful, False if error occurred</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/delete_video', methods=[\"GET\", \"POST\"])\ndef delete_video():\n    \"\"\"\n    Delete a video and its associated data from the database.\n\n    This endpoint handles video deletion requests. It removes the video entry\n    and all associated annotations from MongoDB storage.\n\n    Parameters\n    ----------\n    video_id : str\n        Unique identifier of the video to delete, passed in request JSON\n\n    Returns\n    -------\n    dict\n        JSON response containing:\n        - done : bool\n            True if deletion successful, False if error occurred\n    \"\"\"\n    video_id = request.json[\"video_id\"]\n    mongo.remove_video(video_id)\n    return {\"done\":True}\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.forgot_password","title":"<code>forgot_password()</code>","text":"<p>Handle the forgot password process for users.</p> <p>Returns:</p> Type Description <code>str</code> <p>The rendered HTML content of the forgot password page if the form is not submitted/valid.</p> <code>Response</code> <p>A redirect response to the password reset page if the form is submitted and valid.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/forgot_password', methods=['POST', 'GET'])\ndef forgot_password():\n    \"\"\"\n    Handle the forgot password process for users.\n\n    Returns\n    -------\n    str\n        The rendered HTML content of the forgot password page if the form is not submitted/valid.\n    werkzeug.wrappers.Response\n        A redirect response to the password reset page if the form is submitted and valid.\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::forgot_password() ******\")\n\n    form = ForgotForm()\n\n    if form.validate_on_submit():\n        token = generate_confirmation_token(form.email.data)\n        reset_url = url_for('password_reset', token=token, _external=True)\n        html = render_template('user/user_forgot_password_mail.html', reset_url=reset_url)\n        subject = \"Password reset\"\n\n        send_mail(form.email.data, subject, html)\n        flash('Email sent to ' + form.email.data, 'success')\n\n    return render_template('user/forgot_password.html', form=form)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.get_concept_vocabulary","title":"<code>get_concept_vocabulary()</code>","text":"<p>Retrieve concept vocabulary and their synonyms.</p> <p>This endpoint handles requests to get synonyms for a list of concepts using NLTK Wordnet. Accepts POST requests with JSON data containing concepts and returns a vocabulary mapping.</p> <p>Parameters:</p> Name Type Description Default <code>concepts</code> <code>list</code> <p>List of concept strings to find synonyms for, passed in request JSON</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON response containing: - conceptVocabulary : dict     Mapping of concepts to their synonyms from Wordnet</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/get_concept_vocabulary', methods=[\"GET\", \"POST\"])\ndef get_concept_vocabulary():\n    \"\"\"\n    Retrieve concept vocabulary and their synonyms.\n\n    This endpoint handles requests to get synonyms for a list of concepts using NLTK Wordnet.\n    Accepts POST requests with JSON data containing concepts and returns a vocabulary mapping.\n\n    Parameters\n    ----------\n    concepts : list\n        List of concept strings to find synonyms for, passed in request JSON\n\n    Returns\n    -------\n    dict\n        JSON response containing:\n        - conceptVocabulary : dict\n            Mapping of concepts to their synonyms from Wordnet\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: main.py::get_concept_vocabulary() ******\")\n\n    data = request.json\n\n    # Getting concepts:\n    concepts = data[\"concepts\"]\n    # Finding synonyms with NLTK Wordnet:\n    conceptVocabulary = get_synonyms_from_list(concepts)\n\n    json = {\n        #\"concepts\": concepts,\n        \"conceptVocabulary\": conceptVocabulary\n    }\n\n    return json\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.gold_standard","title":"<code>gold_standard()</code>","text":"<p>Create gold standard annotations from multiple annotators' work.</p> <p>This endpoint handles the creation of consensus annotations by combining the work of multiple annotators based on agreement thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>str</code> <p>Selected video identifier from form</p> required <code>annotators</code> <code>list</code> <p>List of selected annotator IDs from form</p> required <code>agreements</code> <code>float</code> <p>Agreement threshold for including annotations in gold standard</p> required <code>name</code> <code>str</code> <p>Name for the gold standard annotation set</p> required <p>Returns:</p> Type Description <code>str</code> <p>Rendered HTML template 'gold_standard.html' containing: - video_choices : dict     Available videos and their metadata - form : GoldStandardForm     Form for gold standard creation parameters</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/gold_standard', methods=['GET', 'POST'])\n@login_required\ndef gold_standard():\n    \"\"\"\n    Create gold standard annotations from multiple annotators' work.\n\n    This endpoint handles the creation of consensus annotations by combining\n    the work of multiple annotators based on agreement thresholds.\n\n    Parameters\n    ----------\n    video : str\n        Selected video identifier from form\n    annotators : list\n        List of selected annotator IDs from form\n    agreements : float\n        Agreement threshold for including annotations in gold standard\n    name : str\n        Name for the gold standard annotation set\n\n    Returns\n    -------\n    str\n        Rendered HTML template 'gold_standard.html' containing:\n        - video_choices : dict\n            Available videos and their metadata\n        - form : GoldStandardForm\n            Form for gold standard creation parameters\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::gold_standard() ******\")\n\n    form = GoldStandardForm()\n\n    video_choices = mongo.get_graphs_info()\n    form.video.choices = [(c, video_choices[c][\"title\"]) for c in video_choices]\n\n    # WTFORM impone che tutte le scelte siano definite prima, quindi metto tutti gli annotatori possibili,\n    # verranno poi filtrati cliccando il video\n\n    for v in video_choices:\n        for annotator in video_choices[v][\"annotators\"]:\n            choice = (annotator[\"id\"], annotator[\"name\"])\n            if choice not in form.annotators.choices:\n                form.annotators.choices.append(choice)\n\n    if form.validate_on_submit():\n        create_gold(form.video.data, form.annotators.data, form.agreements.data, form.name.data)\n\n    return render_template('gold_standard.html',  video_choices=video_choices, form=form)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.index","title":"<code>index()</code>","text":"<p>Render the index page.</p> <p>Returns:</p> Type Description <code>str</code> <p>The rendered HTML content of the index page.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/')\ndef index():\n    \"\"\"\n    Render the index page.\n\n    Returns\n    -------\n    str\n        The rendered HTML content of the index page.\n    \"\"\"\n    return render_template('index.html')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.lemmatize_term","title":"<code>lemmatize_term()</code>","text":"<p>Lemmatize a term in the specified language.</p> <p>This endpoint processes a concept term to extract its semantic structure and returns the lemmatized form along with linguistic information.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Language code for lemmatization</p> required <code>concept</code> <code>str</code> <p>Term to be lemmatized</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON response containing: - text : str     Original input text - lemma : str     Lemmatized form of the text - pos : str     Part of speech tag - dep : str     Dependency relation - head : str     Head word in dependency relation</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/lemmatize_term', methods=[\"GET\", \"POST\"])\ndef lemmatize_term():\n    \"\"\"\n    Lemmatize a term in the specified language.\n\n    This endpoint processes a concept term to extract its semantic structure\n    and returns the lemmatized form along with linguistic information.\n\n    Parameters\n    ----------\n    language : str\n        Language code for lemmatization\n    concept : str\n        Term to be lemmatized\n\n    Returns\n    -------\n    dict\n        JSON response containing:\n        - text : str\n            Original input text\n        - lemma : str\n            Lemmatized form of the text\n        - pos : str\n            Part of speech tag\n        - dep : str\n            Dependency relation\n        - head : str\n            Head word in dependency relation\n    \"\"\"\n    language = request.json[\"lang\"]\n    concept = request.json[\"concept\"]\n    return SemanticText(concept,language).get_semantic_structure_info()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.login","title":"<code>login()</code>","text":"<p>Handle the login process for users.</p> <p>Returns:</p> Type Description <code>str</code> <p>The rendered HTML content of the login page if the user is not authenticated or the form is not submitted/valid.</p> <code>Response</code> <p>A redirect response to the next page if the user is authenticated or the form is submitted and valid.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/login', methods=['POST', 'GET'])\ndef login():\n    \"\"\"\n    Handle the login process for users.\n\n    Returns\n    -------\n    str\n        The rendered HTML content of the login page if the user is not authenticated or the form is not submitted/valid.\n    werkzeug.wrappers.Response\n        A redirect response to the next page if the user is authenticated or the form is submitted and valid.\n    \"\"\"\n    form = LoginForm()\n\n    if current_user.is_authenticated:\n        next_page = url_for('index')\n        return redirect(next_page)\n\n    if form.is_submitted():\n        if form.validate():\n            user = User(form.email.data)\n            login_user(user, remember=form.remember_me.data)\n            next_page = request.args.get('next')\n            if not next_page or urlparse(next_page).netloc != '':\n                next_page = url_for('index')\n            return redirect(next_page)\n\n    return render_template('user/login.html', form=form)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.logout","title":"<code>logout()</code>","text":"<p>Handle the logout process for users.</p> <p>Returns:</p> Type Description <code>str</code> <p>The rendered HTML content of the index page after logging out the user.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/logout')\ndef logout():\n    \"\"\"\n    Handle the logout process for users.\n\n    Returns\n    -------\n    str\n        The rendered HTML content of the index page after logging out the user.\n    \"\"\"\n    logout_user()\n    return render_template('index.html')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.password_reset","title":"<code>password_reset(token)</code>","text":"<p>Handle the password reset process for users.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The password reset token sent to the user's email.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The rendered HTML content of the password reset page if the form is not submitted/valid.</p> <code>Response</code> <p>A redirect response to the index page if the token is invalid or expired.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/password_reset/&lt;token&gt;', methods=['POST', 'GET'])\ndef password_reset(token):\n    \"\"\"\n    Handle the password reset process for users.\n\n    Parameters\n    ----------\n    token : str\n        The password reset token sent to the user's email.\n\n    Returns\n    -------\n    str\n        The rendered HTML content of the password reset page if the form is not submitted/valid.\n    werkzeug.wrappers.Response\n        A redirect response to the index page if the token is invalid or expired.\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::password_reset() ******\")\n\n    form = PasswordResetForm()\n\n    try:\n        email = confirm_token(token)\n\n        if form.validate_on_submit():\n            hashpass = bcrypt.hashpw(form.password.data.encode('utf-8'), bcrypt.gensalt())\n            password_hash = hashpass.decode('utf8')\n            mongo.reset_password(email, password_hash)\n            flash('Password updated', 'success')\n\n        return render_template('user/password_reset.html', form=form)\n\n    except:\n        flash('The link is invalid or has expired.', 'danger')\n        return render_template('index.html')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.prepare_annotated_graph","title":"<code>prepare_annotated_graph()</code>","text":"<p>Prepare concept graph annotations for download in JSON-LD format.</p> <p>This endpoint receives annotation data, converts it to JSON-LD format with SKOS vocabulary, and prepares it for client-side download. The function creates a collection of concepts and their relationships in a standardized semantic web format.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>JSON data containing: - id : str     Video identifier  - conceptVocabulary : dict     Dictionary mapping concepts to their vocabulary - language : str     Language code for the annotations</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON-LD formatted data containing: - @context : dict     JSON-LD context definitions - @graph : list     List of nodes in the concept graph including:     - Concept definitions     - Relationships     - SKOS vocabulary collection</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/download_graph', methods=[\"GET\", \"POST\"])\ndef prepare_annotated_graph():\n    \"\"\"\n    Prepare concept graph annotations for download in JSON-LD format.\n\n    This endpoint receives annotation data, converts it to JSON-LD format with SKOS vocabulary,\n    and prepares it for client-side download. The function creates a collection of concepts\n    and their relationships in a standardized semantic web format.\n\n    Parameters\n    ----------\n    annotations : dict\n        JSON data containing:\n        - id : str\n            Video identifier \n        - conceptVocabulary : dict\n            Dictionary mapping concepts to their vocabulary\n        - language : str\n            Language code for the annotations\n\n    Returns\n    -------\n    dict\n        JSON-LD formatted data containing:\n        - @context : dict\n            JSON-LD context definitions\n        - @graph : list\n            List of nodes in the concept graph including:\n            - Concept definitions\n            - Relationships\n            - SKOS vocabulary collection\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: main.py::download_graph(): Inizio ******\")\n\n    annotations = request.json\n\n    _, json = annotations_to_jsonLD(annotations,isAutomatic=False)\n\n    conceptVocabulary = create_skos_dictionary(annotations[\"conceptVocabulary\"], annotations[\"id\"], \"manu\", annotations[\"language\"])\n\n    json[\"graph\"][\"@graph\"].append({ \"id\":\"localVocabulary\",\"type\": \"skos:Collection\",\"skos:member\": [elem for elem in conceptVocabulary[\"@graph\"]]})\n\n    result = {\n        \"@context\": conceptVocabulary[\"@context\"],\n        \"@graph\": json[\"graph\"][\"@graph\"]\n    }\n\n    print(\"***** EKEEL - Video Annotation: main.py::download_annotated_graph(): Fine ******\")\n    # real download happens on the js side\n    return result   \n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.register","title":"<code>register()</code>","text":"<p>Handle the registration process for new users.</p> <p>Returns:</p> Type Description <code>str</code> <p>The rendered HTML content of the registration page if the form is not submitted/valid.</p> <code>Response</code> <p>A redirect response to the confirmation code page if the form is submitted and valid.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/register', methods=['POST', 'GET'])\ndef register():\n    \"\"\"\n    Handle the registration process for new users.\n\n    Returns\n    -------\n    str\n        The rendered HTML content of the registration page if the form is not submitted/valid.\n    werkzeug.wrappers.Response\n        A redirect response to the confirmation code page if the form is submitted and valid.\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::register() ******\")\n\n    form = RegisterForm()\n    if form.validate_on_submit():\n        password = bcrypt.hashpw(form.password.data.encode('utf-8'), bcrypt.gensalt())\n        password_hash = password.decode('utf8')\n\n        # generate a random string of lenght N composed of lowercase letters and numbers\n\n        code = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(6)).upper()\n        hashed_code = bcrypt.hashpw(code.encode('utf-8'), bcrypt.gensalt())\n        code_on_creation_hash = hashed_code.decode('utf8')\n\n        new_user = {\n            'name': form.name.data,\n            'surname': form.surname.data,\n            'email': form.email.data,\n            'password_hash': password_hash,\n            'code_on_creation_hash': code_on_creation_hash,\n            'nb_try_code_on_creation': 0\n        }\n\n        unverified_users.insert_one(new_user)\n\n        send_confirmation_mail(form.email.data, code)\n\n\n        mail = json.dumps(form.email.data)\n        next_page = url_for('confirm_code', mail=mail)\n        return redirect(next_page)\n\n    return render_template('user/register.html', form=form)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.upload_annotated_graph","title":"<code>upload_annotated_graph()</code>","text":"<p>Upload and store annotated concept graph in JSON-LD format.</p> <p>This endpoint receives annotation data for a video, converts it to JSON-LD format, and stores it in MongoDB. The annotations include concept relationships, vocabulary, and metadata about the annotation process.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>JSON data containing: - id : str     Video identifier - conceptVocabulary : dict     Mapping of concepts to synonyms - language : str     Language of annotations - is_completed : bool     Whether annotation is complete</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON response containing: - done : bool     True if upload successful, False if error occurred</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/upload_graph', methods=[\"GET\", \"POST\"])\ndef upload_annotated_graph():\n    \"\"\"\n    Upload and store annotated concept graph in JSON-LD format.\n\n    This endpoint receives annotation data for a video, converts it to JSON-LD format,\n    and stores it in MongoDB. The annotations include concept relationships, vocabulary,\n    and metadata about the annotation process.\n\n    Parameters\n    ----------\n    annotations : dict\n        JSON data containing:\n        - id : str\n            Video identifier\n        - conceptVocabulary : dict\n            Mapping of concepts to synonyms\n        - language : str\n            Language of annotations\n        - is_completed : bool\n            Whether annotation is complete\n\n    Returns\n    -------\n    dict\n        JSON response containing:\n        - done : bool\n            True if upload successful, False if error occurred\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: main.py::upload_annotations(): Inizio ******\")\n    annotations = request.json\n\n    _, data = annotations_to_jsonLD(annotations,isAutomatic=False)\n\n    data[\"video_id\"] = annotations[\"id\"]\n    data[\"annotator_id\"] = current_user.mongodb_id\n    data[\"annotator_name\"] = current_user.complete_name\n    data[\"email\"] = current_user.email\n    data[\"conceptVocabulary\"] = create_skos_dictionary(annotations[\"conceptVocabulary\"], annotations[\"id\"], \"manu\", annotations[\"language\"])\n    data[\"annotation_completed\"] = annotations[\"is_completed\"]\n\n    data[\"graph\"][\"@graph\"].extend([{\"id\": x[\"id\"], \"type\" : \"skos:Concept\"} for x in data[\"conceptVocabulary\"][\"@graph\"]])\n\n\n    # inserting annotations on DB\n    try: \n        mongo.insert_graph(data)    \n    except Exception as e:\n        print(e)\n        flash(e,\"error\")\n        return {\"done\":False}\n\n    print(\"***** EKEEL - Video Annotation: main.py::upload_annotations(): Fine ******\")\n    # TODO show a message on screen\n    return {\"done\":True}\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.video_segmentation_refinement","title":"<code>video_segmentation_refinement()</code>","text":"<p>Refine video segmentation and update concept definitions.</p> <p>This endpoint processes video segments to refine concept definitions and timestamps, storing updated annotations in the database.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>JSON request data containing: - id : str     Video identifier - conceptVocabulary : dict     Mapping of concepts to synonyms - definitions : dict     Current concept definitions with timestamps - concept_map : dict     Current concept relationships</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON response containing: - definitions : dict     Updated concept definitions with refined timestamps - downloadable_jsonld_graph : dict     JSON-LD formatted graph containing:     - @context : dict         JSON-LD context definitions     - @graph : list         Updated nodes including:         - Concept definitions         - Relationships         - SKOS vocabulary</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/refinement', methods=[\"GET\", \"POST\"])\ndef video_segmentation_refinement():\n    \"\"\"\n    Refine video segmentation and update concept definitions.\n\n    This endpoint processes video segments to refine concept definitions\n    and timestamps, storing updated annotations in the database.\n\n    Parameters\n    ----------\n    data : dict \n        JSON request data containing:\n        - id : str\n            Video identifier\n        - conceptVocabulary : dict\n            Mapping of concepts to synonyms\n        - definitions : dict\n            Current concept definitions with timestamps\n        - concept_map : dict\n            Current concept relationships\n\n    Returns\n    -------\n    dict\n        JSON response containing:\n        - definitions : dict\n            Updated concept definitions with refined timestamps\n        - downloadable_jsonld_graph : dict\n            JSON-LD formatted graph containing:\n            - @context : dict\n                JSON-LD context definitions\n            - @graph : list\n                Updated nodes including:\n                - Concept definitions\n                - Relationships\n                - SKOS vocabulary\n    \"\"\"\n    data = request.json\n    video_id = data[\"id\"]\n    concept_vocabulary = data[\"conceptVocabulary\"]\n\n    # for design this should not return None\n    video = VideoAnalyzer(video_id, {\"language\"})\n    new_concepts,definitions = video.adjust_or_insert_definitions_and_indepth_times(data[\"definitions\"],_show_output=True)\n\n    #from pprint import pprint\n    #pprint(definitions)\n    _,burst_graph = create_burst_graph(video_id,definitions,data[\"concept_map\"])\n    try:\n        local_vocabulary = create_local_vocabulary(video_id,concept_vocabulary)\n    except Exception as e:\n        print(e)\n        flash(e,'message')\n    skos_concepts = local_vocabulary[\"skos:member\"]\n    if len(new_concepts) &gt; 0:\n        skos_concepts.extend(convert_to_skos_concepts(new_concepts,concept_vocabulary,video.data[\"language\"]))\n    downloadable_jsonld_graph = {\"@context\":burst_graph[\"@context\"],\"@graph\":burst_graph[\"@graph\"].copy()+[local_vocabulary]}\n    burst_graph[\"@graph\"].extend([{\"id\":concept[\"id\"],\"type\":concept[\"type\"]} for concept in skos_concepts])\n\n    mongo.insert_graph({ \"video_id\":video_id,\n                            \"annotator_id\":current_user.complete_name.replace(\" \",\"_\")+\"_Burst_Analysis\",\n                            \"annotator_name\":\"Burst_Analysis\",\n                            \"email\":\"Burst_Analysis\",\n                            \"graph\": burst_graph,\n                            \"conceptVocabulary\": {\"@context\": burst_graph[\"@context\"], \"@graph\": skos_concepts}})\n\n    return {\"definitions\":definitions,\n            \"downloadable_jsonld_graph\":downloadable_jsonld_graph}\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.main.video_selection","title":"<code>video_selection()</code>","text":"<p>Handle video selection and processing for annotation purposes.</p> <p>This function manages the video selection interface and processes new video submissions. It handles both GET requests to display the video selection page and POST requests to  process new video submissions. For new videos, it:</p> <ul> <li> <p>Downloads the video</p> </li> <li> <p>Extracts and processes the transcript</p> </li> <li> <p>Segments the video</p> </li> <li> <p>Creates thumbnails</p> </li> <li> <p>Processes concepts and vocabulary</p> </li> <li> <p>Sets up the annotation environment</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the video to be processed (from addVideoForm)</p> required <p>Returns:</p> Type Description <code>str</code> <p>On GET or failed form validation: renders video_selection.html template showing existing videos and upload form</p> <code>Response</code> <p>On successful POST: renders mooc_annotator.html template with processed video data and annotation interface</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/main.py</code> <pre><code>@app.route('/video_selection', methods=['GET', 'POST'])\n@login_required\ndef video_selection():\n    \"\"\"\n    Handle video selection and processing for annotation purposes.\n\n    This function manages the video selection interface and processes new video submissions.\n    It handles both GET requests to display the video selection page and POST requests to \n    process new video submissions. For new videos, it:\n\n    - Downloads the video\n\n    - Extracts and processes the transcript\n\n    - Segments the video\n\n    - Creates thumbnails\n\n    - Processes concepts and vocabulary\n\n    - Sets up the annotation environment\n\n    Parameters\n    ----------\n    url : str\n        URL of the video to be processed (from addVideoForm)\n\n    Returns\n    -------\n    str\n        On GET or failed form validation: renders video_selection.html template\n        showing existing videos and upload form\n    werkzeug.wrappers.Response\n        On successful POST: renders mooc_annotator.html template with processed\n        video data and annotation interface\n\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: main.py::video_selection(): Inizio ******\")\n    form = addVideoForm()\n    videos = mongo.get_videos([\"video_id\",\"title\", \"creator\"])\n    annotator = current_user.mongodb_id\n    for video in videos:\n        annotation_status = mongo.get_annotation_status(annotator, video[\"video_id\"])\n        if annotation_status is None:\n            annotation_status = \"None\"\n        else:\n            annotation_status = \"Completed\" if annotation_status[\"annotation_completed\"] else \"Progressing\"\n        video[\"annotation_status\"] = annotation_status\n\n\n    if not form.validate_on_submit():\n        return render_template('video_selection.html', form=form, videos=videos)\n\n    try:\n        url = form.url.data\n        vid_analyzer = VideoAnalyzer(url)\n        vid_analyzer.download_video()\n\n        # NOTE extracting transcript from audio with whisper on high-end i9 8 core process at ~1.3 sec/s\n        vid_analyzer.request_transcript()\n        vid_analyzer.analyze_transcript()\n        vid_analyzer.request_terms()\n        vid_analyzer.filter_terms()\n        vid_analyzer.transcript_segmentation()\n        vid_analyzer.create_thumbnails()\n        #vid_analyzer.analyze_video()  for now we don't extract slides\n        video_id = vid_analyzer.video_id\n        data = vid_analyzer.data\n\n        language = vid_analyzer.identify_language()\n        text = SemanticText(\" \".join(timed_sentence[\"text\"] for timed_sentence in data[\"transcript_data\"][\"text\"] if not \"[\" in timed_sentence['text']), language)\n        conll_sentences = conll_gen(video_id,text,language)\n        if vid_analyzer.data[\"transcript_data\"][\"is_whisper_transcribed\"]:\n            #lemmatized_subtitles, all_lemmas = html_interactable_transcript_word_level(data[\"transcript_data\"][\"text\"], language)\n            #all_lemmas = vid_analyzer.data[\"transcript_data\"][\"lemmas\"]\n            lemmatized_subtitles = html_interactable_transcript_word_level(data[\"transcript_data\"][\"text\"])\n        else:\n            lemmatized_subtitles, all_lemmas = html_interactable_transcript_legacy(data[\"transcript_data\"][\"text\"], conll_sentences, language)\n        annotator = current_user.complete_name\n        relations = mongo.get_concept_map(current_user.mongodb_id, video_id)\n        definitions = mongo.get_definitions(current_user.mongodb_id, video_id)\n        completed_graph = mongo.get_annotation_status(current_user.mongodb_id, video_id)\n        marked_completed = completed_graph is not None and completed_graph[\"annotation_completed\"]\n\n        # Obtaining concept vocabulary from DB\n        conceptVocabulary  = mongo.get_vocabulary(current_user.mongodb_id, video_id)\n\n        # If the concept vocabulary is in the DB then initialize concept to the ones of the vocabulary\n        if conceptVocabulary is not None:\n            conceptVocabulary = {key:value for key,value in conceptVocabulary.items()}\n            lemmatized_concepts = []\n            sem_text = SemanticText(\"\",language)\n            for concept in conceptVocabulary.keys():\n                lemmatized_concepts.append(sem_text.set_text(concept).get_semantic_structure_info())\n\n        # If the concept vocabulary is new (empty) in DB then initialize it from the terms extracted\n        if conceptVocabulary is None:\n            lemmatized_concepts = [SemanticText(term[\"term\"].lower() if term[\"term\"].istitle() else term[\"term\"],language).get_semantic_structure_info() for term in vid_analyzer.data[\"transcript_data\"][\"terms\"]]\n            #-----------------------------------------------------------------\n            # 1) Automatically obtain synonyms using wordnet NLTK\n            #\n            #conceptVocabulary = get_synonyms_from_list(lemmatized_concepts)\n            # 2) Start with empty synonyms in concept vocabulary\n            #\n            conceptVocabulary = {}\n            for concept in lemmatized_concepts :\n                conceptVocabulary[concept[\"text\"]] = []\n            #-----------------------------------------------------------------\n        # This shouldn't happen but in case of different versions of annotations is kept for compatibility\n        for rel in relations:\n            if rel[\"prerequisite\"] not in conceptVocabulary.keys():\n                lemmatized_concepts.append(SemanticText(rel[\"prerequisite\"],language).get_semantic_structure_info())\n            if rel[\"target\"] not in conceptVocabulary.keys():\n                lemmatized_concepts.append(SemanticText(rel[\"target\"],language).get_semantic_structure_info())\n\n        NLPSingleton().destroy()  \n\n        return render_template('mooc_annotator.html', \n                               result=data[\"transcript_data\"][\"text\"], video_id=video_id, start_times=list(map(lambda x: x[0],data[\"video_data\"][\"segments\"])),\n                               images_path=vid_analyzer.images_path, concepts=lemmatized_concepts,is_temp_transcript=not data[\"transcript_data\"][\"is_whisper_transcribed\"],\n                               video_duration=data['duration'], lemmatized_subtitles=lemmatized_subtitles, annotator=annotator, language=language, is_completed=marked_completed,\n                               conceptVocabulary=conceptVocabulary, title=data['title'], relations=relations, definitions=definitions)\n    except Exception as e:\n        import sys\n        import os\n        import traceback\n\n        tb_details = traceback.extract_tb(sys.exc_info()[2])\n\n        print(f\"\\033[91mException in video selection: {e}\\033[0m\")\n        for frame in tb_details:\n            filename = os.path.basename(frame.filename)\n            # Read the specific line of code\n            line_number = frame.lineno\n            with open(frame.filename, 'r') as f:\n                lines = f.readlines()\n                error_line = lines[line_number - 1].strip()\n            print(f\"\\033[91mFile: {filename}, Function: {frame.name}, Line: {line_number} | {error_line}\\033[0m\")\n        flash(e, \"Danger\")\n\n    print(\"***** EKEEL - Video Annotation: main.py::video_selection(): Fine ******\")\n\n    return render_template('video_selection.html', form=form, videos=videos)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/transcribe/","title":"transcribe","text":""},{"location":"codebase/EKEELVideoAnnotation/transcribe/#transcribe","title":"Transcribe","text":""},{"location":"codebase/EKEELVideoAnnotation/transcribe/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.transcribe--external-video-transcription-service","title":"External Video Transcription Service","text":"<p>Provides video transcription services for the EKEEL annotation system. Used in deployment as an external worker service.</p> Notes <p>More details about deployment can be found here</p> <p>Functions:</p> Name Description <code>main</code> <p>Main worker process for continuous video transcription</p>"},{"location":"codebase/EKEELVideoAnnotation/transcribe/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.transcribe.main","title":"<code>main()</code>","text":"<p>Continuous video transcription worker process.</p> <p>Runs an infinite loop to process untranscribed videos by:</p> <ol> <li> <p>Retrieving untranscribed videos from MongoDB</p> </li> <li> <p>Downloading the video from YouTube</p> </li> <li> <p>Converting to WAV</p> </li> <li> <p>Transcribing with <code>stable-whisper</code> library and large-v3 model</p> </li> <li> <p>Storing results</p> </li> </ol> Source code in <code>EVA_apps/EKEELVideoAnnotation/transcribe.py</code> <pre><code>def main():\n    \"\"\"\n    Continuous video transcription worker process.\n\n    Runs an infinite loop to process untranscribed videos by:\\n\n    1. Retrieving untranscribed videos from MongoDB\\n\n    2. Downloading the video from YouTube\\n\n    3. Converting to WAV\\n\n    4. Transcribing with `stable-whisper` library and large-v3 model\\n\n    5. Storing results\\n\n    \"\"\"\n    # TODO stable-ts version 2.17.3: passing the language is not working, will be inferenced at cost of small increase in time\n    # self._model.transcribe(wav_path.__str__(), decode_options={\"language\":language}) \\\n    #             .save_as_json(json_path.__str__())\n    model = stable_whisper.load_model(name='large-v3', in_memory=True, cpu_preload=True)\n    print(\"Model loaded\")\n\n    from pathlib import Path\n    base_folder = Path(__file__).parent.joinpath(\"static\").joinpath(\"videos\")\n\n    from database.mongo import get_untranscribed_videos, insert_video_data, get_video_data, remove_annotations_data\n    from time import sleep, time\n    from json import load\n    from media.audio import convert_mp4_to_wav\n    from media.segmentation import VideoAnalyzer\n    import os\n\n    try:\n        while True:\n            try:\n                videos_metadata:list = get_untranscribed_videos()\n                print(f\"Jobs: {videos_metadata}\")\n            except Exception as e:\n                import sys\n                import os\n                import traceback\n\n                tb_details = traceback.extract_tb(sys.exc_info()[2])\n\n                print(f\"Exception: {e}\")\n                for frame in tb_details:\n                    filename = os.path.basename(frame.filename)\n                    # Read the specific line of code\n                    line_number = frame.lineno\n                    with open(frame.filename, 'r') as f:\n                        lines = f.readlines()\n                        error_line = lines[line_number - 1].strip()\n                    print(f\"File: {filename}, Function: {frame.name}, Line: {line_number} | {error_line}\")\n                # If there is an error at network level sleep and try again reconnecting\n                sleep(300)\n                from env import MONGO_CLUSTER_USERNAME, MONGO_CLUSTER_PASSWORD\n                import pymongo\n                global client\n                global db\n                client = pymongo.MongoClient(\n                            \"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/ekeel?retryWrites=true&amp;w=majority\")\n\n                db = client.ekeel\n                continue\n            for (video_id, language) in videos_metadata:\n                print(f\"New job: {video_id}\")\n                start_time = time()\n                video_folder_path = base_folder.joinpath(video_id)\n                try:\n                    VideoAnalyzer(\"https://www.youtube.com/watch?v=\"+video_id, request_fields_from_db=[\"video_id\"]).download_video()\n                    convert_mp4_to_wav(video_folder_path, video_id)\n                except Exception as e:\n                    print(e)\n                    sleep(300)\n                    continue\n\n                wav_path = video_folder_path.joinpath(video_id+\".wav\")\n                json_path = video_folder_path.joinpath(video_id+\".json\")\n\n                model.transcribe(wav_path.__str__()).save_as_json(json_path.__str__())\n\n                with open(json_path) as f:\n                    transcribed_data = load(f)[\"segments\"]\n\n                os.remove(wav_path)\n                #os.remove(json_path)  # Don't remove json for debug purposes\n\n                video_data = get_video_data(video_id)\n                video_data[\"transcript_data\"] = {\n                                 \"is_whisper_transcribed\":True, \n                                 \"is_autogenerated\":True, \n                                 \"text\":transcribed_data\n                                }\n\n                insert_video_data(video_data,update=False)\n                remove_annotations_data(video_id)\n                print(f\"Done job: {video_id} in {round(time()-start_time,1)} seconds\")\n            sleep(60)\n    except Exception as e:\n        import sys\n        import os\n        import traceback\n\n        tb_details = traceback.extract_tb(sys.exc_info()[2])\n\n        print(f\"Exception: {e}\")\n        for frame in tb_details:\n            filename = os.path.basename(frame.filename)\n            # Read the specific line of code\n            line_number = frame.lineno\n            with open(frame.filename, 'r') as f:\n                lines = f.readlines()\n                error_line = lines[line_number - 1].strip()\n            print(f\"File: {filename}, Function: {frame.name}, Line: {line_number} | {error_line}\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/","title":"extractor","text":""},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#extractor","title":"Extractor","text":"<p>Burst detection module for text analysis.</p> <p>This module implements Kleinberg's burst detection algorithm to identify bursts of term occurrences in text.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor","title":"<code>BurstExtractor</code>","text":"<p>Burst detection implementation using Kleinberg's algorithm.</p> <p>Detects bursts of term occurrences in text by analyzing their temporal distribution using an automaton model.</p> <p>Attributes:</p> Name Type Description <code>_rawtext</code> <code>str</code> <p>The input text to analyze</p> <code>_terminology</code> <code>list</code> <p>List of terms to detect bursts for</p> <code>_offsets</code> <code>dict</code> <p>Dictionary mapping terms to their occurrence positions</p> <code>_bursts</code> <code>DataFrame</code> <p>Detected bursts with columns [keyword, level, start, end]</p> <code>_s</code> <code>float</code> <p>Base of exponential distribution</p> <code>_gamma</code> <code>float</code> <p>Cost coefficient between states</p> <p>Methods:</p> Name Description <code>find_offsets</code> <p>Find term occurrences in text</p> <code>generate_bursts</code> <p>Detect bursts using Kleinberg's algorithm</p> <code>filter_bursts</code> <p>Filter bursts by hierarchy level</p> <code>break_bursts</code> <p>Break long bursts into smaller ones</p> <code>get_words_with_bursts</code> <p>Get terms that have bursts at given level</p> <code>get_excluded_words</code> <p>Get terms with no bursts at given level</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/extractor.py</code> <pre><code>class BurstExtractor:\n    \"\"\"\n    Burst detection implementation using Kleinberg's algorithm.\n\n    Detects bursts of term occurrences in text by analyzing their temporal\n    distribution using an automaton model.\n\n    Attributes\n    ----------\n    _rawtext : str\n        The input text to analyze\n    _terminology : list\n        List of terms to detect bursts for\n    _offsets : dict\n        Dictionary mapping terms to their occurrence positions\n    _bursts : pandas.DataFrame\n        Detected bursts with columns [keyword, level, start, end]\n    _s : float\n        Base of exponential distribution\n    _gamma : float\n        Cost coefficient between states\n\n    Methods\n    -------\n    find_offsets(words, occ_index_file)\n        Find term occurrences in text\n    generate_bursts(s, gamma)\n        Detect bursts using Kleinberg's algorithm\n    filter_bursts(level)\n        Filter bursts by hierarchy level\n    break_bursts(burst_length, num_occurrences)\n        Break long bursts into smaller ones\n    get_words_with_bursts(level)\n        Get terms that have bursts at given level\n    get_excluded_words(level)\n        Get terms with no bursts at given level\n    \"\"\"\n\n    def __init__(self, text, wordlist):\n        \"\"\"\n        Initialize burst detector.\n\n        Parameters\n        ----------\n        text : str\n            Text to analyze\n        wordlist : list\n            Terms to detect bursts for\n        \"\"\"\n        # open and load the text into a variable\n        self._rawtext = text\n\n        self._terminology = wordlist\n\n        # initialize parameters of Kleinberg\n        self._s = None\n        self._gamma = None\n        # initialize the final structures\n        self._offsets = defaultdict(list)\n        self._bursts = pd.DataFrame(columns=['keyword', 'level', 'start', 'end'],\n                                    dtype='int64')\n\n    def find_offsets(self, words=None, occ_index_file: str=None) -&gt; dict:\n        \"\"\"\n        Find term occurrences in text.\n\n        Parameters\n        ----------\n        words : list, optional\n            Terms to find offsets for\n        occ_index_file : str, optional\n            Pre-computed offsets file\n\n        Returns\n        -------\n        dict\n            Term to offset position mapping\n        \"\"\"\n        if occ_index_file is not None:\n            ### use offsets that are available in occ index file\n            self._offsets = {}\n\n            for o in occ_index_file.itertuples():\n                if o.Lemma not in self._offsets:\n                    self._offsets[o.Lemma] = []\n\n                if o.idFrase not in self._offsets[o.Lemma]:\n                    self._offsets[o.Lemma].append(o.idFrase)\n\n        else:\n            ### find the offsets using NLTK\n            # (non dovrebbe mai servire)\n\n            # reset and populate the offsets dict\n            self._offsets = defaultdict(list)\n\n            sentences = nltk.sent_tokenize(self._rawtext)\n            # search each word in each sentence\n            for word in self._terminology:\n                for index, sent in enumerate(sentences):\n                    if word.upper() in sent.upper():\n                        # add the index of the sentence in the list of offsets of that word\n                        self._offsets[word].append(index)\n        return self._offsets\n\n    def generate_bursts(self, s=2, gamma=1) -&gt; pd.DataFrame:\n        \"\"\"\n        Detect bursts using Kleinberg's algorithm.\n\n        Parameters\n        ----------\n        s : float, optional\n            Base of exponential distribution (&gt;1)\n        gamma : float, optional\n            Cost coefficient between states (&gt;0)\n\n        Returns\n        -------\n        pandas.DataFrame\n            Detected bursts with columns [keyword, level, start, end]\n        \"\"\"\n        if not self._offsets:\n            return None\n            choice = input(\"\"\"You must first detect the offsets (by calling the method 'find_offset').\n                            Do you want to find offsets now (without an index file) \n                            and then automatically detect the bursts? Press y/n\\n\"\"\")\n            if choice in ['y', 'Y']:\n                print('The offsets will be detected and then the process will compute bursts.\\n')\n                self._offsets = self.find_offsets()\n            else:\n                print(\"Neither offsets or bursts will be computed.\\n\")\n                return None\n\n        # reset self._burst\n        self._bursts = pd.DataFrame(columns=['keyword', 'level', 'start', 'end'],\n                                    dtype='int64')\n\n        for keyword in self._offsets:\n            # compute bursts\n            curr_bursts = kleinberg(self._offsets[keyword], s, gamma)\n            # insert the name of the word in the array\n            curr_bursts = np.insert(curr_bursts, 0, values=keyword, axis=1)\n            # convert it to a df and append it to the complete df of bursts\n            curr_bursts_df = pd.DataFrame(curr_bursts,\n                                          columns=['keyword', 'level', 'start', 'end'])\n            self._bursts = pd.concat([self._bursts, curr_bursts_df], ignore_index=True)\n\n        return self._bursts\n\n    def filter_bursts(self, level=1, save_monolevel_keywords=False, \n                     replace_original_results=False) -&gt; pd.DataFrame:\n        \"\"\"\n        Filter bursts by hierarchy level.\n\n        Parameters\n        ----------\n        level : int, optional\n            Burst hierarchy level to filter by\n        save_monolevel_keywords : bool, optional\n            Keep terms with single burst\n        replace_original_results : bool, optional\n            Update internal burst data\n\n        Returns\n        -------\n        pandas.DataFrame\n            Filtered bursts\n        \"\"\"\n        if self._bursts.shape[0] == 0:\n            raise ValueError(\"Bursts non yet extracted: \"\n                             \"call the method 'generate_bursts' first!\")\n\n        # avoid index errors\n        if level &lt; 0:\n            raise ValueError(\"The level must have a positive value.\")\n        if level &gt; self._bursts['level'].max():\n            print(\"\"\"The desired level exceeds the maximum level present in the results:\n                    the latter will be used.\"\"\")\n            level = self._bursts['level'].max()\n\n        b = self._bursts.copy()\n\n        if save_monolevel_keywords:\n            # don't filter the terms with only a burst, even if this is less that the desired\n            for t in b[\"keyword\"].unique().tolist():\n                if b[b[\"keyword\"] == t].shape[0] == 1:\n                    i = b.index[b['keyword'] == t][0]\n                    b.at[i, \"level\"] = 1\n\n        filtered = b.where(b['level'] == level).dropna()\n\n        if replace_original_results:\n            self._bursts = filtered.copy()\n        else:\n            return filtered\n\n    def break_bursts(self, burst_length=30, num_occurrences=3, \n                    replace_original_results=False, verbose=False):\n        \"\"\"\n        Break long bursts into smaller ones.\n\n        Parameters\n        ----------\n        burst_length : int, optional\n            Minimum length to break burst\n        num_occurrences : int, optional\n            Maximum occurrences to consider\n        replace_original_results : bool, optional\n            Update internal burst data\n        verbose : bool, optional\n            Print detailed information\n\n        Returns\n        -------\n        pandas.DataFrame or None\n            Modified burst data if not replacing\n        \"\"\"\n        if verbose:\n            print(\"The following burst have been deleted and replaced with smaller bursts:\\n\")\n\n        b = self._bursts.copy()\n\n        for i, row in self._bursts.iterrows():\n            curr_len = (row[\"end\"] - row[\"start\"]) + 1\n            if curr_len &gt;= burst_length and len(self._offsets[row[\"keyword\"]]) &lt;= num_occurrences:\n\n                b.drop(i, inplace=True)\n                last_idx = b.index[-1]\n                b.loc[last_idx + 1] = {\"keyword\": row[\"keyword\"], \"level\": row[\"level\"],\n                                       \"start\": row[\"start\"], \"end\": row[\"start\"]}\n                b.loc[last_idx + 1] = {\"keyword\": row[\"keyword\"], \"level\": row[\"level\"],\n                                       \"start\": row[\"end\"], \"end\": row[\"end\"]}\n\n                if verbose:\n                    print(row[\"keyword\"], \"\\toffsets:\", self._offsets[row[\"keyword\"]],\n                          \"\\tstart:\", int(row[\"start\"]), \"\\tend:\", int(row[\"end\"]), \"\\n\")\n\n        if replace_original_results:\n            self._bursts = b.copy()\n        else:\n            return b\n\n    def get_words_with_bursts(self, level=1) -&gt; set:\n        \"\"\"\n        Get terms that have bursts at given level.\n\n        Parameters\n        ----------\n        level : int, optional\n            Burst hierarchy level\n\n        Returns\n        -------\n        set\n            Terms with bursts at specified level\n        \"\"\"\n        filtered_burst = self.filter_bursts(level)\n\n        return set(filtered_burst['keyword'].unique())\n\n    def get_excluded_words(self, level=1) -&gt; set:\n        \"\"\"\n        Get terms with no bursts at given level.\n\n        Parameters\n        ----------\n        level : int, optional\n            Burst hierarchy level\n\n        Returns\n        -------\n        set\n            Terms without bursts at specified level\n        \"\"\"\n        filtered_burst = self.filter_bursts(level)\n\n        return set(self._terminology) - set(filtered_burst['keyword'].unique())\n\n    @property\n    def text_filename(self):\n        \"\"\"Get input text filename.\"\"\"\n        return self._text_filename\n\n    @property\n    def rawtext(self):\n        \"\"\"Get raw input text.\"\"\"\n        return self._rawtext\n\n    @property\n    def terminology(self):\n        \"\"\"Get list of terms.\"\"\"\n        return self._terminology\n\n    @property\n    def offsets(self):\n        \"\"\"Get term offsets dictionary.\"\"\"\n        return self._offsets\n\n    @property\n    def bursts(self):\n        \"\"\"Get detected bursts DataFrame.\"\"\"\n        return self._bursts\n\n    def __repr__(self):\n        return 'BurstExtractor(text_filename={0}, wordlist={1})'.format(\n            repr(self._text_filename), repr(self._terminology))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.bursts","title":"<code>bursts</code>  <code>property</code>","text":"<p>Get detected bursts DataFrame.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.offsets","title":"<code>offsets</code>  <code>property</code>","text":"<p>Get term offsets dictionary.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.rawtext","title":"<code>rawtext</code>  <code>property</code>","text":"<p>Get raw input text.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.terminology","title":"<code>terminology</code>  <code>property</code>","text":"<p>Get list of terms.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.text_filename","title":"<code>text_filename</code>  <code>property</code>","text":"<p>Get input text filename.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.__init__","title":"<code>__init__(text, wordlist)</code>","text":"<p>Initialize burst detector.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to analyze</p> required <code>wordlist</code> <code>list</code> <p>Terms to detect bursts for</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/extractor.py</code> <pre><code>def __init__(self, text, wordlist):\n    \"\"\"\n    Initialize burst detector.\n\n    Parameters\n    ----------\n    text : str\n        Text to analyze\n    wordlist : list\n        Terms to detect bursts for\n    \"\"\"\n    # open and load the text into a variable\n    self._rawtext = text\n\n    self._terminology = wordlist\n\n    # initialize parameters of Kleinberg\n    self._s = None\n    self._gamma = None\n    # initialize the final structures\n    self._offsets = defaultdict(list)\n    self._bursts = pd.DataFrame(columns=['keyword', 'level', 'start', 'end'],\n                                dtype='int64')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.break_bursts","title":"<code>break_bursts(burst_length=30, num_occurrences=3, replace_original_results=False, verbose=False)</code>","text":"<p>Break long bursts into smaller ones.</p> <p>Parameters:</p> Name Type Description Default <code>burst_length</code> <code>int</code> <p>Minimum length to break burst</p> <code>30</code> <code>num_occurrences</code> <code>int</code> <p>Maximum occurrences to consider</p> <code>3</code> <code>replace_original_results</code> <code>bool</code> <p>Update internal burst data</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Print detailed information</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>Modified burst data if not replacing</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/extractor.py</code> <pre><code>def break_bursts(self, burst_length=30, num_occurrences=3, \n                replace_original_results=False, verbose=False):\n    \"\"\"\n    Break long bursts into smaller ones.\n\n    Parameters\n    ----------\n    burst_length : int, optional\n        Minimum length to break burst\n    num_occurrences : int, optional\n        Maximum occurrences to consider\n    replace_original_results : bool, optional\n        Update internal burst data\n    verbose : bool, optional\n        Print detailed information\n\n    Returns\n    -------\n    pandas.DataFrame or None\n        Modified burst data if not replacing\n    \"\"\"\n    if verbose:\n        print(\"The following burst have been deleted and replaced with smaller bursts:\\n\")\n\n    b = self._bursts.copy()\n\n    for i, row in self._bursts.iterrows():\n        curr_len = (row[\"end\"] - row[\"start\"]) + 1\n        if curr_len &gt;= burst_length and len(self._offsets[row[\"keyword\"]]) &lt;= num_occurrences:\n\n            b.drop(i, inplace=True)\n            last_idx = b.index[-1]\n            b.loc[last_idx + 1] = {\"keyword\": row[\"keyword\"], \"level\": row[\"level\"],\n                                   \"start\": row[\"start\"], \"end\": row[\"start\"]}\n            b.loc[last_idx + 1] = {\"keyword\": row[\"keyword\"], \"level\": row[\"level\"],\n                                   \"start\": row[\"end\"], \"end\": row[\"end\"]}\n\n            if verbose:\n                print(row[\"keyword\"], \"\\toffsets:\", self._offsets[row[\"keyword\"]],\n                      \"\\tstart:\", int(row[\"start\"]), \"\\tend:\", int(row[\"end\"]), \"\\n\")\n\n    if replace_original_results:\n        self._bursts = b.copy()\n    else:\n        return b\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.filter_bursts","title":"<code>filter_bursts(level=1, save_monolevel_keywords=False, replace_original_results=False)</code>","text":"<p>Filter bursts by hierarchy level.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Burst hierarchy level to filter by</p> <code>1</code> <code>save_monolevel_keywords</code> <code>bool</code> <p>Keep terms with single burst</p> <code>False</code> <code>replace_original_results</code> <code>bool</code> <p>Update internal burst data</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered bursts</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/extractor.py</code> <pre><code>def filter_bursts(self, level=1, save_monolevel_keywords=False, \n                 replace_original_results=False) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter bursts by hierarchy level.\n\n    Parameters\n    ----------\n    level : int, optional\n        Burst hierarchy level to filter by\n    save_monolevel_keywords : bool, optional\n        Keep terms with single burst\n    replace_original_results : bool, optional\n        Update internal burst data\n\n    Returns\n    -------\n    pandas.DataFrame\n        Filtered bursts\n    \"\"\"\n    if self._bursts.shape[0] == 0:\n        raise ValueError(\"Bursts non yet extracted: \"\n                         \"call the method 'generate_bursts' first!\")\n\n    # avoid index errors\n    if level &lt; 0:\n        raise ValueError(\"The level must have a positive value.\")\n    if level &gt; self._bursts['level'].max():\n        print(\"\"\"The desired level exceeds the maximum level present in the results:\n                the latter will be used.\"\"\")\n        level = self._bursts['level'].max()\n\n    b = self._bursts.copy()\n\n    if save_monolevel_keywords:\n        # don't filter the terms with only a burst, even if this is less that the desired\n        for t in b[\"keyword\"].unique().tolist():\n            if b[b[\"keyword\"] == t].shape[0] == 1:\n                i = b.index[b['keyword'] == t][0]\n                b.at[i, \"level\"] = 1\n\n    filtered = b.where(b['level'] == level).dropna()\n\n    if replace_original_results:\n        self._bursts = filtered.copy()\n    else:\n        return filtered\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.find_offsets","title":"<code>find_offsets(words=None, occ_index_file=None)</code>","text":"<p>Find term occurrences in text.</p> <p>Parameters:</p> Name Type Description Default <code>words</code> <code>list</code> <p>Terms to find offsets for</p> <code>None</code> <code>occ_index_file</code> <code>str</code> <p>Pre-computed offsets file</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Term to offset position mapping</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/extractor.py</code> <pre><code>def find_offsets(self, words=None, occ_index_file: str=None) -&gt; dict:\n    \"\"\"\n    Find term occurrences in text.\n\n    Parameters\n    ----------\n    words : list, optional\n        Terms to find offsets for\n    occ_index_file : str, optional\n        Pre-computed offsets file\n\n    Returns\n    -------\n    dict\n        Term to offset position mapping\n    \"\"\"\n    if occ_index_file is not None:\n        ### use offsets that are available in occ index file\n        self._offsets = {}\n\n        for o in occ_index_file.itertuples():\n            if o.Lemma not in self._offsets:\n                self._offsets[o.Lemma] = []\n\n            if o.idFrase not in self._offsets[o.Lemma]:\n                self._offsets[o.Lemma].append(o.idFrase)\n\n    else:\n        ### find the offsets using NLTK\n        # (non dovrebbe mai servire)\n\n        # reset and populate the offsets dict\n        self._offsets = defaultdict(list)\n\n        sentences = nltk.sent_tokenize(self._rawtext)\n        # search each word in each sentence\n        for word in self._terminology:\n            for index, sent in enumerate(sentences):\n                if word.upper() in sent.upper():\n                    # add the index of the sentence in the list of offsets of that word\n                    self._offsets[word].append(index)\n    return self._offsets\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.generate_bursts","title":"<code>generate_bursts(s=2, gamma=1)</code>","text":"<p>Detect bursts using Kleinberg's algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>float</code> <p>Base of exponential distribution (&gt;1)</p> <code>2</code> <code>gamma</code> <code>float</code> <p>Cost coefficient between states (&gt;0)</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Detected bursts with columns [keyword, level, start, end]</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/extractor.py</code> <pre><code>def generate_bursts(self, s=2, gamma=1) -&gt; pd.DataFrame:\n    \"\"\"\n    Detect bursts using Kleinberg's algorithm.\n\n    Parameters\n    ----------\n    s : float, optional\n        Base of exponential distribution (&gt;1)\n    gamma : float, optional\n        Cost coefficient between states (&gt;0)\n\n    Returns\n    -------\n    pandas.DataFrame\n        Detected bursts with columns [keyword, level, start, end]\n    \"\"\"\n    if not self._offsets:\n        return None\n        choice = input(\"\"\"You must first detect the offsets (by calling the method 'find_offset').\n                        Do you want to find offsets now (without an index file) \n                        and then automatically detect the bursts? Press y/n\\n\"\"\")\n        if choice in ['y', 'Y']:\n            print('The offsets will be detected and then the process will compute bursts.\\n')\n            self._offsets = self.find_offsets()\n        else:\n            print(\"Neither offsets or bursts will be computed.\\n\")\n            return None\n\n    # reset self._burst\n    self._bursts = pd.DataFrame(columns=['keyword', 'level', 'start', 'end'],\n                                dtype='int64')\n\n    for keyword in self._offsets:\n        # compute bursts\n        curr_bursts = kleinberg(self._offsets[keyword], s, gamma)\n        # insert the name of the word in the array\n        curr_bursts = np.insert(curr_bursts, 0, values=keyword, axis=1)\n        # convert it to a df and append it to the complete df of bursts\n        curr_bursts_df = pd.DataFrame(curr_bursts,\n                                      columns=['keyword', 'level', 'start', 'end'])\n        self._bursts = pd.concat([self._bursts, curr_bursts_df], ignore_index=True)\n\n    return self._bursts\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.get_excluded_words","title":"<code>get_excluded_words(level=1)</code>","text":"<p>Get terms with no bursts at given level.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Burst hierarchy level</p> <code>1</code> <p>Returns:</p> Type Description <code>set</code> <p>Terms without bursts at specified level</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/extractor.py</code> <pre><code>def get_excluded_words(self, level=1) -&gt; set:\n    \"\"\"\n    Get terms with no bursts at given level.\n\n    Parameters\n    ----------\n    level : int, optional\n        Burst hierarchy level\n\n    Returns\n    -------\n    set\n        Terms without bursts at specified level\n    \"\"\"\n    filtered_burst = self.filter_bursts(level)\n\n    return set(self._terminology) - set(filtered_burst['keyword'].unique())\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/extractor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.extractor.BurstExtractor.get_words_with_bursts","title":"<code>get_words_with_bursts(level=1)</code>","text":"<p>Get terms that have bursts at given level.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Burst hierarchy level</p> <code>1</code> <p>Returns:</p> Type Description <code>set</code> <p>Terms with bursts at specified level</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/extractor.py</code> <pre><code>def get_words_with_bursts(self, level=1) -&gt; set:\n    \"\"\"\n    Get terms that have bursts at given level.\n\n    Parameters\n    ----------\n    level : int, optional\n        Burst hierarchy level\n\n    Returns\n    -------\n    set\n        Terms with bursts at specified level\n    \"\"\"\n    filtered_burst = self.filter_bursts(level)\n\n    return set(filtered_burst['keyword'].unique())\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/kleinberg/","title":"kleinberg","text":""},{"location":"codebase/EKEELVideoAnnotation/burst/kleinberg/#kleinberg","title":"Kleinberg","text":""},{"location":"codebase/EKEELVideoAnnotation/burst/kleinberg/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.kleinberg.kleinberg","title":"<code>kleinberg(offsets, s, gamma)</code>","text":"<p>KLEINBERG'S BURSTS ANALYSIS ALGORITHM</p> <p>It detects the intervals of a bursting activity of a word in a text, given the indexes of the sentences where the word appears.</p> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>list</code> <p>A list of indexes of the sentences where the word appears (must be non-empty).</p> required <code>s</code> <code>float</code> <p>The base of the exponential distribution (must be greater than 1).</p> required <code>gamma</code> <code>float</code> <p>The coefficient of the costs between states (must be positive).</p> required <p>Returns:</p> Type Description <code>array</code> <p>A numpy array containing all the intervals of burst.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If s &lt;= 1.</p> <code>ValueError</code> <p>If gamma &lt;= 0.</p> <code>ValueError</code> <p>If offsets is empty.</p> <code>ValueError</code> <p>If input contains events with zero time between.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/kleinberg.py</code> <pre><code>def kleinberg(offsets: list, s: float, gamma: float) -&gt; np.array:\n    \"\"\"\n    KLEINBERG'S BURSTS ANALYSIS ALGORITHM\n\n    It detects the intervals of a bursting activity of a word in a text,\n    given the indexes of the sentences where the word appears.\n\n    Parameters\n    ----------\n    offsets : list\n        A list of indexes of the sentences where the word appears (must be non-empty).\n    s : float\n        The base of the exponential distribution (must be greater than 1).\n    gamma : float\n        The coefficient of the costs between states (must be positive).\n\n    Returns\n    -------\n    np.array\n        A numpy array containing all the intervals of burst.\n\n    Raises\n    ------\n    ValueError\n        If s &lt;= 1.\n    ValueError\n        If gamma &lt;= 0.\n    ValueError\n        If offsets is empty.\n    ValueError\n        If input contains events with zero time between.\n    \"\"\"\n\n    if s &lt;= 1:\n        raise ValueError(\"S must be greater than 1!\")\n    if gamma &lt;= 0:\n        raise ValueError(\"Gamma must be positive!\")\n    if len(offsets) &lt; 1:\n        raise ValueError(\"offsets must be non-empty!\")\n\n    offsets = np.array(offsets, dtype=object)\n\n    if offsets.size == 1:\n        bursts = np.array([0, offsets[0], offsets[0]], ndmin=2, dtype=object)\n        return bursts\n\n    offsets = np.sort(offsets)\n    gaps = np.diff(offsets)\n\n    if not np.all(gaps):\n        raise ValueError(\"Input cannot contain events with zero time between!\")\n\n    T = np.sum(gaps)\n    n = np.size(gaps)\n    g_hat = T / n\n\n    k = int(math.ceil(float(1 + math.log(T, s) + math.log(1 / np.amin(gaps), s))))\n\n    gamma_log_n = gamma * math.log(n)\n\n    def tau(i, j):\n        if i &gt;= j:\n            return 0\n        else:\n            return (j - i) * gamma_log_n\n\n    alpha_function = np.vectorize(lambda x: s ** x / g_hat)\n    alpha = alpha_function(np.arange(k))\n\n    def f(j, x):\n        return alpha[j] * math.exp(-alpha[j] * x)\n\n    C = np.repeat(float(\"inf\"), k)\n    C[0] = 0\n\n    q = np.empty((k, 0))\n    for t in range(n):\n        C_prime = np.repeat(float(\"inf\"), k)\n        q_prime = np.empty((k, t + 1))\n        q_prime.fill(np.nan)\n\n        for j in range(k):\n            cost_function = np.vectorize(lambda x: C[x] + tau(x, j))\n            cost = cost_function(np.arange(0, k))\n\n            el = np.argmin(cost)\n\n            if f(j, gaps[t]) &gt; 0:\n                C_prime[j] = cost[el] - math.log(f(j, gaps[t]))\n\n            if t &gt; 0:\n                q_prime[j, :t] = q[el, :]\n\n            q_prime[j, t] = j + 1\n\n        C = C_prime\n        q = q_prime\n\n    j = np.argmin(C)\n    q = q[j, :]\n\n    prev_q = 0\n\n    N = 0\n    for t in range(n):\n        if q[t] &gt; prev_q:\n            N = N + q[t] - prev_q\n        prev_q = q[t]\n\n    bursts = np.array([np.repeat(np.nan, N),\n                       np.repeat(offsets[0], N),\n                       np.repeat(offsets[0], N)],\n                      ndmin=2, dtype=object).transpose()\n\n    burst_counter = -1\n    prev_q = 0\n    stack = np.repeat(np.nan, N)\n    stack_counter = -1\n    for t in range(n):\n        if q[t] &gt; prev_q:\n            num_levels_opened = q[t] - prev_q\n            for i in range(int(num_levels_opened)):\n                burst_counter += 1\n                bursts[burst_counter, 0] = int(prev_q + i)\n                bursts[burst_counter, 1] = offsets[t]\n                stack_counter += 1\n                stack[stack_counter] = burst_counter\n        elif q[t] &lt; prev_q:\n            num_levels_closed = prev_q - q[t]\n            for i in range(int(num_levels_closed)):\n                bursts[int(stack[stack_counter]), 2] = offsets[t]  # fixed\n                stack_counter -= 1\n        prev_q = q[t]\n\n    while stack_counter &gt;= 0:\n        bursts[int(stack[stack_counter]), 2] = offsets[n]  # fixed\n        stack_counter -= 1\n\n    return bursts\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/","title":"prototype","text":""},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#prototype","title":"Prototype","text":"<p>Burst analysis module for concept mapping.</p> <p>This module provides functionality for detecting bursts of concepts in video  transcripts and creating semantic relationships between them.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.Burst","title":"<code>Burst</code>","text":"<p>Burst analysis for video concept mapping.</p> <p>This class implements burst detection and relationship mapping between concepts in video transcripts using Kleinberg's algorithm.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>Input text to analyze</p> <code>words</code> <code>list</code> <p>Words to detect bursts for</p> <code>conll</code> <code>list</code> <p>CoNLL-U formatted text</p> <code>video_id</code> <code>str</code> <p>Video identifier</p> <code>threshold</code> <code>float</code> <p>Burst detection threshold</p> <code>top_n</code> <code>int or None</code> <p>Number of top bursts to consider</p> <code>S</code> <code>float</code> <p>Base of exponential distribution</p> <code>GAMMA</code> <code>float</code> <p>Cost coefficient between states</p> <code>LEVEL</code> <code>int</code> <p>Burst level threshold</p> <code>ALLEN_WEIGHTS</code> <code>dict</code> <p>Weights for Allen relations</p> <code>USE_INVERSES</code> <code>bool</code> <p>Whether to use inverse relations</p> <code>MAX_GAP</code> <code>int</code> <p>Maximum gap between bursts</p> <code>NORM_FORMULA</code> <code>str</code> <p>Formula for normalization</p> <code>occurrences</code> <code>DataFrame</code> <p>Word occurrence positions</p> <code>first_occurence</code> <code>dict</code> <p>First occurrence position of each word</p> <p>Methods:</p> Name Description <code>launch_burst_analysis</code> <p>Run complete burst analysis pipeline</p> <code>df_to_data</code> <p>Convert analysis results to concept maps</p> <code>_merge_contained_definitions</code> <p>Merge overlapping concept definitions</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>class Burst:\n    \"\"\"\n    Burst analysis for video concept mapping.\n\n    This class implements burst detection and relationship mapping between\n    concepts in video transcripts using Kleinberg's algorithm.\n\n    Attributes\n    ----------\n    text : str\n        Input text to analyze\n    words : list\n        Words to detect bursts for\n    conll : list\n        CoNLL-U formatted text\n    video_id : str\n        Video identifier\n    threshold : float\n        Burst detection threshold\n    top_n : int or None\n        Number of top bursts to consider\n    S : float\n        Base of exponential distribution\n    GAMMA : float\n        Cost coefficient between states\n    LEVEL : int\n        Burst level threshold\n    ALLEN_WEIGHTS : dict\n        Weights for Allen relations\n    USE_INVERSES : bool\n        Whether to use inverse relations\n    MAX_GAP : int\n        Maximum gap between bursts\n    NORM_FORMULA : str\n        Formula for normalization\n    occurrences : pandas.DataFrame\n        Word occurrence positions\n    first_occurence : dict\n        First occurrence position of each word\n\n    Methods\n    -------\n    launch_burst_analysis()\n        Run complete burst analysis pipeline\n    df_to_data(sorted_edgelist, burst_res, use_conll)  \n        Convert analysis results to concept maps\n    _merge_contained_definitions(definitions)\n        Merge overlapping concept definitions\n    \"\"\"\n\n    def __init__(self, text, words, video_id, conll, syn_map=False, threshold=0, \n                 top_n=None, s=1.05, gamma=0.0001, level=1, allen_weights=None,\n                 use_inverses=False, max_gap=4, norm_formula=\"modified\"):\n        \"\"\"\n        Initialize burst analyzer.\n\n        Parameters\n        ----------\n        text : str\n            Text to analyze\n        words : list\n            Words to detect bursts for\n        video_id : str\n            Video identifier\n        conll : str\n            CoNLL-U formatted text\n        syn_map : dict or bool, optional\n            Synonym mapping dictionary\n        threshold : float, optional\n            Burst detection threshold\n        top_n : int, optional\n            Number of top bursts to consider\n        s : float, optional\n            Base of exponential distribution\n        gamma : float, optional  \n            Cost coefficient between states\n        level : int, optional\n            Burst level threshold\n        allen_weights : dict, optional\n            Custom weights for Allen relations\n        use_inverses : bool, optional\n            Whether to use inverse relations\n        max_gap : int, optional\n            Maximum gap between bursts\n        norm_formula : str, optional\n            Formula for normalization\n        \"\"\"\n        self.text = text\n        self.words = words\n\n        self.conll = parse(conll)\n        self.video_id = video_id\n        self.threshold = threshold\n        self.top_n = top_n\n        self.data = {}\n\n\n        #Kleinberg's parameters\n        self.S = s\n        self.GAMMA = gamma\n        self.LEVEL = level\n\n        # #self.occurences\n        # #Dataframe contentente le colonne \"Lemma\", \"idFrase\", \"idParolaStart\"\n        occurrences_index = []\n        self.first_occurence = {}\n\n        # PHASE 0\n\n        max_word_lenght = 0\n\n        for w in self.words:\n            l = len(w.split(\" \"))\n            if l &gt; max_word_lenght:\n                max_word_lenght = l\n\n        upper_words = [x.upper() for x in self.words]\n        lemmatizer = WordNetLemmatizer()\n\n\n        for sent in self.conll:\n            sent_index = int(sent.metadata[\"sent_id\"])-1\n            conll_words = self.conll[sent_index].filter(upos=lambda x: x != \"PUNCT\")\n            #from pprint import pprint\n            #pprint(self.conll)\n            counter = 0\n            skip = 0\n\n            for i_, word in enumerate(conll_words):\n                #print(word, word[\"id\"], conll_words[i_+1],conll_words[i_+1][\"id\"], conll_words[i_+2],conll_words[i_+2][\"id\"])\n                if isinstance(word[\"id\"], tuple): skip += 2\n                elif skip &gt; 0: skip -= 1; continue\n                counter += 1\n                #word_index = int(word[\"id\"]) # TODO fix\n                word_index = counter\n                words = word[\"lemma\"]\n                words_form = word[\"form\"]\n                nltk_lemmatized = lemmatizer.lemmatize(word[\"form\"])\n\n                for i in range(1, max_word_lenght+1):\n\n                    occ_words = \"\"\n                    # cerco occorrenza della parola nella forma base, lemmatizata con la conll e lematizzata con nltk\n                    if words.upper() in upper_words:\n                        occ_words = words.lower()\n                    elif words_form.upper() in upper_words:\n                        occ_words = words_form.lower()\n                    elif nltk_lemmatized.upper() in upper_words:\n                        occ_words = nltk_lemmatized\n\n                    if occ_words != \"\":\n                        if occ_words not in self.first_occurence:\n                            self.first_occurence[occ_words] = sent_index\n\n                        d = [occ_words, sent_index, word_index]\n                        occurrences_index.append(d)\n\n                    if i + i_ &lt; len(conll_words):\n                        words += \" \" + conll_words[i_ + i][\"lemma\"]\n                        words_form += \" \" + conll_words[i_ + i][\"form\"]\n                        nltk_lemmatized += \" \" + lemmatizer.lemmatize(conll_words[i_ + i][\"form\"])\n                    else:\n                        break\n\n\n        if syn_map == False:\n            self.occurrences = pd.DataFrame(data=occurrences_index, columns=[\"Lemma\", \"idFrase\", \"idParolaStart\"])\n        else:\n            occur = pd.DataFrame(data=occurrences_index, columns=[\"Lemma\", \"idFrase\", \"idParolaStart\"])\n            new_occur = []\n            for o in occur.itertuples(): \n                new_occur.append([syn_map[o.Lemma], o.idFrase, o.idParolaStart])\n            self.occurrences = pd.DataFrame(new_occur, columns=['Lemma', 'idFrase', 'idParolaStart'])\n\n\n        # weights for Allen and type of normalization formula\n        if allen_weights is None:\n            self.ALLEN_WEIGHTS = {'equals': 2, 'before': 5, 'after': 0, 'meets': 3, 'met-by': 0,\n                             'overlaps': 7, 'overlapped-by': 1, 'during': 7, 'includes': 7,\n                             'starts': 4, 'started-by': 2, 'finishes': 2, 'finished-by': 8}\n        else:\n            self.ALLEN_WEIGHTS = allen_weights\n\n\n        self.USE_INVERSES = use_inverses\n        self.MAX_GAP = max_gap\n        self.NORM_FORMULA = norm_formula\n\n        # decide if preserve relations when giving direction to the burst matrix\n        self.PRESERVE_RELATIONS = True\n\n    def _merge_contained_definitions(self, definitions):\n        \"\"\"\n        Merge overlapping concept definitions.\n\n        Parameters\n        ----------\n        definitions : dict\n            Dictionary of concept definitions.\n\n        Returns\n        -------\n        list\n            Merged definitions with overlaps combined.\n        \"\"\"\n        def parse_time(stringed_time:str):\n            \"\"\"\n            Converts time from string to int\n            \"\"\"\n            #            h                    :              mm            :             ss          .            dddddd\n            return int(stringed_time[0])*3600 + int(stringed_time[2:4])*60 + int(stringed_time[5:7]) + float(\"0\"+stringed_time[7:])\n\n        to_remove_indexes = []\n        for i,j,elem1,elem2 in double_iterator(definitions,enumerated=True):\n            if not i in to_remove_indexes \\\n            and not j in to_remove_indexes \\\n            and elem1[\"concept\"] == elem2[\"concept\"] \\\n            and elem1[\"description_type\"] == elem2[\"description_type\"] \\\n            and parse_time(elem1[\"start\"]) &lt; parse_time(elem2[\"end\"]) \\\n            and parse_time(elem2[\"start\"]) &lt;= parse_time(elem1[\"end\"]):\n                elem1[\"end\"] = elem2[\"end\"]\n                elem1[\"end_sent_id\"] = elem2[\"end_sent_id\"]\n                to_remove_indexes.append(j)\n\n        return delete(definitions,to_remove_indexes).tolist()\n\n    @staticmethod\n    def to_edgelist(df):\n        \"\"\"\n        Converts a DataFrame to a sorted list of weighted edges.\n\n        Parameters\n        ----------\n        df : pandas.DataFrame\n            DataFrame containing adjacency matrix.\n\n        Returns\n        -------\n        list of tuple\n            List of tuples (source, target, weight) sorted by weight descending.\n\n        Examples\n        --------\n        Example edge list format:\n            [(source1, target1, weight1), \n             (source2, target2, weight2),\n             ...]\n        \"\"\"\n        edges = []\n        for i in df.index.tolist():\n            for c in df.columns.tolist():\n                edges.append((i, c, df.loc[i][c]))\n\n        edges_sorted = sorted(edges, key=lambda edges: edges[2], reverse=True)\n        return edges_sorted\n\n\n    def launch_burst_analysis(self):\n        \"\"\"\n        Run complete burst analysis pipeline.\n\n        Executes four main phases:\n        1. Burst extraction using Kleinberg's algorithm\n        2. Relation detection between bursts\n        3. Weight normalization\n        4. Directionality assignment\n\n        Returns\n        -------\n        tuple\n            (concept_map, merged_definitions) containing analysis results\n\n        Raises\n        ------\n        ValueError\n            If parameters produce no results\n        \"\"\"\n        print(\"***** EKEEL - Video Annotation: burst_class.py::launch_burst_analysis() ******\")\n        try:\n            # FIRST PHASE: extract bursts\n            #print(\"Extracting bursts...\\n\")\n\n            #print(\"text\")\n            #print(self.text)\n            #print(\"words\")\n            #print(self.words)\n            #print(\"occurrences\")\n            #print(self.occurrences[0:50])\n\n            burst_extr = BurstExtractor(text=self.text, wordlist=self.words)\n            burst_extr.find_offsets(words=self.words, occ_index_file=self.occurrences)\n            burst_extr.generate_bursts(s=self.S, gamma=self.GAMMA)\n            burst_extr.filter_bursts(level=self.LEVEL, save_monolevel_keywords=True, replace_original_results=True)\n            burst_extr.break_bursts(burst_length=30, num_occurrences=3, replace_original_results=True)\n            burst_res = burst_extr.bursts\n\n            if burst_res.empty:\n                raise ValueError(\"The chosen parameters do not produce results\")\n\n            # obtain json with first, last, ongoing, unique tags\n            # bursts_json = burst_proc.get_json_with_bursts(burst_res, self.occurrences)\n\n\n\n            # SECOND PHASE: detect relations between bursts and assign weights to them\n            #print(\"Detecting Allen's relations and assign weights to burst pairs...\\n\")\n            weight_assigner = WeightAssigner(bursts=burst_res,\n                                             relations_weights=self.ALLEN_WEIGHTS)\n            weight_assigner.detect_relations(max_gap=self.MAX_GAP, alpha=0.05, find_also_inverse=self.USE_INVERSES)\n            # output data for the gantt interface and ML projects\n            burst_pairs_df = weight_assigner.burst_pairs\n\n            bursts_weights = weight_assigner.bursts_weights\n\n\n            # THIRD PHASE: normalize the bursts' weights\n            #print(\"Normalizing the matrix with weights of burst pairs...\\n\")\n            weight_norm = WeightsNormalizer(bursts=burst_res,\n                                            burst_pairs=burst_pairs_df,\n                                            burst_weight_matrix=bursts_weights)\n            weight_norm.normalize(formula=self.NORM_FORMULA, occ_index_file=self.occurrences)\n\n            burst_norm = weight_norm.burst_norm.round(decimals=6)\n\n\n            # FINAL STEP: give directionality to bursts\n            #print(\"Giving directionality to the concept matrix built with bursts...\\n\")\n\n            directed_burst = burst_proc.give_direction_using_first_burst(undirected_matrix=burst_norm,\n                                                                         bursts_results=burst_res,\n                                                                         indexes=self.occurrences,\n                                                                         level=self.LEVEL, preserve_relations=self.PRESERVE_RELATIONS)\n\n            # add rows and columns in the matrices for possible discarded terms\n            #print(\"\\nAdding rows and columns for missing concepts in the burst matrix...\\n\")\n            missing_terms = [term for term in self.words\n                             if term not in directed_burst.index]\n\n            for term in missing_terms:\n                directed_burst.loc[term] = 0\n                directed_burst[term] = 0\n\n            #print(\"Shape of final directed burst matrix:\", directed_burst.shape)\n\n            # get an edgelist with the extracted prerequisite relations\n            #print(\"Getting an edgelist with the extracted prerequisite relations...\\n\")\n            sorted_edgelist = pd.DataFrame(self.to_edgelist(directed_burst),\n                                           columns=[\"prerequisite\", \"target\", \"weight\"])\n\n            concept_map, definitions = self.df_to_data(sorted_edgelist, burst_res, use_conll=True)\n            return concept_map, self._merge_contained_definitions(definitions)\n\n\n        except ValueError as e:\n            print(\"error:\", sys.exc_info())\n            #self.updateStatus(\"failed\")\n            raise e\n\n\n    def df_to_data(self, sorted_edgelist: pd.DataFrame, burst_res: pd.DataFrame, use_conll: bool = False) -&gt; tuple[list, list]:\n        \"\"\"\n        Convert analysis results to concept maps.\n\n        Parameters\n        ----------\n        sorted_edgelist : pandas.DataFrame\n            Sorted edges with prerequisites and targets\n        burst_res : pandas.DataFrame  \n            Burst detection results\n        use_conll : bool, optional\n            Whether to use CoNLL tokenization\n\n        Returns\n        -------\n        tuple\n            (concept_map, definitions) containing relationship data\n        \"\"\"\n        print(\"***** EKEEL - Video Annotation: burst_class.py::df_to_data() ******\")\n        concept_map = []\n        definitions = []\n\n        video = VideoAnalyzer(\"https://www.youtube.com/watch?v=\"+self.video_id,{\"transcript_data\"})\n        sem_text= SemanticText(self.text, video.data[\"language\"])\n\n        if not use_conll:\n            sentences = sem_text.tokenize()\n        else:\n            sentences = [sent.metadata[\"text\"] for sent in self.conll]\n\n        timed_sentences = get_timed_sentences(video.data[\"transcript_data\"][\"text\"], sentences)\n\n        if self.top_n is not None:\n            sorted_edgelist = sorted_edgelist.head(self.top_n)\n\n        for rel in sorted_edgelist.itertuples():\n            if self.threshold &lt; rel.weight:\n\n                if self.first_occurence[rel.prerequisite] &gt; self.first_occurence[rel.target]:\n                    sent_id = self.first_occurence[rel.prerequisite]\n                else:\n                    sent_id = self.first_occurence[rel.target]\n\n                a = {\"prerequisite\": rel.prerequisite,\n                     \"target\": rel.target,\n                     \"creator\": \"Burst_Analysis\",\n                     \"weight\": \"Strong\",\n                     \"time\": str(datetime.timedelta(seconds=timed_sentences[sent_id][\"start\"])),\n                     \"sent_id\": sent_id,\n                     \"xywh\": \"None\",\n                     \"word_id\": \"None\",\n                     \"weight_burst\":rel.weight\n                     }\n                if a not in concept_map:\n                    concept_map.append(a)\n\n        concept_longest_burst = {}\n\n        for d in burst_res.itertuples():\n            burst_len = d.end - d.start\n            if d.keyword not in concept_longest_burst:\n                concept_longest_burst[d.keyword] = burst_len\n\n            elif burst_len &gt; concept_longest_burst[d.keyword]:\n                concept_longest_burst[d.keyword] = burst_len\n\n        for d in burst_res.itertuples():\n            if d.end - d.start &gt; 1:\n\n                if concept_longest_burst[d.keyword] == d.end - d.start:\n                    descr_type = \"Definition\"\n                else:\n                    descr_type = \"In Depth\"\n\n                definitions.append({\n                    \"concept\": d.keyword,\n                    \"start_sent_id\": d.start,\n                    \"end_sent_id\": d.end,\n                    \"start\": str(datetime.timedelta(seconds=timed_sentences[d.start][\"start\"])),\n                    \"end\": str(datetime.timedelta(seconds=timed_sentences[d.end][\"end\"])),\n                    \"description_type\": descr_type,\n                    \"creator\": \"Burst_Analysis\"\n                })\n\n        #_,jsonld = create_burst_graph(self.video_id,definitions, concept_map)\n\n        return concept_map, definitions\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.Burst.__init__","title":"<code>__init__(text, words, video_id, conll, syn_map=False, threshold=0, top_n=None, s=1.05, gamma=0.0001, level=1, allen_weights=None, use_inverses=False, max_gap=4, norm_formula='modified')</code>","text":"<p>Initialize burst analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to analyze</p> required <code>words</code> <code>list</code> <p>Words to detect bursts for</p> required <code>video_id</code> <code>str</code> <p>Video identifier</p> required <code>conll</code> <code>str</code> <p>CoNLL-U formatted text</p> required <code>syn_map</code> <code>dict or bool</code> <p>Synonym mapping dictionary</p> <code>False</code> <code>threshold</code> <code>float</code> <p>Burst detection threshold</p> <code>0</code> <code>top_n</code> <code>int</code> <p>Number of top bursts to consider</p> <code>None</code> <code>s</code> <code>float</code> <p>Base of exponential distribution</p> <code>1.05</code> <code>gamma</code> <code>(float, optional)</code> <p>Cost coefficient between states</p> <code>0.0001</code> <code>level</code> <code>int</code> <p>Burst level threshold</p> <code>1</code> <code>allen_weights</code> <code>dict</code> <p>Custom weights for Allen relations</p> <code>None</code> <code>use_inverses</code> <code>bool</code> <p>Whether to use inverse relations</p> <code>False</code> <code>max_gap</code> <code>int</code> <p>Maximum gap between bursts</p> <code>4</code> <code>norm_formula</code> <code>str</code> <p>Formula for normalization</p> <code>'modified'</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def __init__(self, text, words, video_id, conll, syn_map=False, threshold=0, \n             top_n=None, s=1.05, gamma=0.0001, level=1, allen_weights=None,\n             use_inverses=False, max_gap=4, norm_formula=\"modified\"):\n    \"\"\"\n    Initialize burst analyzer.\n\n    Parameters\n    ----------\n    text : str\n        Text to analyze\n    words : list\n        Words to detect bursts for\n    video_id : str\n        Video identifier\n    conll : str\n        CoNLL-U formatted text\n    syn_map : dict or bool, optional\n        Synonym mapping dictionary\n    threshold : float, optional\n        Burst detection threshold\n    top_n : int, optional\n        Number of top bursts to consider\n    s : float, optional\n        Base of exponential distribution\n    gamma : float, optional  \n        Cost coefficient between states\n    level : int, optional\n        Burst level threshold\n    allen_weights : dict, optional\n        Custom weights for Allen relations\n    use_inverses : bool, optional\n        Whether to use inverse relations\n    max_gap : int, optional\n        Maximum gap between bursts\n    norm_formula : str, optional\n        Formula for normalization\n    \"\"\"\n    self.text = text\n    self.words = words\n\n    self.conll = parse(conll)\n    self.video_id = video_id\n    self.threshold = threshold\n    self.top_n = top_n\n    self.data = {}\n\n\n    #Kleinberg's parameters\n    self.S = s\n    self.GAMMA = gamma\n    self.LEVEL = level\n\n    # #self.occurences\n    # #Dataframe contentente le colonne \"Lemma\", \"idFrase\", \"idParolaStart\"\n    occurrences_index = []\n    self.first_occurence = {}\n\n    # PHASE 0\n\n    max_word_lenght = 0\n\n    for w in self.words:\n        l = len(w.split(\" \"))\n        if l &gt; max_word_lenght:\n            max_word_lenght = l\n\n    upper_words = [x.upper() for x in self.words]\n    lemmatizer = WordNetLemmatizer()\n\n\n    for sent in self.conll:\n        sent_index = int(sent.metadata[\"sent_id\"])-1\n        conll_words = self.conll[sent_index].filter(upos=lambda x: x != \"PUNCT\")\n        #from pprint import pprint\n        #pprint(self.conll)\n        counter = 0\n        skip = 0\n\n        for i_, word in enumerate(conll_words):\n            #print(word, word[\"id\"], conll_words[i_+1],conll_words[i_+1][\"id\"], conll_words[i_+2],conll_words[i_+2][\"id\"])\n            if isinstance(word[\"id\"], tuple): skip += 2\n            elif skip &gt; 0: skip -= 1; continue\n            counter += 1\n            #word_index = int(word[\"id\"]) # TODO fix\n            word_index = counter\n            words = word[\"lemma\"]\n            words_form = word[\"form\"]\n            nltk_lemmatized = lemmatizer.lemmatize(word[\"form\"])\n\n            for i in range(1, max_word_lenght+1):\n\n                occ_words = \"\"\n                # cerco occorrenza della parola nella forma base, lemmatizata con la conll e lematizzata con nltk\n                if words.upper() in upper_words:\n                    occ_words = words.lower()\n                elif words_form.upper() in upper_words:\n                    occ_words = words_form.lower()\n                elif nltk_lemmatized.upper() in upper_words:\n                    occ_words = nltk_lemmatized\n\n                if occ_words != \"\":\n                    if occ_words not in self.first_occurence:\n                        self.first_occurence[occ_words] = sent_index\n\n                    d = [occ_words, sent_index, word_index]\n                    occurrences_index.append(d)\n\n                if i + i_ &lt; len(conll_words):\n                    words += \" \" + conll_words[i_ + i][\"lemma\"]\n                    words_form += \" \" + conll_words[i_ + i][\"form\"]\n                    nltk_lemmatized += \" \" + lemmatizer.lemmatize(conll_words[i_ + i][\"form\"])\n                else:\n                    break\n\n\n    if syn_map == False:\n        self.occurrences = pd.DataFrame(data=occurrences_index, columns=[\"Lemma\", \"idFrase\", \"idParolaStart\"])\n    else:\n        occur = pd.DataFrame(data=occurrences_index, columns=[\"Lemma\", \"idFrase\", \"idParolaStart\"])\n        new_occur = []\n        for o in occur.itertuples(): \n            new_occur.append([syn_map[o.Lemma], o.idFrase, o.idParolaStart])\n        self.occurrences = pd.DataFrame(new_occur, columns=['Lemma', 'idFrase', 'idParolaStart'])\n\n\n    # weights for Allen and type of normalization formula\n    if allen_weights is None:\n        self.ALLEN_WEIGHTS = {'equals': 2, 'before': 5, 'after': 0, 'meets': 3, 'met-by': 0,\n                         'overlaps': 7, 'overlapped-by': 1, 'during': 7, 'includes': 7,\n                         'starts': 4, 'started-by': 2, 'finishes': 2, 'finished-by': 8}\n    else:\n        self.ALLEN_WEIGHTS = allen_weights\n\n\n    self.USE_INVERSES = use_inverses\n    self.MAX_GAP = max_gap\n    self.NORM_FORMULA = norm_formula\n\n    # decide if preserve relations when giving direction to the burst matrix\n    self.PRESERVE_RELATIONS = True\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.Burst.df_to_data","title":"<code>df_to_data(sorted_edgelist, burst_res, use_conll=False)</code>","text":"<p>Convert analysis results to concept maps.</p> <p>Parameters:</p> Name Type Description Default <code>sorted_edgelist</code> <code>DataFrame</code> <p>Sorted edges with prerequisites and targets</p> required <code>burst_res</code> <code>DataFrame</code> <p>Burst detection results</p> required <code>use_conll</code> <code>bool</code> <p>Whether to use CoNLL tokenization</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(concept_map, definitions) containing relationship data</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def df_to_data(self, sorted_edgelist: pd.DataFrame, burst_res: pd.DataFrame, use_conll: bool = False) -&gt; tuple[list, list]:\n    \"\"\"\n    Convert analysis results to concept maps.\n\n    Parameters\n    ----------\n    sorted_edgelist : pandas.DataFrame\n        Sorted edges with prerequisites and targets\n    burst_res : pandas.DataFrame  \n        Burst detection results\n    use_conll : bool, optional\n        Whether to use CoNLL tokenization\n\n    Returns\n    -------\n    tuple\n        (concept_map, definitions) containing relationship data\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: burst_class.py::df_to_data() ******\")\n    concept_map = []\n    definitions = []\n\n    video = VideoAnalyzer(\"https://www.youtube.com/watch?v=\"+self.video_id,{\"transcript_data\"})\n    sem_text= SemanticText(self.text, video.data[\"language\"])\n\n    if not use_conll:\n        sentences = sem_text.tokenize()\n    else:\n        sentences = [sent.metadata[\"text\"] for sent in self.conll]\n\n    timed_sentences = get_timed_sentences(video.data[\"transcript_data\"][\"text\"], sentences)\n\n    if self.top_n is not None:\n        sorted_edgelist = sorted_edgelist.head(self.top_n)\n\n    for rel in sorted_edgelist.itertuples():\n        if self.threshold &lt; rel.weight:\n\n            if self.first_occurence[rel.prerequisite] &gt; self.first_occurence[rel.target]:\n                sent_id = self.first_occurence[rel.prerequisite]\n            else:\n                sent_id = self.first_occurence[rel.target]\n\n            a = {\"prerequisite\": rel.prerequisite,\n                 \"target\": rel.target,\n                 \"creator\": \"Burst_Analysis\",\n                 \"weight\": \"Strong\",\n                 \"time\": str(datetime.timedelta(seconds=timed_sentences[sent_id][\"start\"])),\n                 \"sent_id\": sent_id,\n                 \"xywh\": \"None\",\n                 \"word_id\": \"None\",\n                 \"weight_burst\":rel.weight\n                 }\n            if a not in concept_map:\n                concept_map.append(a)\n\n    concept_longest_burst = {}\n\n    for d in burst_res.itertuples():\n        burst_len = d.end - d.start\n        if d.keyword not in concept_longest_burst:\n            concept_longest_burst[d.keyword] = burst_len\n\n        elif burst_len &gt; concept_longest_burst[d.keyword]:\n            concept_longest_burst[d.keyword] = burst_len\n\n    for d in burst_res.itertuples():\n        if d.end - d.start &gt; 1:\n\n            if concept_longest_burst[d.keyword] == d.end - d.start:\n                descr_type = \"Definition\"\n            else:\n                descr_type = \"In Depth\"\n\n            definitions.append({\n                \"concept\": d.keyword,\n                \"start_sent_id\": d.start,\n                \"end_sent_id\": d.end,\n                \"start\": str(datetime.timedelta(seconds=timed_sentences[d.start][\"start\"])),\n                \"end\": str(datetime.timedelta(seconds=timed_sentences[d.end][\"end\"])),\n                \"description_type\": descr_type,\n                \"creator\": \"Burst_Analysis\"\n            })\n\n    #_,jsonld = create_burst_graph(self.video_id,definitions, concept_map)\n\n    return concept_map, definitions\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.Burst.launch_burst_analysis","title":"<code>launch_burst_analysis()</code>","text":"<p>Run complete burst analysis pipeline.</p> <p>Executes four main phases: 1. Burst extraction using Kleinberg's algorithm 2. Relation detection between bursts 3. Weight normalization 4. Directionality assignment</p> <p>Returns:</p> Type Description <code>tuple</code> <p>(concept_map, merged_definitions) containing analysis results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters produce no results</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def launch_burst_analysis(self):\n    \"\"\"\n    Run complete burst analysis pipeline.\n\n    Executes four main phases:\n    1. Burst extraction using Kleinberg's algorithm\n    2. Relation detection between bursts\n    3. Weight normalization\n    4. Directionality assignment\n\n    Returns\n    -------\n    tuple\n        (concept_map, merged_definitions) containing analysis results\n\n    Raises\n    ------\n    ValueError\n        If parameters produce no results\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: burst_class.py::launch_burst_analysis() ******\")\n    try:\n        # FIRST PHASE: extract bursts\n        #print(\"Extracting bursts...\\n\")\n\n        #print(\"text\")\n        #print(self.text)\n        #print(\"words\")\n        #print(self.words)\n        #print(\"occurrences\")\n        #print(self.occurrences[0:50])\n\n        burst_extr = BurstExtractor(text=self.text, wordlist=self.words)\n        burst_extr.find_offsets(words=self.words, occ_index_file=self.occurrences)\n        burst_extr.generate_bursts(s=self.S, gamma=self.GAMMA)\n        burst_extr.filter_bursts(level=self.LEVEL, save_monolevel_keywords=True, replace_original_results=True)\n        burst_extr.break_bursts(burst_length=30, num_occurrences=3, replace_original_results=True)\n        burst_res = burst_extr.bursts\n\n        if burst_res.empty:\n            raise ValueError(\"The chosen parameters do not produce results\")\n\n        # obtain json with first, last, ongoing, unique tags\n        # bursts_json = burst_proc.get_json_with_bursts(burst_res, self.occurrences)\n\n\n\n        # SECOND PHASE: detect relations between bursts and assign weights to them\n        #print(\"Detecting Allen's relations and assign weights to burst pairs...\\n\")\n        weight_assigner = WeightAssigner(bursts=burst_res,\n                                         relations_weights=self.ALLEN_WEIGHTS)\n        weight_assigner.detect_relations(max_gap=self.MAX_GAP, alpha=0.05, find_also_inverse=self.USE_INVERSES)\n        # output data for the gantt interface and ML projects\n        burst_pairs_df = weight_assigner.burst_pairs\n\n        bursts_weights = weight_assigner.bursts_weights\n\n\n        # THIRD PHASE: normalize the bursts' weights\n        #print(\"Normalizing the matrix with weights of burst pairs...\\n\")\n        weight_norm = WeightsNormalizer(bursts=burst_res,\n                                        burst_pairs=burst_pairs_df,\n                                        burst_weight_matrix=bursts_weights)\n        weight_norm.normalize(formula=self.NORM_FORMULA, occ_index_file=self.occurrences)\n\n        burst_norm = weight_norm.burst_norm.round(decimals=6)\n\n\n        # FINAL STEP: give directionality to bursts\n        #print(\"Giving directionality to the concept matrix built with bursts...\\n\")\n\n        directed_burst = burst_proc.give_direction_using_first_burst(undirected_matrix=burst_norm,\n                                                                     bursts_results=burst_res,\n                                                                     indexes=self.occurrences,\n                                                                     level=self.LEVEL, preserve_relations=self.PRESERVE_RELATIONS)\n\n        # add rows and columns in the matrices for possible discarded terms\n        #print(\"\\nAdding rows and columns for missing concepts in the burst matrix...\\n\")\n        missing_terms = [term for term in self.words\n                         if term not in directed_burst.index]\n\n        for term in missing_terms:\n            directed_burst.loc[term] = 0\n            directed_burst[term] = 0\n\n        #print(\"Shape of final directed burst matrix:\", directed_burst.shape)\n\n        # get an edgelist with the extracted prerequisite relations\n        #print(\"Getting an edgelist with the extracted prerequisite relations...\\n\")\n        sorted_edgelist = pd.DataFrame(self.to_edgelist(directed_burst),\n                                       columns=[\"prerequisite\", \"target\", \"weight\"])\n\n        concept_map, definitions = self.df_to_data(sorted_edgelist, burst_res, use_conll=True)\n        return concept_map, self._merge_contained_definitions(definitions)\n\n\n    except ValueError as e:\n        print(\"error:\", sys.exc_info())\n        #self.updateStatus(\"failed\")\n        raise e\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.Burst.to_edgelist","title":"<code>to_edgelist(df)</code>  <code>staticmethod</code>","text":"<p>Converts a DataFrame to a sorted list of weighted edges.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing adjacency matrix.</p> required <p>Returns:</p> Type Description <code>list of tuple</code> <p>List of tuples (source, target, weight) sorted by weight descending.</p> <p>Examples:</p> <p>Example edge list format:     [(source1, target1, weight1),       (source2, target2, weight2),      ...]</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>@staticmethod\ndef to_edgelist(df):\n    \"\"\"\n    Converts a DataFrame to a sorted list of weighted edges.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing adjacency matrix.\n\n    Returns\n    -------\n    list of tuple\n        List of tuples (source, target, weight) sorted by weight descending.\n\n    Examples\n    --------\n    Example edge list format:\n        [(source1, target1, weight1), \n         (source2, target2, weight2),\n         ...]\n    \"\"\"\n    edges = []\n    for i in df.index.tolist():\n        for c in df.columns.tolist():\n            edges.append((i, c, df.loc[i][c]))\n\n    edges_sorted = sorted(edges, key=lambda edges: edges[2], reverse=True)\n    return edges_sorted\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.burst_extraction","title":"<code>burst_extraction(video_id, concepts, n=90)</code>","text":"<p>Extract burst patterns from video transcripts.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video to analyze</p> required <code>concepts</code> <code>list</code> <p>List of concepts to detect bursts for</p> required <code>n</code> <code>int</code> <p>Number of top bursts to consider</p> <code>90</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(concept_map_burst, burst_definitions) containing burst analysis results</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def burst_extraction(video_id, concepts, n=90):\n    \"\"\"\n    Extract burst patterns from video transcripts.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video to analyze\n    concepts : list\n        List of concepts to detect bursts for\n    n : int, optional\n        Number of top bursts to consider\n\n    Returns\n    -------\n    tuple\n        (concept_map_burst, burst_definitions) containing burst analysis results\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: burst_class.py::burst_extraction(): Inizio ******\")\n\n    text, conll = get_text(video_id, return_conll=True)\n    text = text.replace(\"-\", \" \")\n\n\n    concept_map_burst, burst_definitions = Burst(text, concepts, video_id, conll, threshold=0.7,\n                                                 top_n=n, max_gap=1).launch_burst_analysis()\n\n    return concept_map_burst, burst_definitions\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.burst_extraction_with_synonyms","title":"<code>burst_extraction_with_synonyms(video_id, concepts, conceptVocabulary, n=90)</code>","text":"<p>Extracts burst patterns from video transcripts considering concept synonyms.</p> <p>This function extends burst_extraction by incorporating synonym relationships between concepts when analyzing the video transcript.</p> <p>:param video_id: Identifier for the video to analyze :type video_id: str :param concepts: List of concepts to detect bursts for :type concepts: list :param conceptVocabulary: Dictionary mapping concepts to their synonyms :type conceptVocabulary: dict :param n: Number of top bursts to consider, defaults to 90 :type n: int, optional</p> <p>:return: Tuple containing concept map bursts and burst definitions :rtype: tuple(dict, dict)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def burst_extraction_with_synonyms(video_id:str, concepts, conceptVocabulary, n=90):\n    \"\"\"\n    Extracts burst patterns from video transcripts considering concept synonyms.\n\n    This function extends burst_extraction by incorporating synonym relationships\n    between concepts when analyzing the video transcript.\n\n    :param video_id: Identifier for the video to analyze\n    :type video_id: str\n    :param concepts: List of concepts to detect bursts for\n    :type concepts: list\n    :param conceptVocabulary: Dictionary mapping concepts to their synonyms\n    :type conceptVocabulary: dict\n    :param n: Number of top bursts to consider, defaults to 90\n    :type n: int, optional\n\n    :return: Tuple containing concept map bursts and burst definitions\n    :rtype: tuple(dict, dict)\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: burst_class.py::burst_extraction_with_synonyms(): Inizio ******\")\n\n    text, conll = get_text(video_id, return_conll=True)\n    text = text.replace(\"-\", \" \")\n\n    syn_map, new_concepts = get_synonyms_mappings(conceptVocabulary)\n\n    concept_map_burst, burst_definitions = Burst(text, new_concepts, video_id, conll, syn_map, threshold=0.7,\n                                                 top_n=n, max_gap=1).launch_burst_analysis()\n\n    print(\"***** EKEEL - Video Annotation: burst_class.py::burst_extraction_with_synonyms(): Fine ******\")\n    return concept_map_burst, burst_definitions\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.compute_agreement_burst","title":"<code>compute_agreement_burst(concept_map1, concept_map2)</code>","text":"<p>Compute the agreement between two concept maps using burst analysis.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map1</code> <code>list of dict</code> <p>The first concept map, where each dict represents a relationship with 'prerequisite' and 'target' keys.</p> required <code>concept_map2</code> <code>list of dict</code> <p>The second concept map, where each dict represents a relationship with 'prerequisite' and 'target' keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the analysis type and the computed agreement score.</p> <p>Examples:</p> <p>Example of the returned dictionary format:     {         \"analysis_type\": \"agreement\",         \"agreement\": 0.85     }</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def compute_agreement_burst(concept_map1, concept_map2):\n    \"\"\"\n    Compute the agreement between two concept maps using burst analysis.\n\n    Parameters\n    ----------\n    concept_map1 : list of dict\n        The first concept map, where each dict represents a relationship with 'prerequisite' and 'target' keys.\n    concept_map2 : list of dict\n        The second concept map, where each dict represents a relationship with 'prerequisite' and 'target' keys.\n\n    Returns\n    -------\n    dict\n        A dictionary containing the analysis type and the computed agreement score.\n\n    Examples\n    --------\n    Example of the returned dictionary format:\n        {\n            \"analysis_type\": \"agreement\",\n            \"agreement\": 0.85\n        }\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: burst_class.py::compute_agreement_burst() ******\")\n\n    words = []\n    user1 = \"gold\"\n    user2 = \"burst\"\n\n    for rel in concept_map1:\n        if rel[\"prerequisite\"] not in words:\n            words.append(rel[\"prerequisite\"])\n\n        if rel[\"target\"] not in words:\n            words.append(rel[\"target\"])\n\n    for rel in concept_map2:\n        if rel[\"prerequisite\"] not in words:\n            words.append(rel[\"prerequisite\"])\n\n        if rel[\"target\"] not in words:\n            words.append(rel[\"target\"])\n\n    all_combs = agreement.createAllComb(words)\n\n    # Calcolo agreement kappa no-inv all paths\n    term_pairs = {user1: [], user2: []}\n    term_pairs_tuple = {user1: [], user2: []}\n    term_pairs[user1], all_combs, term_pairs_tuple[user1] = agreement.createUserRel(concept_map1, all_combs)\n    term_pairs[user2], all_combs, term_pairs_tuple[user2] = agreement.createUserRel(concept_map2, all_combs)\n\n    coppieannotate, conteggio = agreement.creaCoppieAnnot(user1, user2, term_pairs, all_combs, term_pairs_tuple)\n\n\n    results = {\"analysis_type\": \"agreement\",\n               \"agreement\":round(agreement.computeK(conteggio, all_combs), 3)}\n\n    return results\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.create_burst_graph","title":"<code>create_burst_graph(video_id, definitions, concept_map)</code>","text":"<p>Create a burst graph for a given video, definitions, and concept map.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <code>definitions</code> <code>list of dict</code> <p>List of definitions, where each dict contains details such as concept, start, end, start_sent_id, end_sent_id, creator, and description_type.</p> required <code>concept_map</code> <code>list of dict</code> <p>List of concept relationships, where each dict contains details such as prerequisite, target, weight, time, sent_id, and word_id.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the RDF graph and the JSON-LD representation of the graph.</p> <p>Examples:</p> <p>Example of the returned tuple format:     (Graph, jsonld)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def create_burst_graph(video_id,definitions,concept_map):\n    \"\"\"\n    Create a burst graph for a given video, definitions, and concept map.\n\n    Parameters\n    ----------\n    video_id : str\n        The ID of the video.\n    definitions : list of dict\n        List of definitions, where each dict contains details such as concept, start, end, start_sent_id, end_sent_id, creator, and description_type.\n    concept_map : list of dict\n        List of concept relationships, where each dict contains details such as prerequisite, target, weight, time, sent_id, and word_id.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the RDF graph and the JSON-LD representation of the graph.\n\n    Examples\n    --------\n    Example of the returned tuple format:\n        (Graph, jsonld)\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: burst_class.py::create_burst_graph(): Inizio ******\")\n\n    creator = URIRef(\"Burst_Analysis\")\n\n    g = Graph()\n\n    g.bind(\"oa\", oa)\n    g.bind(\"dctypes\", dctypes)\n    g.bind(\"edu\", edu)\n    g.bind(\"SKOS\", SKOS)\n    g.bind(\"dcterms\", dcterms)\n\n\n    video = URIRef(\"video_\" + str(video_id))\n    #video = URIRef(edurell + \"video_\" + str(video_id))\n    g.add((video, RDF.type, dctypes.MovingImage))\n\n    conll = URIRef(\"conll_\" + str(video_id))\n    #conll = URIRef(edurell + \"conll_\" + str(video_id))\n    g.add((conll, RDF.type, dctypes.Text))\n\n\n    # linking video conll\n    ann_linking_conll = URIRef(\"ann0\")\n    g.add((ann_linking_conll, RDF.type, oa.Annotation))\n    g.add((ann_linking_conll, oa.motivatedBy, edu.linkingConll))\n    g.add((ann_linking_conll, oa.hasBody, conll))\n    g.add((ann_linking_conll, oa.hasTarget, video))\n\n    date = Literal(datetime.datetime.fromtimestamp(time.time()))\n\n    #creo il nuovo nodo dei concetti\n    #localVocabulary = URIRef(\"localVocabulary\")\n    #g.add((localVocabulary, RDF.type, SKOS.Collection))\n\n\n    # add triples for every annotation\n    for i, annotation in enumerate(definitions):\n        ann = URIRef(\"ann\" + str(i + 1))\n\n        g.add((ann, RDF.type, oa.Annotation))\n\n        g.add((ann, dcterms.creator, creator))\n\n        g.add((ann, dcterms.created, date))\n        g.add((ann, oa.motivatedBy, oa.describing))\n        if annotation[\"description_type\"] == \"In Depth\":\n            g.add((ann, SKOS.note, Literal(\"conceptExpansion\")))\n        if annotation[\"description_type\"] == \"Definition\":\n            g.add((ann, SKOS.note, Literal(\"concept\"+annotation[\"description_type\"]))) \n\n        concept = URIRef(\"concept_\" + annotation[\"concept\"].replace(\" \", \"_\"))\n\n\n        #g.add((localVocabulary, SKOS.member, concept))\n        #g.add((concept, RDF.type, SKOS.Concept))\n\n\n        g.add((ann, oa.hasBody, concept))\n\n        blank_target = BNode()\n\n\n        blank_selector = BNode()\n\n        g.add((ann, oa.hasTarget, blank_target))\n        g.add((blank_target, RDF.type, oa.SpecificResource))\n\n        g.add((blank_target, oa.hasSelector, blank_selector))\n        g.add((blank_selector, RDF.type, oa.RangeSelector))\n\n        g.add((blank_target, oa.hasSource, video))\n\n        blank_startSelector = BNode()\n        blank_endSelector = BNode()\n\n        g.add((blank_startSelector, RDF.type, edu.InstantSelector))\n        g.add((blank_endSelector, RDF.type, edu.InstantSelector))\n\n        g.add((blank_selector, oa.hasStartSelector, blank_startSelector))\n        g.add((blank_selector, oa.hasEndSelector, blank_endSelector))\n\n        g.add((blank_startSelector, RDF.value, Literal(annotation[\"start\"] + \"^^xsd:dateTime\")))\n        g.add((blank_startSelector, edu.conllSentId, Literal(annotation[\"start_sent_id\"])))\n        #g.add((blank_startSelector, edu.conllWordId, Literal(annotation[\"word_id\"])))\n\n        g.add((blank_endSelector, RDF.value, Literal(annotation[\"end\"] + \"^^xsd:dateTime\")))\n        g.add((blank_endSelector, edu.conllSentId, Literal(annotation[\"end_sent_id\"])))\n\n\n    num_definitions = len(definitions) + 1\n\n    for i, annotation in enumerate(concept_map):\n        ann = URIRef(\"ann\" + str(num_definitions + i))\n\n        target_concept = URIRef(\"concept_\" +  annotation[\"target\"].replace(\" \", \"_\"))\n        prereq_concept = URIRef(\"concept_\" +  annotation[\"prerequisite\"].replace(\" \", \"_\"))\n\n        #g.add((target_concept, RDF.type, SKOS.Concept))\n        #g.add((prereq_concept, RDF.type, SKOS.Concept))\n\n        g.add((ann, RDF.type, oa.Annotation))\n\n        g.add((ann, dcterms.creator, creator))\n\n        g.add((ann, dcterms.created, date))\n        g.add((ann, oa.motivatedBy, edu.linkingPrerequisite))\n\n        g.add((ann, oa.hasBody, prereq_concept))\n        g.add((ann, SKOS.note, Literal(annotation[\"weight\"].lower() + \"Prerequisite\")))\n\n        blank_target = BNode()\n\n        g.add((ann, oa.hasTarget, blank_target))\n        g.add((blank_target, RDF.type, oa.SpecificResource))\n        g.add((blank_target, dcterms.subject, target_concept))\n\n        g.add((blank_target, oa.hasSource, video))\n\n        blank_selector_video = BNode()\n\n        g.add((blank_target, oa.hasSelector, blank_selector_video))\n        g.add((blank_selector_video, RDF.type, edu.InstantSelector))\n        g.add((blank_selector_video, RDF.value, Literal(annotation[\"time\"] + \"^^xsd:dateTime\")))\n\n        if annotation[\"xywh\"] != \"None\":\n            g.add((blank_selector_video, edu.hasMediaFrag, Literal(annotation[\"xywh\"])))\n\n\n        g.add((blank_selector_video, edu.conllSentId, Literal(annotation[\"sent_id\"])))\n\n        if annotation[\"word_id\"] != \"None\":\n            g.add((blank_selector_video, edu.conllWordId, Literal(annotation[\"word_id\"])))\n\n    context = [\"http://www.w3.org/ns/anno.jsonld\", {\n               \"@base\": \"https://edurell.dibris.unige.it/annotator/auto/\"+video_id+\"/\",\n      \t\t\t\"@version\": 1.1,\n      \t\t\t\"edu\": \"https://teldh.github.io/edurell#\"\n             } ]\n\n    jsonld = json.loads(g.serialize(format='json-ld'))\n    jsonld = pyld.jsonld.compact(jsonld, context)\n\n\n    for o in jsonld[\"@graph\"]:\n        if \"target\" in o:\n            for i, t in enumerate(jsonld[\"@graph\"]):\n                if o[\"motivation\"] != \"edu:linkingConll\" and o[\"target\"] == t[\"id\"]:\n                    o[\"target\"] = t\n                    del jsonld[\"@graph\"][i]\n                    for j, s in enumerate(jsonld[\"@graph\"]):\n                        if o[\"target\"][\"selector\"] == s[\"id\"]:\n                            o[\"target\"][\"selector\"] = s\n                            del jsonld[\"@graph\"][j]\n\n                            if o[\"motivation\"] == \"describing\":\n                                for k, p in enumerate(jsonld[\"@graph\"]):\n                                    if o[\"target\"][\"selector\"][\"startSelector\"] == p[\"id\"]:\n                                        o[\"target\"][\"selector\"][\"startSelector\"] = p\n                                        del jsonld[\"@graph\"][k]\n                                        break\n\n                                for k, p in enumerate(jsonld[\"@graph\"]):\n                                    if o[\"target\"][\"selector\"][\"endSelector\"] == p[\"id\"]:\n                                        o[\"target\"][\"selector\"][\"endSelector\"] = p\n                                        del jsonld[\"@graph\"][k]\n                                        break\n\n    # sort by \"id\": \"ann#\" value\n    jsonld[\"@graph\"] = sorted(jsonld[\"@graph\"],key=lambda x: int(x[\"id\"][3:]) if str(x[\"id\"][3:]).isnumeric() else 4242)\n    return g, jsonld\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.create_local_vocabulary","title":"<code>create_local_vocabulary(video_id, conceptVocabulary)</code>","text":"<p>Create a local vocabulary graph for a given video and concept vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <code>conceptVocabulary</code> <code>dict</code> <p>Dictionary of concepts and their synonyms.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the local vocabulary in JSON-LD format.</p> <p>Examples:</p> <p>Example of the returned dictionary format:     {         \"id\": \"localVocabulary\",         \"type\": \"skos:Collection\",         \"skos:member\": [             {                 \"@id\": \"concept_concept1\",                 \"@type\": \"skos:Concept\",                 \"skos:prefLabel\": {\"@value\": \"concept1\", \"@language\": \"en\"},                 \"skos:altLabel\": [{\"@value\": \"synonym1\", \"@language\": \"en\"}, ...]             },             ...         ]     }</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def create_local_vocabulary(video_id,conceptVocabulary):\n    \"\"\"\n    Create a local vocabulary graph for a given video and concept vocabulary.\n\n    Parameters\n    ----------\n    video_id : str\n        The ID of the video.\n    conceptVocabulary : dict\n        Dictionary of concepts and their synonyms.\n\n    Returns\n    -------\n    dict\n        A dictionary representing the local vocabulary in JSON-LD format.\n\n    Examples\n    --------\n    Example of the returned dictionary format:\n        {\n            \"id\": \"localVocabulary\",\n            \"type\": \"skos:Collection\",\n            \"skos:member\": [\n                {\n                    \"@id\": \"concept_concept1\",\n                    \"@type\": \"skos:Concept\",\n                    \"skos:prefLabel\": {\"@value\": \"concept1\", \"@language\": \"en\"},\n                    \"skos:altLabel\": [{\"@value\": \"synonym1\", \"@language\": \"en\"}, ...]\n                },\n                ...\n            ]\n        }\n    \"\"\"\n    context = [\"http://www.w3.org/ns/anno.jsonld\", {\n               \"@base\": \"https://edurell.dibris.unige.it/annotator/auto/\"+video_id+\"/\",\n      \t\t\t\"@version\": 1.1,\n      \t\t\t\"edu\": \"https://teldh.github.io/edurell#\"\n             } ]\n    language = get_video_data(video_id)[\"language\"]\n    graph = Graph()\n\n    for concept in conceptVocabulary.keys():        \n        uri_concept = URIRef(\"concept_\" + concept.replace(\" \", \"_\"))\n        graph.add((uri_concept, RDF['type'], SKOS.Concept))\n        graph.add((uri_concept, SKOS.prefLabel, Literal(concept, lang=language)))\n        for synonym in conceptVocabulary[concept]:\n            graph.add((uri_concept, SKOS.altLabel, Literal(synonym, lang=language)))\n\n    jsonld = json.loads(graph.serialize(format='json-ld'))\n    jsonld = pyld.jsonld.compact(jsonld, context)\n    print(jsonld)\n    local_vocabulary = {\"id\": \"localVocabulary\", \"type\": \"skos:Collection\"}\n    if \"@graph\" in jsonld.keys():\n        local_vocabulary[\"skos:member\"] = jsonld[\"@graph\"]\n    return local_vocabulary\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/prototype/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.prototype.get_synonyms_mappings","title":"<code>get_synonyms_mappings(conceptVocabulary)</code>","text":"<p>Create mappings between concepts and their synonyms.</p> <p>Parameters:</p> Name Type Description Default <code>conceptVocabulary</code> <code>dict</code> <p>Dictionary mapping concepts to their synonyms</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(syn_map, new_concepts) containing synonym mappings and unique concepts</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/prototype.py</code> <pre><code>def get_synonyms_mappings(conceptVocabulary:list):\n    \"\"\"\n    Create mappings between concepts and their synonyms.\n\n    Parameters\n    ----------\n    conceptVocabulary : dict\n        Dictionary mapping concepts to their synonyms\n\n    Returns\n    -------\n    tuple\n        (syn_map, new_concepts) containing synonym mappings and unique concepts\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: burst_class.py::get_synonyms_mappings(): Inizio ******\")\n\n    syn_map = {}\n    new_concepts = []\n\n    # get unique id for each syn set\n    for concept in conceptVocabulary:\n        synset = [concept]\n        synset = synset + conceptVocabulary[concept]\n        synset.sort()\n        syn_map[concept] = synset[0]\n        new_concepts.append(synset[0])\n\n    new_concepts = list(set(new_concepts))\n\n    print(\"***** EKEEL - Video Annotation: burst_class.py::get_synonyms_mappings(): Fine ******\")\n    return syn_map, new_concepts\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/results_processor/","title":"results_processor","text":""},{"location":"codebase/EKEELVideoAnnotation/burst/results_processor/#results-processor","title":"Results Processor","text":""},{"location":"codebase/EKEELVideoAnnotation/burst/results_processor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.results_processor.find_average_len","title":"<code>find_average_len(burst_results)</code>","text":"<p>Finds the average length of bursts of a concept.</p> <p>Parameters:</p> Name Type Description Default <code>burst_results</code> <code>DataFrame</code> <p>DataFrame with columns [keyword, level, start, end].</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with concepts associated to the average length of their bursts.</p> <p>Examples:</p> <p>Example of the returned dictionary format:     {         \"concept1\": 5.0,         \"concept2\": 3.5,         ...     }</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/results_processor.py</code> <pre><code>def find_average_len(burst_results) -&gt; dict:\n    \"\"\"\n    Finds the average length of bursts of a concept.\n\n    Parameters\n    ----------\n    burst_results : pandas.DataFrame\n        DataFrame with columns [keyword, level, start, end].\n\n    Returns\n    -------\n    dict\n        Dictionary with concepts associated to the average length of their bursts.\n\n    Examples\n    --------\n    Example of the returned dictionary format:\n        {\n            \"concept1\": 5.0,\n            \"concept2\": 3.5,\n            ...\n        }\n    \"\"\"\n    avg = {}\n\n    for t in burst_results[\"keyword\"].unique().tolist():\n\n        sub_df = burst_results.where(burst_results['keyword'] == t).dropna()\n        tot_len = 0\n\n        for i, r in sub_df.iterrows():\n            tot_len += (sub_df.loc[i][\"end\"] - sub_df.loc[i][\"start\"]) + 1\n\n        avg[t] = tot_len / sub_df.shape[0]\n\n    return avg\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/results_processor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.results_processor.find_first_longest","title":"<code>find_first_longest(burst_results, avg)</code>","text":"<p>Finds the first burst having a length that is higher than the average length of all bursts of that concept.</p> <p>Parameters:</p> Name Type Description Default <code>burst_results</code> <code>DataFrame</code> <p>DataFrame with columns [keyword, level, start, end].</p> required <code>avg</code> <code>dict</code> <p>Dictionary with concepts associated to the average length of their bursts.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with concepts associated with the id of the first longest burst.</p> <p>Examples:</p> <p>Example of the returned dictionary format:     {         \"concept1\": 0,         \"concept2\": 3,         ...     }</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/results_processor.py</code> <pre><code>def find_first_longest(burst_results, avg) -&gt; dict:\n    \"\"\"\n    Finds the first burst having a length that is higher than the average length of all bursts of that concept.\n\n    Parameters\n    ----------\n    burst_results : pandas.DataFrame\n        DataFrame with columns [keyword, level, start, end].\n    avg : dict\n        Dictionary with concepts associated to the average length of their bursts.\n\n    Returns\n    -------\n    dict\n        Dictionary with concepts associated with the id of the first longest burst.\n\n    Examples\n    --------\n    Example of the returned dictionary format:\n        {\n            \"concept1\": 0,\n            \"concept2\": 3,\n            ...\n        }\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: burst_results_processor.py::find_first_longest(): ******\")\n\n    \"\"\"\n    Finds the first burst having a length that is higher than the average length of all bursts of that concept.\n\n    :param bursts_results (pandas.DataFrame): df with these columns [keyword,level,start,end]\n    :param avg (dict): average length of bursts of a concept\n\n    :return: first_longest (dict): dictionary with concepts associated with the id of the first longest burst\n    \"\"\"\n    first_longest = {}\n\n    for t in burst_results[\"keyword\"].unique().tolist():\n\n        sub_df = burst_results.where(burst_results['keyword'] == t).dropna()\n\n        for i, r in sub_df.iterrows():\n            if sub_df.shape[0] == 1:\n                first_longest[t] = i\n            else:\n                curr_len = sub_df.loc[i][\"end\"] - sub_df.loc[i][\"start\"] + 1\n                if curr_len &gt; avg[t]:\n                    first_longest[t] = i\n                    break\n\n        if t not in first_longest:\n            first_longest[t] = sub_df[burst_results[\"end\"] - burst_results[\"start\"] + 1 == avg[t]].iloc[0].name\n\n    return first_longest\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/results_processor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.results_processor.get_json_with_bursts","title":"<code>get_json_with_bursts(burst_results, sents_idx)</code>","text":"<p>Gets a list of bursts with first/last/ongoing/unique tags that can be used for the Gantt interface.</p> <p>Parameters:</p> Name Type Description Default <code>burst_results</code> <code>DataFrame</code> <p>DataFrame with columns [keyword, level, start, end].</p> required <code>sents_idx</code> <code>DataFrame</code> <p>DataFrame containing the indexes of sentences where every concept occurs. It must have the following columns: \"Lemma\", \"idFrase\", \"idParolaStart\".</p> required <p>Returns:</p> Type Description <code>list of dict</code> <p>List of bursts with their details including start sentence, end sentence, concept, ID, frequency of term, and status.</p> <p>Examples:</p> <p>Example of the returned list format:     [         {\"startSent\": 0, \"endSent\": 9, \"concept\": \"computer\", \"ID\": 1, \"freqOfTerm\": 7, \"status\": \"FIRST\"},         {\"startSent\": 10, \"endSent\": 19, \"concept\": \"network\", \"ID\": 2, \"freqOfTerm\": 5, \"status\": \"ONGOING\"},         ...     ]</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/results_processor.py</code> <pre><code>def get_json_with_bursts(burst_results, sents_idx):\n    \"\"\"\n    Gets a list of bursts with first/last/ongoing/unique tags that can be used for the Gantt interface.\n\n    Parameters\n    ----------\n    burst_results : pandas.DataFrame\n        DataFrame with columns [keyword, level, start, end].\n    sents_idx : pandas.DataFrame\n        DataFrame containing the indexes of sentences where every concept occurs. It must have the following columns:\n        \"Lemma\", \"idFrase\", \"idParolaStart\".\n\n    Returns\n    -------\n    list of dict\n        List of bursts with their details including start sentence, end sentence, concept, ID, frequency of term, and status.\n\n    Examples\n    --------\n    Example of the returned list format:\n        [\n            {\"startSent\": 0, \"endSent\": 9, \"concept\": \"computer\", \"ID\": 1, \"freqOfTerm\": 7, \"status\": \"FIRST\"},\n            {\"startSent\": 10, \"endSent\": 19, \"concept\": \"network\", \"ID\": 2, \"freqOfTerm\": 5, \"status\": \"ONGOING\"},\n            ...\n        ]\n    \"\"\"\n    bursts_json = []\n\n    #sents_idx = pd.read_csv(occ_index_file, encoding=\"utf-8\", index_col=None, sep=\"\\t\",\n    #                      usecols=[\"Lemma\", \"idFrase\", \"idParolaStart\"])\n\n\n    # format: {\"startSent\": 0, \"endSent\": 9, \"concept\": \"computer\", \"ID\": 1, \"freqOfTerm\": 7, \"status\": \"FIRST\"}\n    for i, row in burst_results.iterrows():\n        curr_dict = {}\n        curr_dict[\"startSent\"] = int(row[\"start\"])\n        curr_dict[\"endSent\"] = int(row[\"end\"])\n        curr_dict[\"concept\"] = row[\"keyword\"]\n        curr_dict[\"ID\"] = int(i)\n\n        curr_dict[\"freqOfTerm\"] = sents_idx[(sents_idx[\"idFrase\"] &gt;= int(row[\"start\"])) &amp;\n                                            (sents_idx[\"idFrase\"] &lt;= int(row[\"end\"])) &amp;\n                                            (sents_idx[\"Lemma\"] == row[\"keyword\"])].shape[0]\n\n        if len(burst_results[burst_results[\"keyword\"] == row[\"keyword\"]][\"start\"]) == 1:\n            curr_dict[\"status\"] = \"UNIQUE\"\n        elif row[\"start\"] == burst_results[burst_results[\"keyword\"] == row[\"keyword\"]][\"start\"].min():\n            curr_dict[\"status\"] = \"FIRST\"\n        elif row[\"end\"] == burst_results[burst_results[\"keyword\"] == row[\"keyword\"]][\"end\"].max():\n            curr_dict[\"status\"] = \"LAST\"\n        else:\n            curr_dict[\"status\"] = \"ONGOING\"\n\n\n\n        bursts_json.append(curr_dict)\n\n    print(\"***** EKEEL - Video Annotation: burst_results_processor.py::get_json_with_bursts(): Fine ******\")\n\n\n    return bursts_json\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/results_processor/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.results_processor.give_direction_using_first_burst","title":"<code>give_direction_using_first_burst(undirected_matrix, bursts_results, indexes, level=1, preserve_relations=False)</code>","text":"<p>Give direction to an undirected matrix using the first burst of each concept.</p> <p>Parameters:</p> Name Type Description Default <code>undirected_matrix</code> <code>DataFrame</code> <p>DataFrame representing the undirected adjacency matrix.</p> required <code>bursts_results</code> <code>DataFrame</code> <p>DataFrame with columns [keyword, level, start, end].</p> required <code>indexes</code> <code>DataFrame</code> <p>DataFrame containing the indexes of sentences where every concept occurs. It must have the following columns: \"Lemma\", \"idFrase\", \"idParolaStart\".</p> required <code>level</code> <code>int</code> <p>The level of bursts to consider (default is 1).</p> <code>1</code> <code>preserve_relations</code> <code>bool</code> <p>If False, the weight in the \"wrong\" direction is killed and the weight in the right direction remains the same (potentially zero). If True, before the weight in the \"wrong\" direction is killed, the weight in the \"right\" direction is checked: if this is zero, it will be replaced with the weight of the wrong direction (and then the wrong is killed).</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representing the directed adjacency matrix.</p> <p>Examples:</p> <p>Example of the returned DataFrame format:     source  target  weight     concept1 concept2 0.5     concept2 concept3 0.7     ...</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/results_processor.py</code> <pre><code>def give_direction_using_first_burst(undirected_matrix: pd.DataFrame,\n                                     bursts_results: pd.DataFrame,\n                                     indexes,\n                                     level=1, preserve_relations=False) -&gt; pd.DataFrame:\n    \"\"\"\n    Give direction to an undirected matrix using the first burst of each concept.\n\n    Parameters\n    ----------\n    undirected_matrix : pandas.DataFrame\n        DataFrame representing the undirected adjacency matrix.\n    bursts_results : pandas.DataFrame\n        DataFrame with columns [keyword, level, start, end].\n    indexes : pandas.DataFrame\n        DataFrame containing the indexes of sentences where every concept occurs. It must have the following columns:\n        \"Lemma\", \"idFrase\", \"idParolaStart\".\n    level : int, optional\n        The level of bursts to consider (default is 1).\n    preserve_relations : bool, optional\n        If False, the weight in the \"wrong\" direction is killed and the weight in the right direction remains the same (potentially zero).\n        If True, before the weight in the \"wrong\" direction is killed, the weight in the \"right\" direction is checked: if this is zero,\n        it will be replaced with the weight of the wrong direction (and then the wrong is killed).\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame representing the directed adjacency matrix.\n\n    Examples\n    --------\n    Example of the returned DataFrame format:\n        source  target  weight\n        concept1 concept2 0.5\n        concept2 concept3 0.7\n        ...\n    \"\"\"\n    filtered_bursts = bursts_results.where(bursts_results['level'] == level).dropna()\n    #indexes = pd.read_csv(occ_index_file, encoding=\"utf-8\", index_col=0, sep=\"\\t\",\n    #                      usecols=[\"Lemma\", \"idFrase\", \"idParolaStart\"])\n\n    directed_df = undirected_matrix.copy()\n\n    for t1 in directed_df.index.tolist():\n\n        other_terms = directed_df.index.tolist()\n        other_terms.remove(t1)\n\n        for t2 in other_terms:\n\n            start_first_burst_t1 = filtered_bursts[filtered_bursts[\"keyword\"] == t1].iloc[0][\"start\"]\n            start_first_burst_t2 = filtered_bursts[filtered_bursts[\"keyword\"] == t2].iloc[0][\"start\"]\n\n            if start_first_burst_t1 &lt; start_first_burst_t2:\n                # t1 is a prereq of t2\n                if preserve_relations and directed_df.at[t1, t2] == 0:\n                    directed_df.at[t1, t2] = directed_df.at[t2, t1]\n                directed_df.at[t2, t1] = 0\n            elif start_first_burst_t2 &lt; start_first_burst_t1:\n                # t2 is a prereq of t1\n                if preserve_relations and directed_df.at[t2, t1] == 0:\n                    directed_df.at[t2, t1] = directed_df.at[t1, t2]\n                directed_df.at[t1, t2] = 0\n            elif start_first_burst_t2 == start_first_burst_t1:\n                # they are in the same sentence: need to check the tokens\n                #t1_pos_in_sent = indexes[indexes[\"idFrase\"] == start_first_burst_t1].loc[t1][\"idParolaStart\"].min()\n                #t2_pos_in_sent = indexes[indexes[\"idFrase\"] == start_first_burst_t1].loc[t2][\"idParolaStart\"].min()\n\n                t1_pos_in_sent = indexes[indexes[\"idFrase\"] == start_first_burst_t1].loc[indexes[\"Lemma\"] == t1][\"idParolaStart\"].min()\n                t2_pos_in_sent = indexes[indexes[\"idFrase\"] == start_first_burst_t1].loc[indexes[\"Lemma\"] == t2][\"idParolaStart\"].min()\n\n                if t1_pos_in_sent &lt; t2_pos_in_sent:\n                    # t1 is a prereq of t2\n                    if preserve_relations and directed_df.at[t1, t2] == 0:\n                        directed_df.at[t1, t2] = directed_df.at[t2, t1]\n                    directed_df.at[t2, t1] = 0\n                elif t2_pos_in_sent &lt; t1_pos_in_sent:\n                    # t2 is a prereq of t1\n                    if preserve_relations and directed_df.at[t2, t1] == 0:\n                        directed_df.at[t2, t1] = directed_df.at[t1, t2]\n                    directed_df.at[t1, t2] = 0\n                else:\n                    # the two concepts are an embedding concept and its nested concept:\n                    # use length to decide direction (the one with less words is the prerequisite)\n                    if len(t1.split()) &lt; len(t2.split()):\n                        directed_df.at[t2, t1] = 0\n                    elif len(t2.split()) &lt; len(t1.split()):\n                        directed_df.at[t1, t2] = 0\n                    # else:\n                    #     print(\"Impossible to give direction to this pair (not even by \"\n                    #           \"looking at the first sentence of their first burst):\", t1, \"\\t\", t2)\n            else:\n                print(\"Impossible to give direction to:\", t1, \"\\t&lt;-&gt;\\t\", t2)\n\n    directed_df = directed_df.round(decimals=3)\n\n    return directed_df\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/","title":"weight","text":""},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#weight","title":"Weight","text":""},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightAssigner","title":"<code>WeightAssigner</code>","text":"<p>A class to detect relations between bursts and assign weights to these relations based on Allen's algebra.</p> <p>Attributes:</p> Name Type Description <code>bursts</code> <code>DataFrame</code> <p>DataFrame containing the bursts. Every row represents a burst (with a unique ID as index). The DataFrame must have the columns: ['keyword', 'start', 'end'].</p> <code>relations_weights</code> <code>dict</code> <p>Dictionary that associates a weight to every relation in Allen's algebra. The key-set must contain all the relations' names (inverse relations included): 'equals', 'before', 'after', 'meets', 'met-by', 'overlaps', 'overlapped-by', 'during', 'includes', 'starts', 'started-by', 'finishes', 'finished-by'. If no dict is passed, the predefined weights will be used.</p> <code>text_filename</code> <code>(str, optional)</code> <p>The name of the file containing the book/chapter in plain text.</p> <code>burst_matrix</code> <code>DataFrame</code> <p>DataFrame consisting of a square matrix of burst weights (i.e., dimension = num_bursts x num_bursts). Rows and columns have as labels the IDs of the bursts.</p> <code>burst_pairs</code> <code>DataFrame</code> <p>DataFrame storing all the detected pairs of Allen-related bursts, in a suitable format for machine learning projects and Gantt interface. Columns are: ['x', 'y', 'Bx_id', 'By_id', 'Bx_start', 'Bx_end', 'By_start', 'By_end', 'Rel'].</p> <p>Methods:</p> Name Description <code>detect_relations</code> <p>Detect relations between bursts and assign weights to these relations.</p> <code>_initialize_dataframes</code> <p>Initialize the dataframes for burst matrix and burst pairs.</p> <code>_prop_tol_gap</code> <p>Propagate tolerance gap for burst relations.</p> <code>_store_weights</code> <p>Store weights for burst relations.</p> <code>_equals</code> <p>Detect 'equals' relation between bursts.</p> <code>_finishes</code> <p>Detect 'finishes' relation between bursts.</p> <code>_before</code> <p>Detect 'before' relation between bursts.</p> <code>_after</code> <p>Detect 'after' relation between bursts.</p> <code>_meets</code> <p>Detect 'meets' relation between bursts.</p> <code>_met_by</code> <p>Detect 'met-by' relation between bursts.</p> <code>_overlaps</code> <p>Detect 'overlaps' relation between bursts.</p> <code>_overlapped_by</code> <p>Detect 'overlapped-by' relation between bursts.</p> <code>_during</code> <p>Detect 'during' relation between bursts.</p> <code>_includes</code> <p>Detect 'includes' relation between bursts.</p> <code>_starts</code> <p>Detect 'starts' relation between bursts.</p> <code>_started_by</code> <p>Detect 'started-by' relation between bursts.</p> <p>Examples:</p> <p>weight_assigner = WeightAssigner(bursts=filtered_bursts,                                  relations_weights=rel_w,                                  text_filename=\"chapter4.txt\") weight_assigner.detect_relations() burst_pairs = weight_assigner.burst_pairs bursts_weights = weight_assigner.bursts_weights.dataframe</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/weight.py</code> <pre><code>class WeightAssigner:\n    \"\"\"\n    A class to detect relations between bursts and assign weights to these relations based on Allen's algebra.\n\n    Attributes\n    ----------\n    bursts : pandas.DataFrame\n        DataFrame containing the bursts. Every row represents a burst (with a unique ID as index).\n        The DataFrame must have the columns: ['keyword', 'start', 'end'].\n    relations_weights : dict\n        Dictionary that associates a weight to every relation in Allen's algebra.\n        The key-set must contain all the relations' names (inverse relations included):\n        'equals', 'before', 'after', 'meets', 'met-by', 'overlaps', 'overlapped-by',\n        'during', 'includes', 'starts', 'started-by', 'finishes', 'finished-by'.\n        If no dict is passed, the predefined weights will be used.\n    text_filename : str, optional\n        The name of the file containing the book/chapter in plain text.\n    burst_matrix : pandas.DataFrame\n        DataFrame consisting of a square matrix of burst weights (i.e., dimension = num_bursts x num_bursts).\n        Rows and columns have as labels the IDs of the bursts.\n    burst_pairs : pandas.DataFrame\n        DataFrame storing all the detected pairs of Allen-related bursts, in a suitable format for machine learning projects and Gantt interface.\n        Columns are: ['x', 'y', 'Bx_id', 'By_id', 'Bx_start', 'Bx_end', 'By_start', 'By_end', 'Rel'].\n\n    Methods\n    -------\n    detect_relations()\n        Detect relations between bursts and assign weights to these relations.\n    _initialize_dataframes()\n        Initialize the dataframes for burst matrix and burst pairs.\n    _prop_tol_gap()\n        Propagate tolerance gap for burst relations.\n    _store_weights()\n        Store weights for burst relations.\n    _equals()\n        Detect 'equals' relation between bursts.\n    _finishes()\n        Detect 'finishes' relation between bursts.\n    _before()\n        Detect 'before' relation between bursts.\n    _after()\n        Detect 'after' relation between bursts.\n    _meets()\n        Detect 'meets' relation between bursts.\n    _met_by()\n        Detect 'met-by' relation between bursts.\n    _overlaps()\n        Detect 'overlaps' relation between bursts.\n    _overlapped_by()\n        Detect 'overlapped-by' relation between bursts.\n    _during()\n        Detect 'during' relation between bursts.\n    _includes()\n        Detect 'includes' relation between bursts.\n    _starts()\n        Detect 'starts' relation between bursts.\n    _started_by()\n        Detect 'started-by' relation between bursts.\n\n    Examples\n    --------\n    weight_assigner = WeightAssigner(bursts=filtered_bursts,\n                                     relations_weights=rel_w,\n                                     text_filename=\"chapter4.txt\")\n    weight_assigner.detect_relations()\n    burst_pairs = weight_assigner.burst_pairs\n    bursts_weights = weight_assigner.bursts_weights.dataframe\n    \"\"\"\n\n    # predefined weights (they also include inverse relations)\n    RELATIONS_WEIGHTS = {'equals': 5, 'before': 2, 'after': 0, 'meets': 3, 'met-by': 0,\n                         'overlaps': 8, 'overlapped-by': 1, 'during': 7, 'includes': 7,\n                         'starts': 6, 'started-by': 2, 'finishes': 2, 'finished-by': 8}\n\n    def __init__(self, bursts: pd.DataFrame, relations_weights: dict = None):\n        \"\"\"\n        Initializes the object from a dataframe storing the concepts' bursts in a\n        text (this can be extracted by a BurstExtractor) and a dictionary containing\n        the weights for Allen's algebra's relations.\n\n        Parameters\n        ----------\n        bursts : pandas.DataFrame\n            A dataframe possibly generated by a BurstExtractor.\n        relations_weights : dict, optional\n            A dictionary containing weights associated to every possible relation.\n            If no dictionary is passed, the predefined weights will be used.\n\n        Returns\n        -------\n        None\n        \"\"\"\n\n        self._bursts = bursts\n        self._relations_weights = relations_weights or self.RELATIONS_WEIGHTS\n\n        # initialize the two final data structures\n        self._initialize_dataframes()\n\n    @classmethod\n    def from_burst_extractor(cls, fitted_burst_extractor: BurstExtractor,\n                             relations_weights: dict = None, level: int = 1):\n        \"\"\"\n        Initializes the object from a BurstExtractor object.\n\n        Parameters\n        ----------\n        fitted_burst_extractor : BurstExtractor\n            A BurstExtractor object with already computed bursts.\n        relations_weights : dict, optional\n            A dictionary containing weights associated to every possible relation.\n            If no dictionary is passed, the predefined weights will be used.\n        level : int, optional\n            The level of bursts to consider (default is 1).\n\n        Returns\n        -------\n        WeightAssigner\n            An instance of the WeightAssigner class.\n        \"\"\"\n\n        bursts = fitted_burst_extractor.filter_bursts(level)\n\n        return cls(bursts, relations_weights)\n\n    def _initialize_dataframes(self) -&gt; None:\n        \"\"\"\n        Initialize the final data structures as empty dataframes or reset them to empty.\n\n        Returns\n        -------\n        None\n        \"\"\"\n\n        # initialize the final square matrix of weights\n        self._burst_matrix = pd.DataFrame(0.0,\n                                          index=self._bursts.index.tolist(),\n                                          columns=self._bursts.index.tolist())\n\n        # initialize the dataset for the machine learning project and for gantt interface\n        self._burst_pairs = pd.DataFrame(columns=['x', 'y',\n                                                  'Bx_id', 'By_id',\n                                                  'Bx_start', 'Bx_end',\n                                                  'By_start', 'By_end',\n                                                  'Rel'])\n\n    def detect_relations(self, max_gap=10, alpha=0.05, find_also_inverse=False):\n        \"\"\"\n        Detects which relations exist between bursts, computes the weight according\n        to the relations_weights schema, and stores it in the final data structure.\n\n        Parameters\n        ----------\n        max_gap : int, optional\n            A maximum number of sentences between two bursts after which no relation will be assigned (default is 10).\n            It is used to reduce the number of 'before' and 'after' relations.\n        alpha : float, optional\n            Proportionality coefficient (default is 0.05). It is multiplied by the total length of the two bursts.\n        find_also_inverse : bool, optional\n            If False (default), only direct relations are detected. If True, the procedure will also\n            detect and assign weights to inverse relations of Allen's algebra.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        weight_assigner = WeightAssigner(bursts=filtered_bursts,\n                                         relations_weights=rel_w)\n        weight_assigner.detect_relations(max_gap=10, alpha=0.05, find_also_inverse=True)\n        burst_pairs = weight_assigner.burst_pairs\n        bursts_weights = weight_assigner.bursts_weights.dataframe\n        \"\"\"\n\n        # reset the two final dataframes\n        self._initialize_dataframes()\n\n        # loop over all the rows in the dataframe (i.e. over all the bursts)\n        for index1, row in self._bursts.iterrows():\n            word1 = row['keyword']\n            start1 = int(row['start'])\n            end1 = int(row['end'])\n\n            # among all the bursts, subsect only bursts that are not 'too before' or 'too after'\n            # (considering a max admissible gap)\n            # subsection = bursts.loc[(bursts['start']&lt;(end1+max_gap)) &amp; (bursts['end']&gt;(start1-max_gap))]\n\n            # consider only the bursts of words different from the current word\n            subsection = self._bursts.where(self._bursts['keyword'] != word1).dropna()\n\n            # loop over all the candidate bursts\n            for index2, row2 in subsection.iterrows():\n\n                word2 = row2['keyword']\n                start2 = int(row2['start'])\n                end2 = int(row2['end'])\n\n                # compute the specific tolerance gap\n                tol_gap = self._prop_tol_gap(start1, end1, start2, end2, alpha)\n\n                # check if there is a relationship and assign the weight\n\n                ### direct relations\n\n                # equals\n                if self._equals(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('equals', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # finishes\n                if self._finishes(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('finishes', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # starts\n                if self._starts(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('starts', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # includes\n                if self._includes(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('includes', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # meets\n                if self._meets(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('meets', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # overlaps\n                if self._overlaps(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('overlaps', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # before\n                if self._before(end1, start2, tol_gap, max_gap):\n                    self._store_weight('before', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                ### inverse relations\n\n                if find_also_inverse:\n\n                    # met-by\n                    if self._met_by(start1, start2, end2, tol_gap):\n                        self._store_weight('met-by', word1, word2, start1, end1,\n                                           start2, end2, index1, index2)\n\n                    # overlapped-by\n                    if self._overlapped_by(start1, end1, start2, end2, tol_gap):\n                        self._store_weight('overlapped-by', word1, word2, start1, end1,\n                                           start2, end2, index1, index2)\n\n                    # during\n                    if self._during(start1, end1, start2, end2, tol_gap):\n                        self._store_weight('during', word1, word2, start1, end1,\n                                           start2, end2, index1, index2)\n\n                    # started-by\n                    if self._started_by(start1, end1, start2, end2, tol_gap):\n                        self._store_weight('started-by', word1, word2, start1, end1,\n                                           start2, end2, index1, index2)\n\n                    # finished-by\n                    if self._finished_by(start1, end1, start2, end2, tol_gap):\n                        self._store_weight('finished-by', word1, word2, start1, end1,\n                                           start2, end2, index1, index2)\n\n                    # after\n                    if self._after(start1, end2, tol_gap, max_gap):\n                        self._store_weight('after', word1, word2, start1, end1,\n                                           start2, end2, index1, index2)\n\n\n    # HELPER METHODS FOR detect_relations: _prop_tol_gap; _store_weight\n\n    def _prop_tol_gap(self, start1, end1, start2, end2, alpha=0.05) -&gt; float:\n        \"\"\"\n        Returns a gap that is proportional to the lengths of two bursts.\n\n        Parameters\n        ----------\n        start1 : int\n            The start index of the first burst.\n        end1 : int\n            The end index of the first burst.\n        start2 : int\n            The start index of the second burst.\n        end2 : int\n            The end index of the second burst.\n        alpha : float, optional\n            Proportionality coefficient (default is 0.05). It is multiplied by the total length of the two bursts.\n\n        Returns\n        -------\n        float\n            The proportional tolerance gap.\n        \"\"\"\n\n        # add 1 because the last sentence is included in the burst\n        length1 = (end1 - start1) + 1\n        length2 = (end2 - start2) + 1\n\n        # compute the specific tol_gap for these two bursts\n        tol_gap = (length1 + length2) * alpha\n\n        return tol_gap\n\n    def _store_weight(self, relation: str, word1: str, word2: str, start1: int, end1: int,\n                      start2: int, end2: int, index1: int, index2: int):\n        \"\"\"\n        Store the weight of a detected relation between bursts.\n\n        Parameters\n        ----------\n        relation : str\n            The type of relation detected.\n        word1 : str\n            The keyword of the first burst.\n        word2 : str\n            The keyword of the second burst.\n        start1 : int\n            The start index of the first burst.\n        end1 : int\n            The end index of the first burst.\n        start2 : int\n            The start index of the second burst.\n        end2 : int\n            The end index of the second burst.\n        index1 : int\n            The index of the first burst in the dataframe.\n        index2 : int\n            The index of the second burst in the dataframe.\n\n        Returns\n        -------\n        None\n        \"\"\"\n\n        # append the relationship at the end of the dataframe of burst pairs\n        idx = self._burst_pairs.shape[0]\n        self._burst_pairs.loc[idx] = [word1, word2,\n                                      index1, index2,\n                                      start1, end1, start2, end2, relation]\n\n        # keep the weight in the final data structure only if it's greater than the currently stored weight\n        if self._relations_weights[relation] &gt; self._burst_matrix.at[index1, index2]:\n            # add weight in the matrix\n            self._burst_matrix.at[index1, index2] = self._relations_weights[relation]\n\n    # HELPER METHODS FOR DEFINING RULES\n\n    def _equals(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((abs(start1 - start2) &lt; tol_gap) &amp;\n                (abs(end1 - end2) &lt; tol_gap))\n\n    def _finishes(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((abs(start1 - start2) &gt; tol_gap) &amp;\n                (abs(end1 - end2) &lt; tol_gap) &amp;\n                (start1 &gt; start2) &amp;\n                (start1 &lt; end2))\n\n    def _starts(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((abs(start1 - start2) &lt; tol_gap) &amp;\n                (abs(end1 - end2) &gt; tol_gap) &amp;\n                (end1 &gt; start2) &amp; (end1 &lt; end2))\n\n    def _during(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start1 &gt; start2) &amp;\n                (end1 &lt; end2) &amp;\n                (abs(start1 - start2) &gt; tol_gap) &amp;\n                (abs(end1 - end2) &gt; tol_gap))\n\n    def _meets(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start1 &lt; start2) &amp;\n                (end1 &lt; end2) &amp;\n                (abs(end1 - start2) &lt; tol_gap))\n\n    def _overlaps(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start1 &lt; start2) &amp;\n                (end1 &gt; start2) &amp;\n                (abs(end1 - start2) &gt; tol_gap) &amp;\n                (end1 &lt; end2) &amp;\n                (abs(end2 - end1) &gt; tol_gap) &amp;\n                (abs(start2 - start1) &gt; tol_gap))\n\n    def _before(self, end1, start2, tol_gap, max_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start2 &gt; (end1 + tol_gap)) &amp;\n                ((start2 - end1) &lt;= max_gap))\n\n    def _met_by(self, start1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start1 &gt; start2) &amp;\n                (start1 &gt; end2) &amp;  # FIXME: anche se le inverse non sono state quasi mai usate, provare ad eliminare questa regola (non si verifica in alcuni casi)\n                (abs(start1 - end2) &lt; tol_gap))\n\n    def _overlapped_by(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start1 &gt; start2) &amp;\n                (start1 &lt; end2) &amp;\n                (abs(start1 - end2) &gt; tol_gap) &amp;\n                (abs(start1 - start2) &gt; tol_gap) &amp;\n                (end1 &gt; end2) &amp;\n                (abs(end1 - end2) &gt; tol_gap))\n\n    def _includes(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start1 &lt; start2) &amp;\n                (end1 &gt; end2) &amp;\n                (abs(start1 - start2) &gt; tol_gap) &amp;\n                (abs(end1 - end2) &gt; tol_gap))\n\n    def _started_by(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((end1 &gt; end2) &amp;\n                (start1 &lt; end2) &amp;\n                (abs(start1 - start2) &lt; tol_gap) &amp;\n                (abs(end1 - end2) &gt; tol_gap))\n\n    def _finished_by(self, start1, end1, start2, end2, tol_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start1 &lt; start2) &amp;\n                (end1 &gt; start2) &amp;\n                (abs(start1 - start2) &gt; tol_gap) &amp;\n                (abs(end1 - end2) &lt; tol_gap))\n\n    def _after(self, start1, end2, tol_gap, max_gap) -&gt; bool:\n        \"\"\"\"\"\"\n        return ((start1 &gt; (end2 + tol_gap)) &amp;\n                ((start1 - end2) &lt;= max_gap))\n\n    @property\n    def bursts(self):\n        \"\"\"Getter of the input dataframe containing the bursts.\"\"\"\n        return self._bursts\n\n    @property\n    def relations_weights(self):\n        \"\"\"Getter of the dictionary containing the weights for all relations in Allen's algebra\"\"\"\n        return self._relations_weights\n\n    @property\n    def bursts_weights(self):\n        \"\"\"Getter of the final dataframe containing the weights between all bursts.\"\"\"\n        return self._burst_matrix\n\n    @property\n    def burst_pairs(self):\n        \"\"\"Getter of the final dataframe containing pairs of related bursts in the format needed for machine learning project.\"\"\"\n        return self._burst_pairs\n\n    def __repr__(self):\n        return \"WeightAssigner(bursts={}, relations_weights={})\".format(\n            repr(self._bursts), repr(self._relations_weights))\n\n    def __str__(self):\n        return \"WeightAssigner object. Input bursts (only first 5 rows):\\n{}\\n,\\nrelations_weights:{}\".format(\n            repr(self._bursts.head()), repr(self._relations_weights))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightAssigner.burst_pairs","title":"<code>burst_pairs</code>  <code>property</code>","text":"<p>Getter of the final dataframe containing pairs of related bursts in the format needed for machine learning project.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightAssigner.bursts","title":"<code>bursts</code>  <code>property</code>","text":"<p>Getter of the input dataframe containing the bursts.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightAssigner.bursts_weights","title":"<code>bursts_weights</code>  <code>property</code>","text":"<p>Getter of the final dataframe containing the weights between all bursts.</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightAssigner.relations_weights","title":"<code>relations_weights</code>  <code>property</code>","text":"<p>Getter of the dictionary containing the weights for all relations in Allen's algebra</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightAssigner.__init__","title":"<code>__init__(bursts, relations_weights=None)</code>","text":"<p>Initializes the object from a dataframe storing the concepts' bursts in a text (this can be extracted by a BurstExtractor) and a dictionary containing the weights for Allen's algebra's relations.</p> <p>Parameters:</p> Name Type Description Default <code>bursts</code> <code>DataFrame</code> <p>A dataframe possibly generated by a BurstExtractor.</p> required <code>relations_weights</code> <code>dict</code> <p>A dictionary containing weights associated to every possible relation. If no dictionary is passed, the predefined weights will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/weight.py</code> <pre><code>def __init__(self, bursts: pd.DataFrame, relations_weights: dict = None):\n    \"\"\"\n    Initializes the object from a dataframe storing the concepts' bursts in a\n    text (this can be extracted by a BurstExtractor) and a dictionary containing\n    the weights for Allen's algebra's relations.\n\n    Parameters\n    ----------\n    bursts : pandas.DataFrame\n        A dataframe possibly generated by a BurstExtractor.\n    relations_weights : dict, optional\n        A dictionary containing weights associated to every possible relation.\n        If no dictionary is passed, the predefined weights will be used.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    self._bursts = bursts\n    self._relations_weights = relations_weights or self.RELATIONS_WEIGHTS\n\n    # initialize the two final data structures\n    self._initialize_dataframes()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightAssigner.detect_relations","title":"<code>detect_relations(max_gap=10, alpha=0.05, find_also_inverse=False)</code>","text":"<p>Detects which relations exist between bursts, computes the weight according to the relations_weights schema, and stores it in the final data structure.</p> <p>Parameters:</p> Name Type Description Default <code>max_gap</code> <code>int</code> <p>A maximum number of sentences between two bursts after which no relation will be assigned (default is 10). It is used to reduce the number of 'before' and 'after' relations.</p> <code>10</code> <code>alpha</code> <code>float</code> <p>Proportionality coefficient (default is 0.05). It is multiplied by the total length of the two bursts.</p> <code>0.05</code> <code>find_also_inverse</code> <code>bool</code> <p>If False (default), only direct relations are detected. If True, the procedure will also detect and assign weights to inverse relations of Allen's algebra.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>weight_assigner = WeightAssigner(bursts=filtered_bursts,                                  relations_weights=rel_w) weight_assigner.detect_relations(max_gap=10, alpha=0.05, find_also_inverse=True) burst_pairs = weight_assigner.burst_pairs bursts_weights = weight_assigner.bursts_weights.dataframe</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/weight.py</code> <pre><code>def detect_relations(self, max_gap=10, alpha=0.05, find_also_inverse=False):\n    \"\"\"\n    Detects which relations exist between bursts, computes the weight according\n    to the relations_weights schema, and stores it in the final data structure.\n\n    Parameters\n    ----------\n    max_gap : int, optional\n        A maximum number of sentences between two bursts after which no relation will be assigned (default is 10).\n        It is used to reduce the number of 'before' and 'after' relations.\n    alpha : float, optional\n        Proportionality coefficient (default is 0.05). It is multiplied by the total length of the two bursts.\n    find_also_inverse : bool, optional\n        If False (default), only direct relations are detected. If True, the procedure will also\n        detect and assign weights to inverse relations of Allen's algebra.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    weight_assigner = WeightAssigner(bursts=filtered_bursts,\n                                     relations_weights=rel_w)\n    weight_assigner.detect_relations(max_gap=10, alpha=0.05, find_also_inverse=True)\n    burst_pairs = weight_assigner.burst_pairs\n    bursts_weights = weight_assigner.bursts_weights.dataframe\n    \"\"\"\n\n    # reset the two final dataframes\n    self._initialize_dataframes()\n\n    # loop over all the rows in the dataframe (i.e. over all the bursts)\n    for index1, row in self._bursts.iterrows():\n        word1 = row['keyword']\n        start1 = int(row['start'])\n        end1 = int(row['end'])\n\n        # among all the bursts, subsect only bursts that are not 'too before' or 'too after'\n        # (considering a max admissible gap)\n        # subsection = bursts.loc[(bursts['start']&lt;(end1+max_gap)) &amp; (bursts['end']&gt;(start1-max_gap))]\n\n        # consider only the bursts of words different from the current word\n        subsection = self._bursts.where(self._bursts['keyword'] != word1).dropna()\n\n        # loop over all the candidate bursts\n        for index2, row2 in subsection.iterrows():\n\n            word2 = row2['keyword']\n            start2 = int(row2['start'])\n            end2 = int(row2['end'])\n\n            # compute the specific tolerance gap\n            tol_gap = self._prop_tol_gap(start1, end1, start2, end2, alpha)\n\n            # check if there is a relationship and assign the weight\n\n            ### direct relations\n\n            # equals\n            if self._equals(start1, end1, start2, end2, tol_gap):\n                self._store_weight('equals', word1, word2, start1, end1,\n                                   start2, end2, index1, index2)\n\n            # finishes\n            if self._finishes(start1, end1, start2, end2, tol_gap):\n                self._store_weight('finishes', word1, word2, start1, end1,\n                                   start2, end2, index1, index2)\n\n            # starts\n            if self._starts(start1, end1, start2, end2, tol_gap):\n                self._store_weight('starts', word1, word2, start1, end1,\n                                   start2, end2, index1, index2)\n\n            # includes\n            if self._includes(start1, end1, start2, end2, tol_gap):\n                self._store_weight('includes', word1, word2, start1, end1,\n                                   start2, end2, index1, index2)\n\n            # meets\n            if self._meets(start1, end1, start2, end2, tol_gap):\n                self._store_weight('meets', word1, word2, start1, end1,\n                                   start2, end2, index1, index2)\n\n            # overlaps\n            if self._overlaps(start1, end1, start2, end2, tol_gap):\n                self._store_weight('overlaps', word1, word2, start1, end1,\n                                   start2, end2, index1, index2)\n\n            # before\n            if self._before(end1, start2, tol_gap, max_gap):\n                self._store_weight('before', word1, word2, start1, end1,\n                                   start2, end2, index1, index2)\n\n            ### inverse relations\n\n            if find_also_inverse:\n\n                # met-by\n                if self._met_by(start1, start2, end2, tol_gap):\n                    self._store_weight('met-by', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # overlapped-by\n                if self._overlapped_by(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('overlapped-by', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # during\n                if self._during(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('during', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # started-by\n                if self._started_by(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('started-by', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # finished-by\n                if self._finished_by(start1, end1, start2, end2, tol_gap):\n                    self._store_weight('finished-by', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n\n                # after\n                if self._after(start1, end2, tol_gap, max_gap):\n                    self._store_weight('after', word1, word2, start1, end1,\n                                       start2, end2, index1, index2)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightAssigner.from_burst_extractor","title":"<code>from_burst_extractor(fitted_burst_extractor, relations_weights=None, level=1)</code>  <code>classmethod</code>","text":"<p>Initializes the object from a BurstExtractor object.</p> <p>Parameters:</p> Name Type Description Default <code>fitted_burst_extractor</code> <code>BurstExtractor</code> <p>A BurstExtractor object with already computed bursts.</p> required <code>relations_weights</code> <code>dict</code> <p>A dictionary containing weights associated to every possible relation. If no dictionary is passed, the predefined weights will be used.</p> <code>None</code> <code>level</code> <code>int</code> <p>The level of bursts to consider (default is 1).</p> <code>1</code> <p>Returns:</p> Type Description <code>WeightAssigner</code> <p>An instance of the WeightAssigner class.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/weight.py</code> <pre><code>@classmethod\ndef from_burst_extractor(cls, fitted_burst_extractor: BurstExtractor,\n                         relations_weights: dict = None, level: int = 1):\n    \"\"\"\n    Initializes the object from a BurstExtractor object.\n\n    Parameters\n    ----------\n    fitted_burst_extractor : BurstExtractor\n        A BurstExtractor object with already computed bursts.\n    relations_weights : dict, optional\n        A dictionary containing weights associated to every possible relation.\n        If no dictionary is passed, the predefined weights will be used.\n    level : int, optional\n        The level of bursts to consider (default is 1).\n\n    Returns\n    -------\n    WeightAssigner\n        An instance of the WeightAssigner class.\n    \"\"\"\n\n    bursts = fitted_burst_extractor.filter_bursts(level)\n\n    return cls(bursts, relations_weights)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightsNormalizer","title":"<code>WeightsNormalizer</code>","text":"<p>Creates objects that compute the Normalized Relation Weight (NRW) for a given burst matrix.</p> <p>Attributes:</p> Name Type Description <code>bursts</code> <code>DataFrame</code> <p>DataFrame containing a burst for each row.</p> <code>burst_pairs</code> <code>DataFrame</code> <p>DataFrame containing pairs of related bursts.</p> <code>burst_weight_matrix</code> <code>DataFrame</code> <p>DataFrame constructed from the weight-matrix generated from BurstAssigner.</p> <code>burst_norm</code> <code>DataFrame</code> <p>DataFrame with size: num_concepts x num_concepts, initialized with zeros.</p> <p>Methods:</p> Name Description <code>normalize</code> <p>Normalize the burst weight matrix using the specified formula.</p> <code>_total_length</code> <p>Compute the total length of bursts for a given concept.</p> <code>_word_frequency</code> <p>Compute the frequency of a concept in the text.</p> <p>Examples:</p> <p>weight_norm = WeightsNormalizer(burst_results=filtered_bursts,                                 burst_pairs=burst_pairs,                                 burst_weight_matrix=burst_weight_matrix) weight_norm.normalize(formula='original') normalized_weights = weight_norm.burst_norm.dataframe normalized_weights = normalized_weights.round(decimals=3)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/weight.py</code> <pre><code>class WeightsNormalizer:\n    \"\"\"\n    Creates objects that compute the Normalized Relation Weight (NRW) for a given burst matrix.\n\n    Attributes\n    ----------\n    bursts : pandas.DataFrame\n        DataFrame containing a burst for each row.\n    burst_pairs : pandas.DataFrame\n        DataFrame containing pairs of related bursts.\n    burst_weight_matrix : pandas.DataFrame\n        DataFrame constructed from the weight-matrix generated from BurstAssigner.\n    burst_norm : pandas.DataFrame\n        DataFrame with size: num_concepts x num_concepts, initialized with zeros.\n\n    Methods\n    -------\n    normalize(formula='original')\n        Normalize the burst weight matrix using the specified formula.\n    _total_length(concept)\n        Compute the total length of bursts for a given concept.\n    _word_frequency(concept)\n        Compute the frequency of a concept in the text.\n\n    Examples\n    --------\n    weight_norm = WeightsNormalizer(burst_results=filtered_bursts,\n                                    burst_pairs=burst_pairs,\n                                    burst_weight_matrix=burst_weight_matrix)\n    weight_norm.normalize(formula='original')\n    normalized_weights = weight_norm.burst_norm.dataframe\n    normalized_weights = normalized_weights.round(decimals=3)\n    \"\"\"\n\n    def __init__(self, bursts: pd.DataFrame, burst_pairs: pd.DataFrame, burst_weight_matrix: pd.DataFrame):\n        \"\"\"\n        Initialize the WeightsNormalizer object.\n\n        Parameters\n        ----------\n        bursts : pandas.DataFrame\n            DataFrame containing a burst for each row.\n        burst_pairs : pandas.DataFrame\n            DataFrame containing pairs of related bursts.\n        burst_weight_matrix : pandas.DataFrame\n            DataFrame constructed from the weight-matrix generated from BurstAssigner.\n        \"\"\"\n\n        self._bursts = bursts.copy()\n        self._burst_pairs = burst_pairs.copy()\n        self._burst_weight_matrix = burst_weight_matrix.copy()\n\n        # prepare a dataset with size: num_concepts x num_concepts\n        self._burst_norm = pd.DataFrame(0.0,\n                                        index=self._bursts['keyword'].unique(),\n                                        columns=self._bursts['keyword'].unique())\n\n        # TODO SE2020: potenzialmente da eliminare in PRET sempre per le stesse ragioni (serve per trovare le frequenze nel testo se non vengono forniti in input dati relativi all'analisi linguistica come quelli contenuti nel conll)\n        self._text_filename = \"\"\n\n    def normalize(self, formula='original', occ_index_file: str=None):\n        \"\"\"\n        Normalize the burst weight matrix using the specified formula.\n\n        Parameters\n        ----------\n        formula : str, optional\n            Type of normalizing formula. Possible values: 'original', 'modified', 'marzo2019_1', 'marzo2019_2'.\n            Default is 'original'.\n        occ_index_file : str, optional\n            Path to the file containing the occurrence index with the following columns:\n            \"Lemma\", \"idFrase\", \"idParolaStart\".\n            If no value is passed, frequencies will be computed using NLTK text processing tools.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        weight_norm = WeightsNormalizer(burst_results=filtered_bursts,\n                                        burst_pairs=burst_pairs,\n                                        burst_weight_matrix=burst_weight_matrix)\n        weight_norm.normalize(formula='original')\n        normalized_weights = weight_norm.burst_norm.dataframe\n        normalized_weights = normalized_weights.round(decimals=3)\n        \"\"\"\n        if formula not in ['original', 'modified', 'marzo2019_1', 'marzo2019_2']:\n            raise ValueError(\"Error: the argument 'formula' must be either 'original' or 'modified'.\")\n\n        # reset dataframe to zeros if any value has been modified during a first call of the function\n        if self._burst_norm.ne(0.0).any().any():\n            for col in self._burst_norm.columns:\n                self._burst_norm[col] = 0.0\n\n        # precompute the word frequencies in their bursts\n        for burst_id in self._bursts.index:\n            start = self._bursts.at[burst_id, 'start']\n            end = self._bursts.at[burst_id, 'end']\n            freq = self._word_frequency(self._bursts.at[burst_id, 'keyword'], start, end,\n                                        occ_index_file)\n            # add the freq in a column of the same dataframe\n            self._bursts.at[burst_id, 'word freq'] = freq\n\n        # precompute the total length of bursts of each word\n        tot_burst_len = {}\n        for word in self._bursts['keyword'].unique():\n            tot_burst_len[word] = self._total_length(word)\n\n        # main body of the method: compare bursts and assign a normalized weight\n\n        # for each word X\n        for word_X in self._bursts['keyword'].unique():\n\n            # retrieve the list of indexes in the bursts df assigned to the bursts of word X\n            bursts_X_indexes = self._bursts.where(self._bursts['keyword'] == word_X).dropna().index.tolist()\n\n            # for each burst of word X\n            for burst_X_i in bursts_X_indexes:\n                # don't consider the current burst if the entire row is zero\n                if (self._burst_weight_matrix.loc[burst_X_i] == 0).all():\n                    pass\n\n                other_words = self._bursts['keyword'].unique().tolist()\n                other_words.remove(word_X)\n\n                # for each word different from word X\n                for word_Y in other_words:\n                    # retrieve the list of indexes in the df assigned to the bursts of word Y\n                    bursts_Y_indexes = self._bursts.where(self._bursts['keyword'] == word_Y).dropna().index.tolist()\n\n                    # for each burst of this second word Y\n                    for burst_Y_j in bursts_Y_indexes:\n                        # don't consider the current burst if the entire column is zero\n                        if (self._burst_weight_matrix[burst_Y_j] == 0).all():\n                            pass\n\n                        # if the two bursts are related:\n                        if self._burst_weight_matrix.at[burst_X_i, burst_Y_j] &gt; 0:\n                            # retrieve the weight and freqs\n                            relation_weight_BX = self._burst_weight_matrix.at[burst_X_i, burst_Y_j]\n                            freq_BX = self._bursts.at[burst_X_i, 'word freq']\n                            freq_BY = self._bursts.at[burst_Y_j, 'word freq']\n\n                            # compute NRW using the chosen formula\n                            if formula == 'original':\n                                NRW = (relation_weight_BX * (freq_BX / tot_burst_len[word_X]) *\n                                       (freq_BY / tot_burst_len[word_Y]))\n                                \"\"\"\n                                # per dare pi\u00f9 peso a i pesi rispetto a lunghezze e frequenze\n                                NRW = relation_weight_BX * ( (freq_BX / tot_burst_len[word_X]) +\n                                       (freq_BY / tot_burst_len[word_Y])) \n\n                                # per esaltare le parole meno frequenti\n                                NRW = relation_weight_BX * ( (tot_burst_len[word_X] / freq_BX) +\n                                       (tot_burst_len[word_Y] / freq_BY)) \n                                \"\"\"\n                            elif formula == 'modified':\n                                # find total number of bursts of these words\n                                # TODO: (OPTIM) move it outside the loop\n                                num_bursts_X = self._bursts.where(self._bursts['keyword'] == word_X).dropna().shape[0]\n                                num_bursts_Y = self._bursts.where(self._bursts['keyword'] == word_Y).dropna().shape[0]\n                                NRW = relation_weight_BX * ((freq_BX * num_bursts_X) / tot_burst_len[word_X]) * (\n                                            (freq_BY * num_bursts_Y) / tot_burst_len[word_Y])\n\n                            elif formula == 'marzo2019_1':\n                                # freq(Y, Bj) / length of the single burst of Y under examination (i.e. BYj)\n                                BYj_len = self._single_burst_length(burst_Y_j)\n                                NRW = relation_weight_BX * (freq_BX / tot_burst_len[word_X]) * (freq_BY / BYj_len)\n\n                            elif formula == 'marzo2019_2':\n                                # similar to the previous but also for BXi\n                                BXi_len = self._single_burst_length(burst_X_i)\n                                BYj_len = self._single_burst_length(burst_Y_j)\n                                NRW = relation_weight_BX * (freq_BX / BXi_len) * (freq_BY / BYj_len)\n\n\n                            # update the final matrix\n                            # (i.e. sum the NRW between the current burst of word X\n                            # and its related burst of word Y to the already stored weight between word X and word Y)\n                            self._burst_norm.at[word_X, word_Y] += NRW\n\n    def _total_length(self, keyword):\n        \"\"\"\n        Normalize the burst weight matrix using the specified formula.\n\n        Parameters\n        ----------\n        formula : str, optional\n            Type of normalizing formula. Possible values: 'original', 'modified', 'marzo2019_1', 'marzo2019_2'.\n            Default is 'original'.\n        occ_index_file : str, optional\n            Path to the file containing the occurrence index with the following columns:\n            \"Lemma\", \"idFrase\", \"idParolaStart\".\n            If no value is passed, frequencies will be computed using NLTK text processing tools.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        weight_norm = WeightsNormalizer(burst_results=filtered_bursts,\n                                        burst_pairs=burst_pairs,\n                                        burst_weight_matrix=burst_weight_matrix)\n        weight_norm.normalize(formula='original')\n        normalized_weights = weight_norm.burst_norm.dataframe\n        normalized_weights = normalized_weights.round(decimals=3)\n        \"\"\"\n        tot_len = 0\n\n        sub_df = self._bursts.where(self._bursts['keyword'] == keyword).dropna()\n\n        for i, r in sub_df.iterrows():\n            tot_len += (sub_df.loc[i][\"end\"] - sub_df.loc[i][\"start\"]) + 1\n\n        #tot_len = (self._burst_results.where(self._burst_results['keyword'] == keyword).sum()['end'] -\n        #           self._burst_results.where(self._burst_results['keyword'] == keyword).sum()['start'])\n\n        return tot_len\n\n    def _single_burst_length(self, burst_id):\n        \"\"\"\n        Compute the length of a single burst.\n\n        Parameters\n        ----------\n        burst_id : int\n            The ID of the burst.\n\n        Returns\n        -------\n        int\n            The length of the burst.\n        \"\"\"\n\n        sub_df = self._bursts.loc[burst_id]\n        length = sub_df['end'] - sub_df['start'] + 1\n\n        return length\n\n    def _total_length_related(self, x, y):\n        \"\"\"\n        Finds the total length of bursts of y that have a relation with some burst of x.\n\n        Parameters\n        ----------\n        x : str\n            The keyword for the first concept.\n        y : str\n            The keyword for the second concept.\n\n        Returns\n        -------\n        int\n            The total length of related bursts.\n        \"\"\"\n\n        sub_df = self._burst_pairs[(self._burst_pairs[\"x\"] == x) &amp; (self._burst_pairs[\"y\"] == y)]\n        # delete duplicate bursts of Y\n        sub_df = sub_df.drop_duplicates(['By_start', 'By_end'])\n        length = sub_df['By_end'].sum() - sub_df['By_start'].sum() + sub_df.shape[0]\n\n        return length\n\n    #def _word_frequency(self, keyword, start, end, occ_index_file: str=None):\n    def _word_frequency(self, keyword, start, end, sents_idx):\n        \"\"\"\n        Finds the frequency of a keyword in the portion of text between the limits of a burst.\n\n        Parameters\n        ----------\n        keyword : str\n            The keyword for which to compute the frequency.\n        start : int\n            The start index of the burst.\n        end : int\n            The end index of the burst.\n        sents_idx : pandas.DataFrame\n            DataFrame containing the indexes of sentences where every concept occurs. It must have the following columns:\n            \"Lemma\", \"idFrase\", \"idParolaStart\".\n\n        Returns\n        -------\n        int\n            The frequency of the keyword in the specified portion of text.\n        \"\"\"\n\n        freq = 0\n\n        #if occ_index_file is not None:\n        if sents_idx is not None:\n            # use the occurrences provided in the index file to compute frequencies\n            '''sents_idx = pd.read_csv(occ_index_file,\n                                    index_col=0,\n                                    usecols=[\"Lemma\", \"idFrase\", \"idParolaStart\"],\n                                    encoding=\"utf-8\", sep=\"\\t\")'''\n            # TODO: improve readability using .loc[[keyword]] that always returns a dataframe and thus avoid problems with .shape[0]\n            #if type(sents_idx.loc[keyword]) == pd.Series:\n            if type(sents_idx.loc[sents_idx['Lemma'] == keyword]) == pd.Series:\n                # there is only one occurrence\n                freq = 1\n            else:\n                occs_in_burst = sents_idx.loc[sents_idx['Lemma'] == keyword][(sents_idx.loc[sents_idx['Lemma'] == keyword].idFrase &gt;= start) &amp;\n                                                   (sents_idx.loc[sents_idx['Lemma'] == keyword].idFrase &lt;= end)]\n                freq = occs_in_burst.shape[0]\n        '''else:\n            # occurrences are not provided: use NLTK to compute frequencies\n            # TODO SE2020: questa parte non dovrebbe servire in PRET\n            with open(self._text_filename, 'r', encoding='utf-8') as text:\n                splitted_text = nltk.sent_tokenize(str(text.read()))\n\n                for sent in splitted_text[start:end + 1]:\n                    freq += sent.upper().count(keyword.upper())'''\n\n        return freq\n\n\n    @property\n    def burst_results(self):\n        \"\"\"Getter\"\"\"\n        return self._bursts\n\n    @property\n    def burst_weight_matrix(self):\n        \"\"\"Getter\"\"\"\n        return self._burst_weight_matrix\n\n    @property\n    def burst_norm(self):\n        \"\"\"Getter of the final dataframe with normalized weights\"\"\"\n        return self._burst_norm\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightsNormalizer.burst_norm","title":"<code>burst_norm</code>  <code>property</code>","text":"<p>Getter of the final dataframe with normalized weights</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightsNormalizer.burst_results","title":"<code>burst_results</code>  <code>property</code>","text":"<p>Getter</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightsNormalizer.burst_weight_matrix","title":"<code>burst_weight_matrix</code>  <code>property</code>","text":"<p>Getter</p>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightsNormalizer.__init__","title":"<code>__init__(bursts, burst_pairs, burst_weight_matrix)</code>","text":"<p>Initialize the WeightsNormalizer object.</p> <p>Parameters:</p> Name Type Description Default <code>bursts</code> <code>DataFrame</code> <p>DataFrame containing a burst for each row.</p> required <code>burst_pairs</code> <code>DataFrame</code> <p>DataFrame containing pairs of related bursts.</p> required <code>burst_weight_matrix</code> <code>DataFrame</code> <p>DataFrame constructed from the weight-matrix generated from BurstAssigner.</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/weight.py</code> <pre><code>def __init__(self, bursts: pd.DataFrame, burst_pairs: pd.DataFrame, burst_weight_matrix: pd.DataFrame):\n    \"\"\"\n    Initialize the WeightsNormalizer object.\n\n    Parameters\n    ----------\n    bursts : pandas.DataFrame\n        DataFrame containing a burst for each row.\n    burst_pairs : pandas.DataFrame\n        DataFrame containing pairs of related bursts.\n    burst_weight_matrix : pandas.DataFrame\n        DataFrame constructed from the weight-matrix generated from BurstAssigner.\n    \"\"\"\n\n    self._bursts = bursts.copy()\n    self._burst_pairs = burst_pairs.copy()\n    self._burst_weight_matrix = burst_weight_matrix.copy()\n\n    # prepare a dataset with size: num_concepts x num_concepts\n    self._burst_norm = pd.DataFrame(0.0,\n                                    index=self._bursts['keyword'].unique(),\n                                    columns=self._bursts['keyword'].unique())\n\n    # TODO SE2020: potenzialmente da eliminare in PRET sempre per le stesse ragioni (serve per trovare le frequenze nel testo se non vengono forniti in input dati relativi all'analisi linguistica come quelli contenuti nel conll)\n    self._text_filename = \"\"\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/burst/weight/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.burst.weight.WeightsNormalizer.normalize","title":"<code>normalize(formula='original', occ_index_file=None)</code>","text":"<p>Normalize the burst weight matrix using the specified formula.</p> <p>Parameters:</p> Name Type Description Default <code>formula</code> <code>str</code> <p>Type of normalizing formula. Possible values: 'original', 'modified', 'marzo2019_1', 'marzo2019_2'. Default is 'original'.</p> <code>'original'</code> <code>occ_index_file</code> <code>str</code> <p>Path to the file containing the occurrence index with the following columns: \"Lemma\", \"idFrase\", \"idParolaStart\". If no value is passed, frequencies will be computed using NLTK text processing tools.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>weight_norm = WeightsNormalizer(burst_results=filtered_bursts,                                 burst_pairs=burst_pairs,                                 burst_weight_matrix=burst_weight_matrix) weight_norm.normalize(formula='original') normalized_weights = weight_norm.burst_norm.dataframe normalized_weights = normalized_weights.round(decimals=3)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/burst/weight.py</code> <pre><code>def normalize(self, formula='original', occ_index_file: str=None):\n    \"\"\"\n    Normalize the burst weight matrix using the specified formula.\n\n    Parameters\n    ----------\n    formula : str, optional\n        Type of normalizing formula. Possible values: 'original', 'modified', 'marzo2019_1', 'marzo2019_2'.\n        Default is 'original'.\n    occ_index_file : str, optional\n        Path to the file containing the occurrence index with the following columns:\n        \"Lemma\", \"idFrase\", \"idParolaStart\".\n        If no value is passed, frequencies will be computed using NLTK text processing tools.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    weight_norm = WeightsNormalizer(burst_results=filtered_bursts,\n                                    burst_pairs=burst_pairs,\n                                    burst_weight_matrix=burst_weight_matrix)\n    weight_norm.normalize(formula='original')\n    normalized_weights = weight_norm.burst_norm.dataframe\n    normalized_weights = normalized_weights.round(decimals=3)\n    \"\"\"\n    if formula not in ['original', 'modified', 'marzo2019_1', 'marzo2019_2']:\n        raise ValueError(\"Error: the argument 'formula' must be either 'original' or 'modified'.\")\n\n    # reset dataframe to zeros if any value has been modified during a first call of the function\n    if self._burst_norm.ne(0.0).any().any():\n        for col in self._burst_norm.columns:\n            self._burst_norm[col] = 0.0\n\n    # precompute the word frequencies in their bursts\n    for burst_id in self._bursts.index:\n        start = self._bursts.at[burst_id, 'start']\n        end = self._bursts.at[burst_id, 'end']\n        freq = self._word_frequency(self._bursts.at[burst_id, 'keyword'], start, end,\n                                    occ_index_file)\n        # add the freq in a column of the same dataframe\n        self._bursts.at[burst_id, 'word freq'] = freq\n\n    # precompute the total length of bursts of each word\n    tot_burst_len = {}\n    for word in self._bursts['keyword'].unique():\n        tot_burst_len[word] = self._total_length(word)\n\n    # main body of the method: compare bursts and assign a normalized weight\n\n    # for each word X\n    for word_X in self._bursts['keyword'].unique():\n\n        # retrieve the list of indexes in the bursts df assigned to the bursts of word X\n        bursts_X_indexes = self._bursts.where(self._bursts['keyword'] == word_X).dropna().index.tolist()\n\n        # for each burst of word X\n        for burst_X_i in bursts_X_indexes:\n            # don't consider the current burst if the entire row is zero\n            if (self._burst_weight_matrix.loc[burst_X_i] == 0).all():\n                pass\n\n            other_words = self._bursts['keyword'].unique().tolist()\n            other_words.remove(word_X)\n\n            # for each word different from word X\n            for word_Y in other_words:\n                # retrieve the list of indexes in the df assigned to the bursts of word Y\n                bursts_Y_indexes = self._bursts.where(self._bursts['keyword'] == word_Y).dropna().index.tolist()\n\n                # for each burst of this second word Y\n                for burst_Y_j in bursts_Y_indexes:\n                    # don't consider the current burst if the entire column is zero\n                    if (self._burst_weight_matrix[burst_Y_j] == 0).all():\n                        pass\n\n                    # if the two bursts are related:\n                    if self._burst_weight_matrix.at[burst_X_i, burst_Y_j] &gt; 0:\n                        # retrieve the weight and freqs\n                        relation_weight_BX = self._burst_weight_matrix.at[burst_X_i, burst_Y_j]\n                        freq_BX = self._bursts.at[burst_X_i, 'word freq']\n                        freq_BY = self._bursts.at[burst_Y_j, 'word freq']\n\n                        # compute NRW using the chosen formula\n                        if formula == 'original':\n                            NRW = (relation_weight_BX * (freq_BX / tot_burst_len[word_X]) *\n                                   (freq_BY / tot_burst_len[word_Y]))\n                            \"\"\"\n                            # per dare pi\u00f9 peso a i pesi rispetto a lunghezze e frequenze\n                            NRW = relation_weight_BX * ( (freq_BX / tot_burst_len[word_X]) +\n                                   (freq_BY / tot_burst_len[word_Y])) \n\n                            # per esaltare le parole meno frequenti\n                            NRW = relation_weight_BX * ( (tot_burst_len[word_X] / freq_BX) +\n                                   (tot_burst_len[word_Y] / freq_BY)) \n                            \"\"\"\n                        elif formula == 'modified':\n                            # find total number of bursts of these words\n                            # TODO: (OPTIM) move it outside the loop\n                            num_bursts_X = self._bursts.where(self._bursts['keyword'] == word_X).dropna().shape[0]\n                            num_bursts_Y = self._bursts.where(self._bursts['keyword'] == word_Y).dropna().shape[0]\n                            NRW = relation_weight_BX * ((freq_BX * num_bursts_X) / tot_burst_len[word_X]) * (\n                                        (freq_BY * num_bursts_Y) / tot_burst_len[word_Y])\n\n                        elif formula == 'marzo2019_1':\n                            # freq(Y, Bj) / length of the single burst of Y under examination (i.e. BYj)\n                            BYj_len = self._single_burst_length(burst_Y_j)\n                            NRW = relation_weight_BX * (freq_BX / tot_burst_len[word_X]) * (freq_BY / BYj_len)\n\n                        elif formula == 'marzo2019_2':\n                            # similar to the previous but also for BXi\n                            BXi_len = self._single_burst_length(burst_X_i)\n                            BYj_len = self._single_burst_length(burst_Y_j)\n                            NRW = relation_weight_BX * (freq_BX / BXi_len) * (freq_BY / BYj_len)\n\n\n                        # update the final matrix\n                        # (i.e. sum the NRW between the current burst of word X\n                        # and its related burst of word Y to the already stored weight between word X and word Y)\n                        self._burst_norm.at[word_X, word_Y] += NRW\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/","title":"mongo","text":""},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#mongo","title":"Mongo","text":"<p>MongoDB database interface module.</p> <p>This module provides functions for interacting with the MongoDB database, handling user data, annotations, videos, and concept maps.</p>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_annotation_status","title":"<code>get_annotation_status(annotator, video_id)</code>","text":"<p>Get annotation completion status.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>Annotator identifier</p> required <code>video_id</code> <code>str</code> <p>Video identifier</p> required <p>Returns:</p> Type Description <code>dict or None</code> <p>Annotation completion status if found</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_annotation_status(annotator, video_id):\n    \"\"\"\n    Get annotation completion status.\n\n    Parameters\n    ----------\n    annotator : str\n        Annotator identifier  \n    video_id : str\n        Video identifier\n\n    Returns\n    -------\n    dict or None\n        Annotation completion status if found\n    \"\"\"\n    return db.graphs.find_one({\"video_id\":video_id, \"annotator_id\":str(annotator)},{\"annotation_completed\":1}) \n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_concept_map","title":"<code>get_concept_map(annotator, video_id)</code>","text":"<p>Get concept map relationships.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>Annotator identifier</p> required <code>video_id</code> <code>str</code> <p>Video identifier</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of concept map relationships</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_concept_map(annotator, video_id):\n    \"\"\"\n    Get concept map relationships.\n\n    Parameters\n    ----------\n    annotator : str\n        Annotator identifier\n    video_id : str  \n        Video identifier\n\n    Returns\n    -------\n    list\n        List of concept map relationships\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_concept_map(): Inizio ******\")\n\n    collection = db.graphs\n\n    pipeline = [\n        {\"$unwind\": \"$graph.@graph\"},\n        {\n            \"$match\":\n                {\n                    \"video_id\": str(video_id),\n                    \"annotator_id\": str(annotator),\n                    \"graph.@graph.type\": \"Annotation\",\n                    \"graph.@graph.motivation\": \"edu:linkingPrerequisite\",\n                }\n\n        },\n\n        {\"$project\":\n            {\n                \"prerequisite\": \"$graph.@graph.body\",\n                \"target\": \"$graph.@graph.target.dcterms:subject.id\",\n                \"weight\": \"$graph.@graph.skos:note\",\n                \"time\": \"$graph.@graph.target.selector.value\",\n                \"sent_id\": \"$graph.@graph.target.selector.edu:conllSentId\",\n                \"word_id\": \"$graph.@graph.target.selector.edu:conllWordId\",\n                \"xywh\": \"$graph.@graph.target.selector.edu:hasMediaFrag\",\n                \"creator\": \"$graph.@graph.creator\",\n                \"_id\": 0\n            }\n        },\n\n        {\"$sort\": {\"time\": 1}}\n\n    ]\n\n    aggregation = collection.aggregate(pipeline)\n    concept_map = list(aggregation)\n\n    for rel in concept_map:\n        rel[\"prerequisite\"] = rel[\"prerequisite\"].replace(\"concept_\",\"\").replace(\"_\",\" \")\n        rel[\"target\"] = rel[\"target\"].replace(\"concept_\",\"\").replace(\"_\",\" \")\n        rel[\"weight\"] = (rel[\"weight\"].replace(\"Prerequisite\",\"\")).capitalize()\n        rel[\"time\"] = rel[\"time\"].replace(\"^^xsd:dateTime\",\"\")\n        if \"xywh\" not in rel:\n            rel[\"xywh\"] = \"None\"\n        if \"word_id\" not in rel:\n            rel[\"word_id\"] = \"None\"\n        if \"sent_id\" not in rel:\n            rel[\"sent_id\"] = \"None\"\n\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_concept_map(): Fine ******\")\n\n    return concept_map\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_concepts","title":"<code>get_concepts(annotator, video_id)</code>","text":"<p>Get list of concepts for an annotation.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>Annotator identifier</p> required <code>video_id</code> <code>str</code> <p>Video identifier</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of concept names</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_concepts(annotator, video_id):\n    \"\"\"\n    Get list of concepts for an annotation.\n\n    Parameters\n    ----------\n    annotator : str\n        Annotator identifier\n    video_id : str\n        Video identifier\n\n    Returns\n    -------\n    list\n        List of concept names\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_concepts(): Inizio ******\")\n\n    collection = db.graphs\n\n    pipeline = [\n        {\"$unwind\": \"$graph.@graph\"},\n        {\n            \"$match\":\n                {\n                    \"video_id\": str(video_id),\n                    \"annotator_id\": str(annotator),\n                    \"graph.@graph.type\": \"skos:Concept\"\n                }\n\n        },\n\n        {\"$project\":\n            {\n                \"concept\": \"$graph.@graph.id\",\n                \"_id\": 0\n            }\n        }\n\n    ]\n\n    aggregation = collection.aggregate(pipeline)\n    results = list(aggregation)\n    concepts = []\n\n    for concept in results:\n        concepts.append(concept[\"concept\"].replace(\"concept_\",\"\").replace(\"_\",\" \"))\n\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_concepts(): Fine ******\")\n\n\n    return concepts\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_definitions","title":"<code>get_definitions(annotator, video_id)</code>","text":"<p>Retrieve definitions from the database for a given annotator and video.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>The ID of the annotator.</p> required <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <p>Returns:</p> Type Description <code>list of dict</code> <p>A list of definitions with their respective details such as concept, start, end, start_sent_id, end_sent_id, creator, and description_type.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_definitions(annotator, video_id):\n    \"\"\"\n    Retrieve definitions from the database for a given annotator and video.\n\n    Parameters\n    ----------\n    annotator : str\n        The ID of the annotator.\n    video_id : str\n        The ID of the video.\n\n    Returns\n    -------\n    list of dict\n        A list of definitions with their respective details such as concept, start, end, start_sent_id, end_sent_id, creator, and description_type.\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_definitions(): Inizio ******\")\n\n    collection = db.graphs\n\n    pipeline = [\n        {\"$unwind\": \"$graph.@graph\"},\n        {\n            \"$match\":\n                {\n                    \"video_id\": str(video_id),\n                    \"annotator_id\": str(annotator),\n                    \"graph.@graph.type\": \"Annotation\",\n                    \"graph.@graph.motivation\": \"describing\",\n                }\n\n        },\n\n        {\"$project\":\n            {\n                \"concept\": \"$graph.@graph.body\",\n                \"start\": \"$graph.@graph.target.selector.startSelector.value\",\n                \"end\": \"$graph.@graph.target.selector.endSelector.value\",\n                \"start_sent_id\": \"$graph.@graph.target.selector.startSelector.edu:conllSentId\",\n                \"end_sent_id\": \"$graph.@graph.target.selector.endSelector.edu:conllSentId\",\n                \"creator\": \"$graph.@graph.creator\",\n                \"description_type\": \"$graph.@graph.skos:note\",\n                \"_id\": 0\n            }\n        },\n\n        {\"$sort\": {\"start\": 1}}\n\n    ]\n\n    aggregation = collection.aggregate(pipeline)\n    definitions = list(aggregation)\n\n    for d in definitions:\n        d[\"concept\"] = d[\"concept\"].replace(\"concept_\",\"\").replace(\"_\",\" \")\n        d[\"end\"] = d[\"end\"].replace(\"^^xsd:dateTime\",\"\")\n        d[\"start\"] = d[\"start\"].replace(\"^^xsd:dateTime\", \"\")\n        d[\"description_type\"] = d[\"description_type\"].replace(\"concept\", \"\")\n\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_definitions(): Fine ******\")\n\n    return definitions\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_emails_registered","title":"<code>get_emails_registered()</code>","text":"<p>Get list of all registered email addresses.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of email addresses</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_emails_registered():\n    \"\"\"\n    Get list of all registered email addresses.\n\n    Returns\n    -------\n    list\n        List of email addresses\n    \"\"\"\n    return [user['email'] for user in users.find({}, {\"email\": 1, \"_id\": 0})]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_graph","title":"<code>get_graph(user, video)</code>","text":"<p>Get concept map graph for user and video.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>User identifier</p> required <code>video</code> <code>str</code> <p>Video identifier</p> required <p>Returns:</p> Type Description <code>dict or None</code> <p>Graph data if found, None otherwise</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_graph(user, video):\n    \"\"\"\n    Get concept map graph for user and video.\n\n    Parameters\n    ----------\n    user : str\n        User identifier\n    video : str\n        Video identifier\n\n    Returns\n    -------\n    dict or None\n        Graph data if found, None otherwise\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_graph() ******\")\n    collection = db.graphs\n    item = collection.find_one({\"annotator_id\":user, \"video_id\":video},{\"_id\":0,\"graph\":1})\n    if item is not None:\n        return item[\"graph\"]\n    return None\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_graphs_info","title":"<code>get_graphs_info(selected_video=None)</code>","text":"<p>Get graph information for videos.</p> <p>Parameters:</p> Name Type Description Default <code>selected_video</code> <code>str</code> <p>Video ID to get specific info for</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Graph information including titles and annotators</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_graphs_info(selected_video=None):\n    \"\"\"\n    Get graph information for videos.\n\n    Parameters\n    ----------\n    selected_video : str, optional\n        Video ID to get specific info for\n\n    Returns\n    -------\n    dict\n        Graph information including titles and annotators\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_graphs_info(): Inizio ******\")\n\n    # If selected video is None\n    # Returns all videos graphs, with the title, creator and the annotators\n    # Else returns only the selected video\n\n    collection = db.graphs\n\n    pipeline = [\n\n        # join con videos collection\n\n        {\n            \"$lookup\":{\n                \"from\": \"videos\",\n                \"localField\": \"video_id\",\n                \"foreignField\": \"video_id\",\n                \"as\": \"video\"\n            }\n        },\n\n        {\"$project\":\n            {\n                \"annotator_id\": 1,\n                \"annotator_name\": 1,\n                \"video_id\": 1,\n                \"title\": \"$video.title\",\n                \"creator\": \"$video.creator\",\n                \"_id\": 0}\n         },\n\n        {\"$sort\": {\"creator\": pymongo.ASCENDING}}\n    ]\n\n    aggregation = list(collection.aggregate(pipeline))\n    graphs_info = {}\n\n    # Remove possibly unavailable videos from resulting list\n    for vid in reversed(aggregation):\n        if not len(vid[\"creator\"]) or not len(vid[\"title\"]):\n            aggregation.remove(vid)\n\n    for vid in aggregation:\n\n        if \"annotator_id\" in vid:\n            annotator = {\"id\": vid[\"annotator_id\"], \"name\": vid[\"annotator_name\"]}\n\n            if vid[\"video_id\"] not in graphs_info:\n                graphs_info[vid[\"video_id\"]] = {\"title\": vid[\"title\"][0], \"creator\": vid[\"creator\"][0], \"annotators\": [annotator]}\n            else:\n                graphs_info[vid[\"video_id\"]][\"annotators\"].append(annotator)\n\n    if selected_video is not None:\n        if selected_video in graphs_info:\n            return graphs_info[selected_video]\n        else:\n            return None\n\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_graphs_info(): Fine ******\")\n\n    return graphs_info\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_untranscribed_videos","title":"<code>get_untranscribed_videos()</code>","text":"<p>Get list of videos needing transcription.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of tuples containing (video_id, language)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_untranscribed_videos():\n    \"\"\"\n    Get list of videos needing transcription.\n\n    Returns\n    -------\n    list\n        List of tuples containing (video_id, language)\n    \"\"\"\n    docs = list(db.videos.find({\"transcript_data.is_whisper_transcribed\":False},{\"video_id\":1, \"language\":1}))\n    if len(docs):\n        docs = [(doc[\"video_id\"], doc[\"language\"]) for doc in docs]\n    return docs\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_video_data","title":"<code>get_video_data(video_id, fields=None)</code>","text":"<p>Get video metadata from database.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>ID of video to retrieve</p> required <code>fields</code> <code>list</code> <p>Specific fields to retrieve</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Video metadata</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_video_data(video_id:str, fields:list | None= None):\n    \"\"\"\n    Get video metadata from database.\n\n    Parameters\n    ----------\n    video_id : str\n        ID of video to retrieve\n    fields : list, optional\n        Specific fields to retrieve\n\n    Returns\n    -------\n    dict\n        Video metadata\n    \"\"\"\n    collection = db.videos\n    if fields is None:\n        metadata = collection.find_one({\"video_id\": video_id})\n        if metadata is not None:\n            metadata.pop('_id')\n    else:\n\n        def build_projection(fields:\"dict|list\"):\n            projection = {}\n\n            for field in fields:\n                if isinstance(field, dict):\n                    # Handle nested fields\n                    for outer_key, inner_keys in field.items():\n                        projection[outer_key] = {key: True for key in inner_keys}\n                else:\n                    # For top-level fields\n                    projection[field] = True\n\n            return projection\n\n        projection = build_projection(fields)\n        metadata = list(collection.find({\"video_id\":video_id}, projection))\n        if len(metadata):\n            metadata = metadata[0]\n            metadata.pop(\"_id\")\n    return metadata\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.get_vocabulary","title":"<code>get_vocabulary(annotator, video_id)</code>","text":"<p>Get concept vocabulary with synonyms.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>Annotator identifier</p> required <code>video_id</code> <code>str</code> <p>Video identifier</p> required <p>Returns:</p> Type Description <code>dict or None</code> <p>Concept vocabulary mapping if found</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def get_vocabulary(annotator, video_id):\n    \"\"\"\n    Get concept vocabulary with synonyms.\n\n    Parameters\n    ----------\n    annotator : str\n        Annotator identifier\n    video_id : str\n        Video identifier\n\n    Returns\n    -------\n    dict or None\n        Concept vocabulary mapping if found\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_vocabulary(): Inizio ******\")\n\n\n    collection = db.graphs\n\n    pipeline = [\n        {\"$unwind\": \"$conceptVocabulary.@graph\"},\n        {\n            \"$match\":\n                {\n                    \"video_id\": str(video_id),\n                    \"annotator_id\": str(annotator),\n                    \"conceptVocabulary.@graph.type\": \"skos:Concept\"\n                }\n        },\n\n        {\"$project\":\n            {\n                \"prefLabel\": \"$conceptVocabulary.@graph.skos:prefLabel.@value\",\n                \"altLabel\": \"$conceptVocabulary.@graph.skos:altLabel.@value\",\n                \"_id\": 0\n            }\n        }\n\n    ]\n\n    aggregation = collection.aggregate(pipeline)\n    results = list(aggregation)\n\n    # define new concept vocabulary\n    conceptVocabulary = {}\n\n    # if there is none on DB\n    if len(results) == 0:\n        return None\n\n    # iterate for each concept and build the vocabulary basing on the number of synonyms\n    for concept in results: \n\n        if \"altLabel\" in concept :\n            if isinstance(concept[\"altLabel\"], list):\n                conceptVocabulary[concept[\"prefLabel\"]] = concept[\"altLabel\"]\n            else:\n                conceptVocabulary[concept[\"prefLabel\"]] = [concept[\"altLabel\"]]\n        else:\n            conceptVocabulary[concept[\"prefLabel\"]]=[]\n\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::get_vocabulary(): Fine ******\")\n\n    return conceptVocabulary\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.insert_burst","title":"<code>insert_burst(data)</code>","text":"<p>Insert or update burst analysis data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Burst data containing extraction_method and video_id</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def insert_burst(data):\n    \"\"\"\n    Insert or update burst analysis data.\n\n    Parameters\n    ----------\n    data : dict\n        Burst data containing extraction_method and video_id\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::insert_burst(): Inizio ******\")\n\n    collection = db.graphs\n    query = {\n        \"extraction_method\": \"Burst\",\n        \"video_id\": data[\"video_id\"]\n    }\n\n    if collection.find_one(query) is None:\n        collection.insert_one(data)\n    else:\n        new_graph = {\"$set\": {\"graph\": data[\"graph\"]}}\n        collection.update_one(query, new_graph)\n\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::insert_burst(): Fine ******\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.insert_conll_MongoDB","title":"<code>insert_conll_MongoDB(data)</code>","text":"<p>Insert CoNLL format data into database.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>CoNLL data with video_id field</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def insert_conll_MongoDB(data):\n    \"\"\"\n    Insert CoNLL format data into database.\n\n    Parameters\n    ----------\n    data : dict\n        CoNLL data with video_id field\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::insert_conll_MongoDB() ******\")\n    collection = db.conlls\n    if collection.find_one({\"video_id\": data[\"video_id\"]}) is None:\n        collection.insert_one(data)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.insert_gold","title":"<code>insert_gold(data)</code>","text":"<p>Insert or update gold standard data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Gold standard data including graph and concept vocabulary</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def insert_gold(data):\n    \"\"\"\n    Insert or update gold standard data.\n\n    Parameters\n    ----------\n    data : dict\n        Gold standard data including graph and concept vocabulary\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::insert_gold(): Inizio ******\")\n\n\n    collection = db.graphs\n    query = {\n        \"graph_type\": \"gold_standard\",\n        \"video_id\": data[\"video_id\"]\n    }\n\n    if collection.find_one(query) is None:\n        collection.insert_one(data)\n    else:\n        new_graph = {\"$set\": {\"graph\": data[\"graph\"], \"conceptVocabulary\": data[\"conceptVocabulary\"]}}\n        collection.update_one(query, new_graph)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.insert_graph","title":"<code>insert_graph(data)</code>","text":"<p>Insert or update graph data in database.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Graph data including annotator_id and video_id</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def insert_graph(data):\n    \"\"\"\n    Insert or update graph data in database.\n\n    Parameters\n    ----------\n    data : dict\n        Graph data including annotator_id and video_id\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::insert_graph(): Inizio ******\")\n\n\n    collection = db.graphs\n    query = {\n        \"annotator_id\": data[\"annotator_id\"],\n        \"video_id\": data[\"video_id\"]\n    }\n\n    if collection.find_one(query) is None:\n        collection.insert_one(data)\n    else:\n        new_graph = {\"$set\": \n            {\"graph\": data[\"graph\"], \n             \"conceptVocabulary\": data[\"conceptVocabulary\"], \n             \"annotation_completed\": data[\"annotation_completed\"]}}\n        collection.update_one(query, new_graph)\n\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::insert_graph(): Fine ******\")    \n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.insert_video_data","title":"<code>insert_video_data(data, update=True)</code>","text":"<p>Insert or update video data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Video data to insert/update</p> required <code>update</code> <code>bool</code> <p>Whether to update existing document or replace</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def insert_video_data(data:dict, update=True):\n    \"\"\"\n    Insert or update video data.\n\n    Parameters\n    ----------\n    data : dict\n        Video data to insert/update\n    update : bool, optional\n        Whether to update existing document or replace\n\n    Returns\n    -------\n    None\n    \"\"\"\n    collection = db.videos\n    mongo_doc:dict | None = collection.find_one({'video_id':data['video_id'] })\n    if mongo_doc is None:\n        collection.insert_one(data)\n        return\n    elif update:\n        mongo_doc.pop(\"_id\")\n        for key,value in data.items():\n            mongo_doc[key] = value\n    else:\n        mongo_doc = data\n    collection.delete_one({'video_id':data['video_id']})\n    collection.insert_one(mongo_doc)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.remove_account","title":"<code>remove_account(email)</code>","text":"<p>Remove user account.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>Email address of account to remove</p> required <p>Returns:</p> Type Description <code>str</code> <p>Status message indicating result</p> Warning <p>This operation is destructive and may affect related data</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def remove_account(email):\n    \"\"\"\n    Remove user account.\n\n    Parameters\n    ----------\n    email : str\n        Email address of account to remove\n\n    Returns\n    -------\n    str\n        Status message indicating result\n\n    Warning\n    -------\n    This operation is destructive and may affect related data\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: db_mongo.py::remove_account() ******\")\n\n    query = {\"email\": email}\n\n    if users.find_one(query) is not None:\n        try: \n            users.delete_one(query)\n        except:\n            return \"Error\"\n        return \"Done verified removed\"\n    elif unverified_users.find_one(query) is not None:\n        try: \n            unverified_users.delete_one(query)\n        except:\n            return \"Error\"\n        return \"Done unverified removed\"\n    return \"Not Found\"\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.remove_annotations_data","title":"<code>remove_annotations_data(video_id, user=None)</code>","text":"<p>Remove annotation data for a video.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>ID of video to remove annotations for</p> required <code>user</code> <code>dict</code> <p>User data to filter annotations by</p> <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def remove_annotations_data(video_id:str, user:dict=None):\n    \"\"\"\n    Remove annotation data for a video.\n\n    Parameters\n    ----------\n    video_id : str\n        ID of video to remove annotations for\n    user : dict, optional\n        User data to filter annotations by\n    \"\"\"\n    for coll in [db.graphs, db.conlls]:\n        if user is None:\n            while True:\n                doc = coll.find_one_and_delete({\"video_id\":video_id})\n                if doc is None:\n                    return\n\n    [coll.find_one_and_delete({\"video_id\":video_id, \"annotator_id\": user[\"id\"], \"annotator_name\": user[\"name\"]}) for coll in [db.graphs, db.conlls]]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.remove_video","title":"<code>remove_video(video_id)</code>","text":"<p>Remove video and associated data.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>ID of video to remove</p> required Warning <p>This operation is destructive and may affect related data</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def remove_video(video_id):\n    \"\"\"\n    Remove video and associated data.\n\n    Parameters\n    ----------\n    video_id : str\n        ID of video to remove\n\n    Warning\n    -------\n    This operation is destructive and may affect related data\n    \"\"\"\n    query = {\"video_id\":video_id}\n    collections = ['videos','graphs','conlls']\n    for coll_name in collections:\n        collection = db.get_collection(coll_name)\n        if collection.find_one(query):\n            try:\n                collection.delete_many(query)\n                #collection.delete_one(query)\n                print(f'removing from {coll_name}')\n            except:\n                pass\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/database/mongo/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.database.mongo.string_to_seconds","title":"<code>string_to_seconds(str)</code>","text":"<p>Convert time string to seconds.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <code>str</code> <p>Time string in format \"HH:MM:SS^^\"</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total seconds</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/database/mongo.py</code> <pre><code>def string_to_seconds(str):\n    \"\"\"\n    Convert time string to seconds.\n\n    Parameters\n    ----------\n    str : str\n        Time string in format \"HH:MM:SS^^\"\n\n    Returns\n    -------\n    int\n        Total seconds\n    \"\"\"\n    s = str.split(\"^^\")[0].split(\":\")\n    seconds = int(s[2]) + int(s[1]) * 60 + int(s[0]) * 60 * 60\n\n    return seconds\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/embedding/cluster/","title":"cluster","text":""},{"location":"codebase/EKEELVideoAnnotation/embedding/cluster/#cluster","title":"Cluster","text":"<p>Text clustering module for subtitle analysis.</p> <p>This module provides functionality for clustering subtitle sentences based on their semantic embeddings and timestamps.</p>"},{"location":"codebase/EKEELVideoAnnotation/embedding/cluster/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.embedding.cluster.Cluster","title":"<code>Cluster</code>","text":"<p>Cluster of related subtitles with their embeddings.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>Cluster identifier</p> <code>sentences</code> <code>list</code> <p>List of subtitle texts</p> <code>embeddings</code> <code>list</code> <p>List of sentence embeddings</p> <code>mean_embedding</code> <code>Tensor</code> <p>Mean embedding vector for cluster</p> <code>start_time</code> <code>float</code> <p>Start timestamp of cluster</p> <code>end_time</code> <code>float</code> <p>End timestamp of cluster</p> <code>keyframes</code> <code>list</code> <p>List of keyframe indices</p> <code>summary</code> <code>list</code> <p>Summary sentences for cluster</p> <p>Methods:</p> Name Description <code>merge_cluster</code> <p>Merge another cluster into this one</p> <code>add_sentence</code> <p>Add new sentence with its embedding</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/embedding/cluster.py</code> <pre><code>class Cluster:\n    \"\"\"\n    Cluster of related subtitles with their embeddings.\n\n    Attributes\n    ----------\n    index : int\n        Cluster identifier\n    sentences : list\n        List of subtitle texts\n    embeddings : list\n        List of sentence embeddings\n    mean_embedding : torch.Tensor\n        Mean embedding vector for cluster\n    start_time : float\n        Start timestamp of cluster\n    end_time : float\n        End timestamp of cluster\n    keyframes : list\n        List of keyframe indices\n    summary : list\n        Summary sentences for cluster\n\n    Methods\n    -------\n    merge_cluster(c)\n        Merge another cluster into this one\n    add_sentence(s, e)\n        Add new sentence with its embedding\n    \"\"\"\n\n    def __init__(self, index, subtitle, embedding):\n        self.index = index\n        self.sentences = [subtitle[\"text\"]]\n        self.embeddings = [embedding]\n        self.mean_embedding = embedding\n        self.start_time = subtitle[\"start\"]\n        self.end_time = subtitle[\"end\"]\n        self.keyframes = []\n        self.summary = []\n\n    def merge_cluster(self, c):\n        \"\"\"\n        Merge another cluster into this one.\n\n        Parameters\n        ----------\n        c : Cluster\n            Cluster to merge\n\n        Returns\n        -------\n        Cluster\n            Self with merged contents\n        \"\"\"\n        self.sentences = self.sentences + c.sentences\n        self.embeddings = self.embeddings + c.embeddings\n        self.mean_embedding = self.mean_embedding + c.mean_embedding\n        self.end_time = c.end_time\n        self.keyframes = self.keyframes + c.keyframes\n        return self\n\n    def add_sentence_deprecated(self, s, e):\n        \"\"\"Old ver, use add_sentence instead. Add sentence s and compute the new mean embedding value for the cluster\"\"\"\n        self.sentences.append(s)\n        # for i in range(0,len(e)):\n        #     self._mean_embedding[i] = int((self._mean_embedding[i] + e[i])/2)\n        somma = self._mean_embedding.add(e)\n        self._mean_embedding = torch.div(somma, 2)\n\n    def add_sentence(self, s, e):\n        \"\"\"\n        Add new sentence and update mean embedding.\n\n        Parameters\n        ----------\n        s : str\n            Sentence text\n        e : torch.Tensor\n            Sentence embedding\n        \"\"\"\n        self.sentences.append(s)\n        self.embeddings.append(e)\n\n        temp_emb = self.embeddings[0]\n        # for i in range(len(self.embeddings[0])):\n        #     tot = 0\n        #     for j in range(len(self.embeddings)):\n        #         tot += self.embeddings[j][i]\n        #     tot /= len(self.embeddings)\n        #     temp_emb.append(tot)\n\n        for emb in range(1, len(self.embeddings)):\n            temp_emb.add(emb)\n\n        temp_emb = torch.div(temp_emb, len(self.embeddings))\n\n        self.mean_embedding = temp_emb\n\n    def __str__(self):\n        output_str = f\"cluster {self.index}\\n\"\n        output_str += f\"t: ({str(datetime.timedelta(seconds=self.start_time))},{str(datetime.timedelta(seconds=self.end_time))})\\n\"\n        output_str += f\"{self._sentences.__str__()}\\n\"\n        #output_str += f\"{self.keyframes.__str__()}\\n\"\n\n        '''output_str += \"Riassunto: \\n\"\n        for i, s in enumerate(self.summary):\n            output_str += f\"{i}. {s.__str__()}\\n\"'''\n        return output_str\n\n    @property\n    def sentences(self):\n        return self._sentences\n\n    @sentences.setter\n    def sentences(self, s):\n        self._sentences = s\n\n    @property\n    def index(self):\n        return self._index\n\n    @index.setter\n    def index(self, value):\n        self._index = value\n\n    @property\n    def mean_embedding(self):\n        return self._mean_embedding\n\n    @mean_embedding.setter\n    def mean_embedding(self, value):\n        self._mean_embedding = value\n\n    @property\n    def summary(self):\n        return self._summary\n\n    @summary.setter\n    def summary(self, s):\n        self._summary = s\n\n    @property\n    def end_time(self):\n        return self._end_time\n\n    @end_time.setter\n    def end_time(self, e):\n        self._end_time = e\n\n    @property\n    def start_time(self):\n        return self._start_time\n\n    @start_time.setter\n    def start_time(self, s):\n        self._start_time = s\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/embedding/cluster/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.embedding.cluster.Cluster.add_sentence","title":"<code>add_sentence(s, e)</code>","text":"<p>Add new sentence and update mean embedding.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Sentence text</p> required <code>e</code> <code>Tensor</code> <p>Sentence embedding</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/embedding/cluster.py</code> <pre><code>def add_sentence(self, s, e):\n    \"\"\"\n    Add new sentence and update mean embedding.\n\n    Parameters\n    ----------\n    s : str\n        Sentence text\n    e : torch.Tensor\n        Sentence embedding\n    \"\"\"\n    self.sentences.append(s)\n    self.embeddings.append(e)\n\n    temp_emb = self.embeddings[0]\n    # for i in range(len(self.embeddings[0])):\n    #     tot = 0\n    #     for j in range(len(self.embeddings)):\n    #         tot += self.embeddings[j][i]\n    #     tot /= len(self.embeddings)\n    #     temp_emb.append(tot)\n\n    for emb in range(1, len(self.embeddings)):\n        temp_emb.add(emb)\n\n    temp_emb = torch.div(temp_emb, len(self.embeddings))\n\n    self.mean_embedding = temp_emb\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/embedding/cluster/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.embedding.cluster.Cluster.add_sentence_deprecated","title":"<code>add_sentence_deprecated(s, e)</code>","text":"<p>Old ver, use add_sentence instead. Add sentence s and compute the new mean embedding value for the cluster</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/embedding/cluster.py</code> <pre><code>def add_sentence_deprecated(self, s, e):\n    \"\"\"Old ver, use add_sentence instead. Add sentence s and compute the new mean embedding value for the cluster\"\"\"\n    self.sentences.append(s)\n    # for i in range(0,len(e)):\n    #     self._mean_embedding[i] = int((self._mean_embedding[i] + e[i])/2)\n    somma = self._mean_embedding.add(e)\n    self._mean_embedding = torch.div(somma, 2)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/embedding/cluster/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.embedding.cluster.Cluster.merge_cluster","title":"<code>merge_cluster(c)</code>","text":"<p>Merge another cluster into this one.</p> <p>Parameters:</p> Name Type Description Default <code>c</code> <code>Cluster</code> <p>Cluster to merge</p> required <p>Returns:</p> Type Description <code>Cluster</code> <p>Self with merged contents</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/embedding/cluster.py</code> <pre><code>def merge_cluster(self, c):\n    \"\"\"\n    Merge another cluster into this one.\n\n    Parameters\n    ----------\n    c : Cluster\n        Cluster to merge\n\n    Returns\n    -------\n    Cluster\n        Self with merged contents\n    \"\"\"\n    self.sentences = self.sentences + c.sentences\n    self.embeddings = self.embeddings + c.embeddings\n    self.mean_embedding = self.mean_embedding + c.mean_embedding\n    self.end_time = c.end_time\n    self.keyframes = self.keyframes + c.keyframes\n    return self\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/embedding/cluster/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.embedding.cluster.aggregate_short_clusters","title":"<code>aggregate_short_clusters(clusters, seconds)</code>","text":"<p>Merge clusters shorter than specified duration.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>list[Cluster]</code> <p>List of clusters to process</p> required <code>seconds</code> <code>float</code> <p>Minimum cluster duration</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of merged Cluster objects</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/embedding/cluster.py</code> <pre><code>def aggregate_short_clusters(clusters:list[Cluster], seconds):\n    \"\"\"\n    Merge clusters shorter than specified duration.\n\n    Parameters\n    ----------\n    clusters : list[Cluster]\n        List of clusters to process\n    seconds : float\n        Minimum cluster duration\n\n    Returns\n    -------\n    list\n        List of merged Cluster objects\n    \"\"\"\n    merge_times = []\n    s = 0\n    for e, c in enumerate(clusters):\n        if clusters[e].end_time - clusters[s].start_time &gt; seconds:\n            merge_times.append({\"start\":s, \"end\":e})\n            s = e+1\n        elif e == len(clusters)-1:\n            # if the last cluster is too short, I merge it with the last second-last\n            merge_times[-1] = ({\"start\":merge_times[-1][\"start\"], \"end\":e})\n\n    #print(f\"merge times: {merge_times}\")\n\n    refined_clusters = []\n    for m in merge_times:\n        temp_cluster = clusters[m[\"start\"]]\n        for k in range(m[\"start\"]+1,m[\"end\"]+1):\n            temp_cluster.merge_cluster(clusters[k])\n        refined_clusters.append(temp_cluster)\n\n    return refined_clusters\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/embedding/cluster/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.embedding.cluster.create_cluster_list","title":"<code>create_cluster_list(timed_sentences, embeddings, c_threshold)</code>","text":"<p>Create list of clusters from timed sentences.</p> <p>Parameters:</p> Name Type Description Default <code>timed_sentences</code> <code>list</code> <p>List of dictionaries containing text and timestamps</p> required <code>embeddings</code> <code>list</code> <p>List of sentence embeddings</p> required <code>c_threshold</code> <code>float</code> <p>Clustering similarity threshold</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of Cluster objects</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/embedding/cluster.py</code> <pre><code>def create_cluster_list(timed_sentences, embeddings, c_threshold):\n    \"\"\"\n    Create list of clusters from timed sentences.\n\n    Parameters\n    ----------\n    timed_sentences : list\n        List of dictionaries containing text and timestamps\n    embeddings : list\n        List of sentence embeddings\n    c_threshold : float\n        Clustering similarity threshold\n\n    Returns\n    -------\n    list\n        List of Cluster objects\n    \"\"\"\n    c_id = 0\n    cluster_list = [Cluster(c_id, timed_sentences[0], embeddings[0])]\n    sum = 0\n\n    for i in range(1, len(embeddings)):\n        sum += util.pytorch_cos_sim(embeddings[i], embeddings[i - 1])[0].numpy()[0]\n\n    c_threshold = (sum / len(embeddings))/1.5\n    #print(\"somma\", sum, sum / len(embeddings), c_threshold)\n\n\n    for i in range(1, len(embeddings)):\n        '''Cosine similarity'''\n        similarity_mean = util.pytorch_cos_sim(cluster_list[c_id].mean_embedding, embeddings[i])\n        similarity_last = util.pytorch_cos_sim(embeddings[i-1], embeddings[i])\n\n        if similarity_mean[0].numpy()[0] &gt; c_threshold or similarity_last[0].numpy()[0] &gt; c_threshold:\n            cluster_list[c_id].add_sentence(timed_sentences[i][\"text\"], embeddings[i])\n            cluster_list[c_id].end_time = timed_sentences[i][\"end\"]\n\n        else:\n            c_id += 1\n            new_cluster = Cluster(c_id, timed_sentences[i], embeddings[i])\n            cluster_list.append(new_cluster)\n\n    return cluster_list\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/","title":"form","text":""},{"location":"codebase/EKEELVideoAnnotation/forms/form/#form","title":"Form","text":"<p>Flask form definitions for user management and video processing.</p> <p>This module provides WTForms form classes for handling various user interactions including authentication, video submission, and analysis configuration.</p>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.BurstForm","title":"<code>BurstForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>Form for burst video processing configuration.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>TextField</code> <p>Video URL input field</p> <code>type</code> <code>RadioField</code> <p>Processing type selection (semi-automatic or automatic)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class BurstForm(FlaskForm):\n    \"\"\"\n    Form for burst video processing configuration.\n\n    Attributes\n    ----------\n    url : TextField\n        Video URL input field\n    type : RadioField\n        Processing type selection (semi-automatic or automatic)\n    \"\"\"\n    url = TextField('Url', validators=[InputRequired()])\n    type = RadioField('video', choices=[(\"semi\",\"semi-automatic\"), (\"auto\",\"automatic\")])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.ConfirmCodeForm","title":"<code>ConfirmCodeForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>Email confirmation code form.</p> <p>Attributes:</p> Name Type Description <code>code</code> <code>StringField</code> <p>Confirmation code input field</p> <code>submit</code> <code>SubmitField</code> <p>Form submission button</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class ConfirmCodeForm(FlaskForm):\n    \"\"\"\n    Email confirmation code form.\n\n    Attributes\n    ----------\n    code : StringField\n        Confirmation code input field\n    submit : SubmitField\n        Form submission button\n    \"\"\"\n    code = StringField(\"Insert the code received by email\", validators=[InputRequired()])\n    submit = SubmitField('Reset password')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.ForgotForm","title":"<code>ForgotForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>Password recovery request form.</p> <p>Attributes:</p> Name Type Description <code>email</code> <code>StringField</code> <p>User email input field</p> <code>submit</code> <code>SubmitField</code> <p>Form submission button</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class ForgotForm(FlaskForm):\n    \"\"\"\n    Password recovery request form.\n\n    Attributes\n    ----------\n    email : StringField\n        User email input field\n    submit : SubmitField\n        Form submission button\n    \"\"\"\n    email = StringField('Email', validators=[InputRequired(), Email('Email not correct')])\n    submit = SubmitField('Send mail')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.GoldStandardForm","title":"<code>GoldStandardForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>Gold standard creation form.</p> <p>Attributes:</p> Name Type Description <code>video</code> <code>RadioField</code> <p>Video selection field</p> <code>annotators</code> <code>MultiCheckboxField</code> <p>Multiple annotator selection</p> <code>agreements</code> <code>NonValidatingSelectField</code> <p>Combination criteria selection</p> <code>name</code> <code>StringField</code> <p>Gold standard name field</p> <code>submit</code> <code>SubmitField</code> <p>Form submission button</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class GoldStandardForm(FlaskForm):\n    \"\"\"\n    Gold standard creation form.\n\n    Attributes\n    ----------\n    video : RadioField\n        Video selection field\n    annotators : MultiCheckboxField\n        Multiple annotator selection\n    agreements : NonValidatingSelectField\n        Combination criteria selection\n    name : StringField\n        Gold standard name field\n    submit : SubmitField\n        Form submission button\n    \"\"\"\n    video = RadioField('Video', choices = [])\n    annotators = MultiCheckboxField('Annotators', choices=[], validators=[InputRequired()])\n    agreements = NonValidatingSelectField('Combination criteria', choices = [])\n    name = StringField(\"Gold Name\", validators=[InputRequired()])\n    submit = SubmitField('Launch Creation')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.LoginForm","title":"<code>LoginForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>User login form.</p> <p>Attributes:</p> Name Type Description <code>email</code> <code>StringField</code> <p>Email input field</p> <code>password</code> <code>PasswordField</code> <p>Password input field</p> <code>remember_me</code> <code>BooleanField</code> <p>Remember login option</p> <code>submit</code> <code>SubmitField</code> <p>Form submission button</p> <p>Methods:</p> Name Description <code>validate</code> <p>Validate login credentials</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class LoginForm(FlaskForm):\n    \"\"\"\n    User login form.\n\n    Attributes\n    ----------\n    email : StringField\n        Email input field\n    password : PasswordField\n        Password input field\n    remember_me : BooleanField\n        Remember login option\n    submit : SubmitField\n        Form submission button\n\n    Methods\n    -------\n    validate()\n        Validate login credentials\n    \"\"\"\n    email = StringField('Email', validators=[InputRequired()])\n    password = PasswordField('Password', validators=[InputRequired()])\n    remember_me = BooleanField('Remember Me')\n\n    submit = SubmitField('Login')\n\n    def __init__(self, *args, **kwargs):\n        super(LoginForm, self).__init__(*args, **kwargs)\n\n    def validate(self):\n        \"\"\"\n        Validate login credentials against database.\n\n        Returns\n        -------\n        bool\n            True if credentials are valid, False otherwise\n        \"\"\"\n        initial_validation = super(LoginForm, self).validate()\n        if not initial_validation:\n            return False\n\n        user = mongo.db.students.find_one({\"email\": self.email.data})\n\n        if user:\n            password = user[\"password_hash\"]\n            if bcrypt.checkpw(self.password.data.encode('utf-8'), password.encode('utf-8')):\n                return True\n            else:\n                self.email.errors.append(\"Email or password incorrect\")\n\n        elif mongo.unverified_users.find_one({\"email\": self.email.data}):\n\n            send_confirmation_mail_with_link(self.email.data)\n            self.email.errors.append(\"An email has been sent to your address in order to verify it\")\n            return False\n\n        else:\n            self.email.errors.append(\"Email or password incorrect\")\n\n        return False\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.LoginForm.validate","title":"<code>validate()</code>","text":"<p>Validate login credentials against database.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if credentials are valid, False otherwise</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validate login credentials against database.\n\n    Returns\n    -------\n    bool\n        True if credentials are valid, False otherwise\n    \"\"\"\n    initial_validation = super(LoginForm, self).validate()\n    if not initial_validation:\n        return False\n\n    user = mongo.db.students.find_one({\"email\": self.email.data})\n\n    if user:\n        password = user[\"password_hash\"]\n        if bcrypt.checkpw(self.password.data.encode('utf-8'), password.encode('utf-8')):\n            return True\n        else:\n            self.email.errors.append(\"Email or password incorrect\")\n\n    elif mongo.unverified_users.find_one({\"email\": self.email.data}):\n\n        send_confirmation_mail_with_link(self.email.data)\n        self.email.errors.append(\"An email has been sent to your address in order to verify it\")\n        return False\n\n    else:\n        self.email.errors.append(\"Email or password incorrect\")\n\n    return False\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.MultiCheckboxField","title":"<code>MultiCheckboxField</code>","text":"<p>               Bases: <code>SelectMultipleField</code></p> <p>Custom multiple checkbox field.</p> <p>Attributes:</p> Name Type Description <code>widget</code> <code>ListWidget</code> <p>List widget for checkboxes</p> <code>option_widget</code> <code>CheckboxInput</code> <p>Individual checkbox widget</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class MultiCheckboxField(SelectMultipleField):\n    \"\"\"\n    Custom multiple checkbox field.\n\n    Attributes\n    ----------\n    widget : ListWidget\n        List widget for checkboxes\n    option_widget : CheckboxInput\n        Individual checkbox widget\n    \"\"\"\n    widget = widgets.ListWidget(prefix_label=False)\n    option_widget = widgets.CheckboxInput()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.NonValidatingSelectField","title":"<code>NonValidatingSelectField</code>","text":"<p>               Bases: <code>SelectField</code></p> <p>Select field without validation.</p> <p>Methods:</p> Name Description <code>pre_validate</code> <p>Skip field validation</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class NonValidatingSelectField(SelectField):\n    \"\"\"\n    Select field without validation.\n\n    Methods\n    -------\n    pre_validate(form)\n        Skip field validation\n    \"\"\"\n    def pre_validate(self, form):\n        pass\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.PasswordResetForm","title":"<code>PasswordResetForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>Password reset form.</p> <p>Attributes:</p> Name Type Description <code>password</code> <code>PasswordField</code> <p>New password input field</p> <code>password2</code> <code>PasswordField</code> <p>Password confirmation field</p> <code>submit</code> <code>SubmitField</code> <p>Form submission button</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class PasswordResetForm(FlaskForm):\n    \"\"\"\n    Password reset form.\n\n    Attributes\n    ----------\n    password : PasswordField\n        New password input field\n    password2 : PasswordField\n        Password confirmation field\n    submit : SubmitField\n        Form submission button\n    \"\"\"\n    password = PasswordField('Password', validators=[InputRequired(), Length(min=8)])\n    password2 = PasswordField('Confirm Password', validators=[InputRequired(), EqualTo('password', message = \"Passwords must match\")])\n    submit = SubmitField('Reset password')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.RegisterForm","title":"<code>RegisterForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>User registration form.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>StringField</code> <p>First name input field</p> <code>surname</code> <code>StringField</code> <p>Last name input field</p> <code>email</code> <code>StringField</code> <p>Email input field</p> <code>password</code> <code>PasswordField</code> <p>Password input field</p> <code>password2</code> <code>PasswordField</code> <p>Password confirmation field</p> <code>submit</code> <code>SubmitField</code> <p>Form submission button</p> <p>Methods:</p> Name Description <code>validate_email</code> <p>Validate email uniqueness in database</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class RegisterForm(FlaskForm):\n    \"\"\"\n    User registration form.\n\n    Attributes\n    ----------\n    name : StringField\n        First name input field\n    surname : StringField\n        Last name input field\n    email : StringField\n        Email input field\n    password : PasswordField\n        Password input field\n    password2 : PasswordField\n        Password confirmation field\n    submit : SubmitField\n        Form submission button\n\n    Methods\n    -------\n    validate_email(email)\n        Validate email uniqueness in database\n    \"\"\"\n    name = StringField('First name', validators=[InputRequired()])\n    surname = StringField('Last name', validators=[InputRequired()])\n    email = StringField('Email', validators=[InputRequired(), Email('Email not correct')])\n    password = PasswordField('Password', validators=[InputRequired(), Length(min=8)])\n    password2 = PasswordField('Confirm Password', validators=[InputRequired(), EqualTo('password', message = \"Passwords must match\")])\n    submit = SubmitField('Register')\n\n    def validate_email(self, email):\n        \"\"\"\n        Check if email is already registered.\n\n        Parameters\n        ----------\n        email : StringField\n            Email field to validate\n\n        Raises\n        ------\n        ValidationError\n            If email exists in verified or unverified users\n        \"\"\"\n        if mongo.unverified_users.find_one({\"email\":email.data}):\n            raise ValidationError('There is an unverified account already registered under this email')\n        elif mongo.users.find_one({\"email\":email.data}):\n            raise ValidationError('There is already an account registered under this email')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.RegisterForm.validate_email","title":"<code>validate_email(email)</code>","text":"<p>Check if email is already registered.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>StringField</code> <p>Email field to validate</p> required <p>Raises:</p> Type Description <code>ValidationError</code> <p>If email exists in verified or unverified users</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>def validate_email(self, email):\n    \"\"\"\n    Check if email is already registered.\n\n    Parameters\n    ----------\n    email : StringField\n        Email field to validate\n\n    Raises\n    ------\n    ValidationError\n        If email exists in verified or unverified users\n    \"\"\"\n    if mongo.unverified_users.find_one({\"email\":email.data}):\n        raise ValidationError('There is an unverified account already registered under this email')\n    elif mongo.users.find_one({\"email\":email.data}):\n        raise ValidationError('There is already an account registered under this email')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.addVideoForm","title":"<code>addVideoForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>Form for adding new video for annotation.</p> <p>Attributes:</p> Name Type Description <code>url</code> <code>TextField</code> <p>Video URL input field</p> <code>submit</code> <code>SubmitField</code> <p>Form submission button</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class addVideoForm(FlaskForm):\n    \"\"\"\n    Form for adding new video for annotation.\n\n    Attributes\n    ----------\n    url : TextField\n        Video URL input field\n    submit : SubmitField\n        Form submission button\n    \"\"\"\n    url = TextField('Url', validators=[InputRequired()])\n    submit = SubmitField('Start annotate')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/form/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.form.analysisForm","title":"<code>analysisForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>Analysis configuration form.</p> <p>Attributes:</p> Name Type Description <code>video</code> <code>RadioField</code> <p>Video selection field</p> <code>annotator</code> <code>RadioField</code> <p>Annotator selection field</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/form.py</code> <pre><code>class analysisForm(FlaskForm):\n    \"\"\"\n    Analysis configuration form.\n\n    Attributes\n    ----------\n    video : RadioField\n        Video selection field\n    annotator : RadioField\n        Annotator selection field\n    \"\"\"\n    video = RadioField('video', choices = [])\n    annotator = RadioField('annotators', choices = [])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/","title":"mail","text":""},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#mail","title":"Mail","text":"<p>Email handling module for user communications.</p> <p>This module provides functionality for sending emails, managing SMTP connections, and handling email verification tokens.</p> <p>Classes:</p> Name Description <code>MailSender</code> <p>Email sending class with SMTP connection management</p> <p>Functions:</p> Name Description <code>send_mail</code> <p>Send email with HTML content</p> <code>generate_confirmation_token</code> <p>Generate email verification token</p> <code>confirm_token</code> <p>Verify email confirmation token</p> <code>send_confirmation_mail_with_link</code> <p>Send verification email with confirmation link</p> <code>send_confirmation_mail</code> <p>Send verification email with confirmation code</p>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender","title":"<code>MailSender</code>","text":"<p>Email sender with SMTP server connection management.</p> <p>Attributes:</p> Name Type Description <code>username</code> <code>str</code> <p>SMTP server login username</p> <code>password</code> <code>str</code> <p>SMTP server login password</p> <code>server_name</code> <code>str</code> <p>SMTP server hostname</p> <code>server_port</code> <code>int</code> <p>SMTP server port</p> <code>use_SSL</code> <code>bool</code> <p>Whether to use SSL connection</p> <code>smtpserver</code> <code>SMTP</code> <p>SMTP server connection</p> <code>connected</code> <code>bool</code> <p>Connection status</p> <code>recipients</code> <code>list</code> <p>List of recipient email addresses</p> <code>html_ready</code> <code>bool</code> <p>Whether HTML content is enabled</p> <code>msg</code> <code>MIMEMultipart</code> <p>Email message content</p> Warning <p>SMTP servers use different ports for SSL and TLS.</p> <p>Methods:</p> Name Description <code>set_message</code> <p>Set email content and metadata</p> <code>clear_message</code> <p>Clear email content</p> <code>connect</code> <p>Connect to SMTP server</p> <code>send_all</code> <p>Send email to all recipients</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>class MailSender:\n    \"\"\"\n    Email sender with SMTP server connection management.\n\n    Attributes\n    ----------\n    username : str\n        SMTP server login username\n    password : str\n        SMTP server login password\n    server_name : str\n        SMTP server hostname\n    server_port : int\n        SMTP server port\n    use_SSL : bool\n        Whether to use SSL connection\n    smtpserver : smtplib.SMTP\n        SMTP server connection\n    connected : bool\n        Connection status\n    recipients : list\n        List of recipient email addresses\n    html_ready : bool\n        Whether HTML content is enabled\n    msg : MIMEMultipart\n        Email message content\n\n    Warning\n    -------\n    SMTP servers use different ports for SSL and TLS.\n\n    Methods\n    -------\n    set_message(plaintext, subject, from, htmltext)\n        Set email content and metadata\n    clear_message()\n        Clear email content\n    connect()\n        Connect to SMTP server\n    send_all(close_connection)\n        Send email to all recipients\n    \"\"\"\n    def __init__(self, in_username, in_password, in_server=(\"smtp.gmail.com\", 587), use_SSL=False):\n        \"\"\"\n        Initialize mail sender with server configuration.\n\n        Parameters\n        ----------\n        in_username : str\n            SMTP server username\n        in_password : str\n            SMTP server password\n        in_server : tuple, optional\n            (hostname, port) for SMTP server\n        use_SSL : bool, optional\n            Whether to use SSL instead of TLS\n        \"\"\"\n        self.username = in_username\n        self.password = in_password\n        self.server_name = in_server[0]\n        self.server_port = in_server[1]\n        self.use_SSL = use_SSL\n\n        if self.use_SSL:\n            self.smtpserver = smtplib.SMTP_SSL(self.server_name, self.server_port)\n        else:\n            self.smtpserver = smtplib.SMTP(self.server_name, self.server_port)\n        self.connected = False\n        self.recipients = []\n\n    def __str__(self):\n        return \"Type: Mail Sender \\n\" \\\n               \"Connection to server {}, port {} \\n\" \\\n               \"Connected: {} \\n\" \\\n               \"Username: {}, Password: {}\".format(self.server_name, self.server_port, self.connected, self.username, self.password)\n\n    def set_message(self, in_plaintext, in_subject=\"\", in_from=None, in_htmltext=None):\n        \"\"\"\n        Create MIME message with optional HTML content.\n\n        Parameters\n        ----------\n        in_plaintext : str\n            Plain text email body\n        in_subject : str, optional\n            Email subject line\n        in_from : str, optional\n            Sender email address\n        in_htmltext : str, optional\n            HTML version of email body\n        \"\"\"\n\n        if in_htmltext is not None:\n            self.html_ready = True\n        else:\n            self.html_ready = False\n\n        if self.html_ready:\n            self.msg = MIMEMultipart('alternative')  # 'alternative' allows attaching an html version of the message later\n            self.msg.attach(MIMEText(in_plaintext, 'plain'))\n            self.msg.attach(MIMEText(in_htmltext, 'html'))\n        else:\n            self.msg = MIMEText(in_plaintext, 'plain')\n\n        self.msg['Subject'] = in_subject\n        if in_from is None:\n            self.msg['From'] = self.username\n        else:\n            self.msg['From'] = in_from\n        self.msg[\"To\"] = None\n        self.msg[\"CC\"] = None\n        self.msg[\"BCC\"] = None\n\n    def clear_message(self):\n        \"\"\"\n        Remove all email body content.\n\n        Clears both plain text and HTML content if present.\n        \"\"\"\n        self.msg.set_payload(\"\")\n\n    def set_subject(self, in_subject):\n        \"\"\"\n        Set email subject line.\n\n        Parameters\n        ----------\n        in_subject : str\n            New subject line\n        \"\"\"\n        self.msg.replace_header(\"Subject\", in_subject)\n\n    def set_from(self, in_from):\n        \"\"\"\n        Set sender email address.\n\n        Parameters\n        ----------\n        in_from : str\n            Sender email address\n        \"\"\"\n        self.msg.replace_header(\"From\", in_from)\n\n    def set_plaintext(self, in_body_text):\n        \"\"\"\n        Set plain text email body.\n\n        Parameters\n        ----------\n        in_body_text : str\n            Plain text content\n\n        Warning\n        -------\n        Replaces entire payload if no HTML content exists\n        \"\"\"\n        if not self.html_ready:\n            self.msg.set_payload(in_body_text)\n        else:\n            payload = self.msg.get_payload()\n            payload[0] = MIMEText(in_body_text)\n            self.msg.set_payload(payload)\n\n    def set_html(self, in_html):\n        \"\"\"\n        Set HTML email body.\n\n        Parameters\n        ----------\n        in_html : str\n            HTML content\n\n        Raises\n        ------\n        TypeError\n            If HTML wasn't enabled in set_message()\n        \"\"\"\n        try:\n            payload = self.msg.get_payload()\n            payload[1] = MIMEText(in_html, 'html')\n            self.msg.set_payload(payload)\n        except TypeError:\n            print(\"ERROR: \"\n                  \"Payload is not a list. Specify an HTML message with in_htmltext in MailSender.set_message()\")\n            raise\n\n    def set_recipients(self, in_recipients):\n        \"\"\"\n        Set list of recipient email addresses.\n\n        Parameters\n        ----------\n        in_recipients : list\n            List of recipient email addresses\n\n        Raises\n        ------\n        TypeError\n            If input is not a list or tuple\n        \"\"\"\n        if not isinstance(in_recipients, (list, tuple)):\n            raise TypeError(\"Recipients must be a list or tuple, is {}\".format(type(in_recipients)))\n\n        self.recipients = in_recipients\n\n    def add_recipient(self, in_recipient):\n        \"\"\"\n        Add single recipient to list.\n\n        Parameters\n        ----------\n        in_recipient : str\n            Recipient email address\n        \"\"\"\n        self.recipients.append(in_recipient)\n\n    def connect(self):\n        \"\"\"\n        Connect to SMTP server.\n\n        Establishes connection using configured credentials.\n        Must be called before sending messages.\n        \"\"\"\n        if not self.use_SSL:\n            self.smtpserver.starttls()\n        self.smtpserver.login(self.username, self.password)\n        self.connected = True\n        print(\"Connected to {}\".format(self.server_name))\n\n    def disconnect(self):\n        \"\"\"\n        Close SMTP server connection.\n        \"\"\"\n        self.smtpserver.close()\n        self.connected = False\n\n    def send_all(self, close_connection=True):\n        \"\"\"\n        Send message to all recipients.\n\n        Parameters\n        ----------\n        close_connection : bool, optional\n            Whether to close connection after sending\n\n        Raises\n        ------\n        ConnectionError\n            If not connected to SMTP server\n        \"\"\"\n        if not self.connected:\n            raise ConnectionError(\"Not connected to any server. Try self.connect() first\")\n\n        print(\"Message: {}\".format(self.msg.get_payload()))\n\n        for recipient in self.recipients:\n                self.msg.replace_header(\"To\", recipient)\n                print(\"Sending to {}\".format(recipient))\n                self.smtpserver.send_message(self.msg)\n\n        print(\"All messages sent\")\n\n        if close_connection:\n            self.disconnect()\n            print(\"Connection closed\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.__init__","title":"<code>__init__(in_username, in_password, in_server=('smtp.gmail.com', 587), use_SSL=False)</code>","text":"<p>Initialize mail sender with server configuration.</p> <p>Parameters:</p> Name Type Description Default <code>in_username</code> <code>str</code> <p>SMTP server username</p> required <code>in_password</code> <code>str</code> <p>SMTP server password</p> required <code>in_server</code> <code>tuple</code> <p>(hostname, port) for SMTP server</p> <code>('smtp.gmail.com', 587)</code> <code>use_SSL</code> <code>bool</code> <p>Whether to use SSL instead of TLS</p> <code>False</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def __init__(self, in_username, in_password, in_server=(\"smtp.gmail.com\", 587), use_SSL=False):\n    \"\"\"\n    Initialize mail sender with server configuration.\n\n    Parameters\n    ----------\n    in_username : str\n        SMTP server username\n    in_password : str\n        SMTP server password\n    in_server : tuple, optional\n        (hostname, port) for SMTP server\n    use_SSL : bool, optional\n        Whether to use SSL instead of TLS\n    \"\"\"\n    self.username = in_username\n    self.password = in_password\n    self.server_name = in_server[0]\n    self.server_port = in_server[1]\n    self.use_SSL = use_SSL\n\n    if self.use_SSL:\n        self.smtpserver = smtplib.SMTP_SSL(self.server_name, self.server_port)\n    else:\n        self.smtpserver = smtplib.SMTP(self.server_name, self.server_port)\n    self.connected = False\n    self.recipients = []\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.add_recipient","title":"<code>add_recipient(in_recipient)</code>","text":"<p>Add single recipient to list.</p> <p>Parameters:</p> Name Type Description Default <code>in_recipient</code> <code>str</code> <p>Recipient email address</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def add_recipient(self, in_recipient):\n    \"\"\"\n    Add single recipient to list.\n\n    Parameters\n    ----------\n    in_recipient : str\n        Recipient email address\n    \"\"\"\n    self.recipients.append(in_recipient)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.clear_message","title":"<code>clear_message()</code>","text":"<p>Remove all email body content.</p> <p>Clears both plain text and HTML content if present.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def clear_message(self):\n    \"\"\"\n    Remove all email body content.\n\n    Clears both plain text and HTML content if present.\n    \"\"\"\n    self.msg.set_payload(\"\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.connect","title":"<code>connect()</code>","text":"<p>Connect to SMTP server.</p> <p>Establishes connection using configured credentials. Must be called before sending messages.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connect to SMTP server.\n\n    Establishes connection using configured credentials.\n    Must be called before sending messages.\n    \"\"\"\n    if not self.use_SSL:\n        self.smtpserver.starttls()\n    self.smtpserver.login(self.username, self.password)\n    self.connected = True\n    print(\"Connected to {}\".format(self.server_name))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.disconnect","title":"<code>disconnect()</code>","text":"<p>Close SMTP server connection.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def disconnect(self):\n    \"\"\"\n    Close SMTP server connection.\n    \"\"\"\n    self.smtpserver.close()\n    self.connected = False\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.send_all","title":"<code>send_all(close_connection=True)</code>","text":"<p>Send message to all recipients.</p> <p>Parameters:</p> Name Type Description Default <code>close_connection</code> <code>bool</code> <p>Whether to close connection after sending</p> <code>True</code> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If not connected to SMTP server</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def send_all(self, close_connection=True):\n    \"\"\"\n    Send message to all recipients.\n\n    Parameters\n    ----------\n    close_connection : bool, optional\n        Whether to close connection after sending\n\n    Raises\n    ------\n    ConnectionError\n        If not connected to SMTP server\n    \"\"\"\n    if not self.connected:\n        raise ConnectionError(\"Not connected to any server. Try self.connect() first\")\n\n    print(\"Message: {}\".format(self.msg.get_payload()))\n\n    for recipient in self.recipients:\n            self.msg.replace_header(\"To\", recipient)\n            print(\"Sending to {}\".format(recipient))\n            self.smtpserver.send_message(self.msg)\n\n    print(\"All messages sent\")\n\n    if close_connection:\n        self.disconnect()\n        print(\"Connection closed\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.set_from","title":"<code>set_from(in_from)</code>","text":"<p>Set sender email address.</p> <p>Parameters:</p> Name Type Description Default <code>in_from</code> <code>str</code> <p>Sender email address</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def set_from(self, in_from):\n    \"\"\"\n    Set sender email address.\n\n    Parameters\n    ----------\n    in_from : str\n        Sender email address\n    \"\"\"\n    self.msg.replace_header(\"From\", in_from)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.set_html","title":"<code>set_html(in_html)</code>","text":"<p>Set HTML email body.</p> <p>Parameters:</p> Name Type Description Default <code>in_html</code> <code>str</code> <p>HTML content</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If HTML wasn't enabled in set_message()</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def set_html(self, in_html):\n    \"\"\"\n    Set HTML email body.\n\n    Parameters\n    ----------\n    in_html : str\n        HTML content\n\n    Raises\n    ------\n    TypeError\n        If HTML wasn't enabled in set_message()\n    \"\"\"\n    try:\n        payload = self.msg.get_payload()\n        payload[1] = MIMEText(in_html, 'html')\n        self.msg.set_payload(payload)\n    except TypeError:\n        print(\"ERROR: \"\n              \"Payload is not a list. Specify an HTML message with in_htmltext in MailSender.set_message()\")\n        raise\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.set_message","title":"<code>set_message(in_plaintext, in_subject='', in_from=None, in_htmltext=None)</code>","text":"<p>Create MIME message with optional HTML content.</p> <p>Parameters:</p> Name Type Description Default <code>in_plaintext</code> <code>str</code> <p>Plain text email body</p> required <code>in_subject</code> <code>str</code> <p>Email subject line</p> <code>''</code> <code>in_from</code> <code>str</code> <p>Sender email address</p> <code>None</code> <code>in_htmltext</code> <code>str</code> <p>HTML version of email body</p> <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def set_message(self, in_plaintext, in_subject=\"\", in_from=None, in_htmltext=None):\n    \"\"\"\n    Create MIME message with optional HTML content.\n\n    Parameters\n    ----------\n    in_plaintext : str\n        Plain text email body\n    in_subject : str, optional\n        Email subject line\n    in_from : str, optional\n        Sender email address\n    in_htmltext : str, optional\n        HTML version of email body\n    \"\"\"\n\n    if in_htmltext is not None:\n        self.html_ready = True\n    else:\n        self.html_ready = False\n\n    if self.html_ready:\n        self.msg = MIMEMultipart('alternative')  # 'alternative' allows attaching an html version of the message later\n        self.msg.attach(MIMEText(in_plaintext, 'plain'))\n        self.msg.attach(MIMEText(in_htmltext, 'html'))\n    else:\n        self.msg = MIMEText(in_plaintext, 'plain')\n\n    self.msg['Subject'] = in_subject\n    if in_from is None:\n        self.msg['From'] = self.username\n    else:\n        self.msg['From'] = in_from\n    self.msg[\"To\"] = None\n    self.msg[\"CC\"] = None\n    self.msg[\"BCC\"] = None\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.set_plaintext","title":"<code>set_plaintext(in_body_text)</code>","text":"<p>Set plain text email body.</p> <p>Parameters:</p> Name Type Description Default <code>in_body_text</code> <code>str</code> <p>Plain text content</p> required Warning <p>Replaces entire payload if no HTML content exists</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def set_plaintext(self, in_body_text):\n    \"\"\"\n    Set plain text email body.\n\n    Parameters\n    ----------\n    in_body_text : str\n        Plain text content\n\n    Warning\n    -------\n    Replaces entire payload if no HTML content exists\n    \"\"\"\n    if not self.html_ready:\n        self.msg.set_payload(in_body_text)\n    else:\n        payload = self.msg.get_payload()\n        payload[0] = MIMEText(in_body_text)\n        self.msg.set_payload(payload)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.set_recipients","title":"<code>set_recipients(in_recipients)</code>","text":"<p>Set list of recipient email addresses.</p> <p>Parameters:</p> Name Type Description Default <code>in_recipients</code> <code>list</code> <p>List of recipient email addresses</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If input is not a list or tuple</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def set_recipients(self, in_recipients):\n    \"\"\"\n    Set list of recipient email addresses.\n\n    Parameters\n    ----------\n    in_recipients : list\n        List of recipient email addresses\n\n    Raises\n    ------\n    TypeError\n        If input is not a list or tuple\n    \"\"\"\n    if not isinstance(in_recipients, (list, tuple)):\n        raise TypeError(\"Recipients must be a list or tuple, is {}\".format(type(in_recipients)))\n\n    self.recipients = in_recipients\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.MailSender.set_subject","title":"<code>set_subject(in_subject)</code>","text":"<p>Set email subject line.</p> <p>Parameters:</p> Name Type Description Default <code>in_subject</code> <code>str</code> <p>New subject line</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def set_subject(self, in_subject):\n    \"\"\"\n    Set email subject line.\n\n    Parameters\n    ----------\n    in_subject : str\n        New subject line\n    \"\"\"\n    self.msg.replace_header(\"Subject\", in_subject)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.confirm_token","title":"<code>confirm_token(token, expiration=5600)</code>","text":"<p>Verify email confirmation token.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>Token to verify</p> required <code>expiration</code> <code>int</code> <p>Token expiration time in seconds</p> <code>5600</code> <p>Returns:</p> Type Description <code>str or bool</code> <p>Email address if valid, False if invalid</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def confirm_token(token, expiration=5600):\n    \"\"\"\n    Verify email confirmation token.\n\n    Parameters\n    ----------\n    token : str\n        Token to verify\n    expiration : int, optional\n        Token expiration time in seconds\n\n    Returns\n    -------\n    str or bool\n        Email address if valid, False if invalid\n    \"\"\"\n    serializer = URLSafeTimedSerializer(app.config['SECRET_KEY'])\n    try:\n        email = serializer.loads(\n            token,\n            salt=app.config['SECURITY_PASSWORD_SALT'],\n            max_age=expiration\n        )\n    except:\n        return False\n    return email\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.generate_confirmation_token","title":"<code>generate_confirmation_token(email)</code>","text":"<p>Generate secure token for email confirmation.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>Email address to encode in token</p> required <p>Returns:</p> Type Description <code>str</code> <p>Secure URL-safe token</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def generate_confirmation_token(email):\n    \"\"\"\n    Generate secure token for email confirmation.\n\n    Parameters\n    ----------\n    email : str\n        Email address to encode in token\n\n    Returns\n    -------\n    str\n        Secure URL-safe token\n    \"\"\"\n    serializer = URLSafeTimedSerializer(app.config['SECRET_KEY'])\n    return serializer.dumps(email, app.config['SECURITY_PASSWORD_SALT'])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.send_confirmation_mail","title":"<code>send_confirmation_mail(email, code)</code>","text":"<p>Send verification email containing confirmation code.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>Recipient email address</p> required <code>code</code> <code>str</code> <p>Verification code</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def send_confirmation_mail(email, code):\n    \"\"\"\n    Send verification email containing confirmation code.\n\n    Parameters\n    ----------\n    email : str\n        Recipient email address\n    code : str\n        Verification code\n    \"\"\"\n    html = render_template('user/user_activate_mail.html', code=code)\n    subject = \"Please confirm your email\"\n\n    send_mail(email, subject, html)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.send_confirmation_mail_with_link","title":"<code>send_confirmation_mail_with_link(email)</code>","text":"<p>Send verification email containing confirmation link.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>Recipient email address</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def send_confirmation_mail_with_link(email):\n    \"\"\"\n    Send verification email containing confirmation link.\n\n    Parameters\n    ----------\n    email : str\n        Recipient email address\n    \"\"\"\n    token = generate_confirmation_token(email)\n    confirm_url = url_for('confirm_email', token=token, _external=True)\n    html = render_template('user/user_activate_mail_with_link.html', confirm_url=confirm_url)\n    subject = \"Please confirm your email\"\n\n    send_mail(email, subject, html)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/mail/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.mail.send_mail","title":"<code>send_mail(To, subject, html)</code>","text":"<p>Send email with HTML content.</p> <p>Parameters:</p> Name Type Description Default <code>To</code> <code>str</code> <p>Recipient email address</p> required <code>subject</code> <code>str</code> <p>Email subject line</p> required <code>html</code> <code>str</code> <p>HTML email content</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/mail.py</code> <pre><code>def send_mail(To, subject, html):\n    \"\"\"\n    Send email with HTML content.\n\n    Parameters\n    ----------\n    To : str\n        Recipient email address\n    subject : str\n        Email subject line\n    html : str\n        HTML email content\n    \"\"\"\n    ourmailsender = MailSender(EMAIL_ACCOUNT, EMAIL_PASSWORD, ('smtp.gmail.com', 587))\n\n    ourmailsender.set_message(\"edurell\", subject, \"EKEEL Annotations\", html)\n\n    ourmailsender.set_recipients([To])\n\n    ourmailsender.connect()\n    ourmailsender.send_all()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/user/","title":"user","text":""},{"location":"codebase/EKEELVideoAnnotation/forms/user/#user","title":"User","text":"<p>User management module for Flask-Login integration.</p> <p>This module provides user authentication and management functionality by extending Flask-Login's UserMixin class.</p>"},{"location":"codebase/EKEELVideoAnnotation/forms/user/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.user.User","title":"<code>User</code>","text":"<p>               Bases: <code>UserMixin</code></p> <p>User class for authentication and session management.</p> <p>This class extends Flask-Login's UserMixin to provide user  authentication functionality with MongoDB backend storage.</p> <p>Attributes:</p> Name Type Description <code>email</code> <code>str</code> <p>User's email address (unique identifier)</p> <code>name</code> <code>str</code> <p>User's first name</p> <code>surname</code> <code>str</code> <p>User's last name</p> <code>complete_name</code> <code>str</code> <p>User's full name (first + last)</p> <code>mongodb_id</code> <code>str</code> <p>MongoDB document ID for the user</p> <p>Methods:</p> Name Description <code>get_id</code> <p>Get user identifier for Flask-Login</p> <code>load_user</code> <p>Load user from database by email</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/user.py</code> <pre><code>class User(UserMixin):\n    \"\"\"\n    User class for authentication and session management.\n\n    This class extends Flask-Login's UserMixin to provide user \n    authentication functionality with MongoDB backend storage.\n\n    Attributes\n    ----------\n    email : str\n        User's email address (unique identifier)\n    name : str\n        User's first name\n    surname : str\n        User's last name\n    complete_name : str\n        User's full name (first + last)\n    mongodb_id : str\n        MongoDB document ID for the user\n\n    Methods\n    -------\n    get_id()\n        Get user identifier for Flask-Login\n    load_user(email)\n        Load user from database by email\n    \"\"\"\n\n    def __init__(self, email):\n        \"\"\"\n        Initialize user from database.\n\n        Parameters\n        ----------\n        email : str\n            User's email address\n        \"\"\"\n        self.email = email\n        u = users.find_one({\"email\": email})\n\n        self.name = u[\"name\"]\n        self.surname = u[\"surname\"]\n        self.complete_name = u[\"name\"] + \" \" + u[\"surname\"]\n        self.mongodb_id = str(u[\"_id\"])\n\n    def get_id(self):\n        \"\"\"\n        Get user identifier for Flask-Login.\n\n        Returns\n        -------\n        str\n            User's email address\n        \"\"\"\n        return self.email\n\n    @login.user_loader\n    def load_user(email):\n        \"\"\"\n        Load user from database by email.\n\n        Used by Flask-Login to reload the user object from the user ID \n        stored in the session.\n\n        Parameters\n        ----------\n        email : str\n            User's email address\n\n        Returns\n        -------\n        User or None\n            User object if found, None otherwise\n        \"\"\"\n        u = users.find_one({\"email\": email})\n        if not u:\n            return None\n        return User(email=u['email'])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/user/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.user.User.__init__","title":"<code>__init__(email)</code>","text":"<p>Initialize user from database.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>User's email address</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/user.py</code> <pre><code>def __init__(self, email):\n    \"\"\"\n    Initialize user from database.\n\n    Parameters\n    ----------\n    email : str\n        User's email address\n    \"\"\"\n    self.email = email\n    u = users.find_one({\"email\": email})\n\n    self.name = u[\"name\"]\n    self.surname = u[\"surname\"]\n    self.complete_name = u[\"name\"] + \" \" + u[\"surname\"]\n    self.mongodb_id = str(u[\"_id\"])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/user/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.user.User.get_id","title":"<code>get_id()</code>","text":"<p>Get user identifier for Flask-Login.</p> <p>Returns:</p> Type Description <code>str</code> <p>User's email address</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/user.py</code> <pre><code>def get_id(self):\n    \"\"\"\n    Get user identifier for Flask-Login.\n\n    Returns\n    -------\n    str\n        User's email address\n    \"\"\"\n    return self.email\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/forms/user/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.forms.user.User.load_user","title":"<code>load_user(email)</code>","text":"<p>Load user from database by email.</p> <p>Used by Flask-Login to reload the user object from the user ID  stored in the session.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>User's email address</p> required <p>Returns:</p> Type Description <code>User or None</code> <p>User object if found, None otherwise</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/forms/user.py</code> <pre><code>@login.user_loader\ndef load_user(email):\n    \"\"\"\n    Load user from database by email.\n\n    Used by Flask-Login to reload the user object from the user ID \n    stored in the session.\n\n    Parameters\n    ----------\n    email : str\n        User's email address\n\n    Returns\n    -------\n    User or None\n        User object if found, None otherwise\n    \"\"\"\n    u = users.find_one({\"email\": email})\n    if not u:\n        return None\n    return User(email=u['email'])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/audio/","title":"audio","text":""},{"location":"codebase/EKEELVideoAnnotation/media/audio/#audio","title":"Audio","text":"<p>Audio processing module for video files.</p> <p>This module provides functionality for converting video files to audio format, specifically focusing on MP4 to WAV conversion for further processing.</p> <p>Functions:</p> Name Description <code>convert_mp4_to_wav</code> <p>Converts MP4 video files to WAV audio format</p>"},{"location":"codebase/EKEELVideoAnnotation/media/audio/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.audio.convert_mp4_to_wav","title":"<code>convert_mp4_to_wav(video_path, video_id)</code>","text":"<p>Convert MP4 video file to WAV audio format.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Directory path containing the video file</p> required <code>video_id</code> <code>str</code> <p>Identifier of the video file</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path object pointing to the output WAV file</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If FFMPEG subprocess fails during conversion</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/audio.py</code> <pre><code>def convert_mp4_to_wav(video_path: str, video_id: str) -&gt; Path:\n    \"\"\"\n    Convert MP4 video file to WAV audio format.\n\n    Parameters\n    ----------\n    video_path : str\n        Directory path containing the video file\n    video_id : str\n        Identifier of the video file\n\n    Returns\n    -------\n    Path\n        Path object pointing to the output WAV file\n\n    Raises\n    ------\n    Exception\n        If FFMPEG subprocess fails during conversion\n    \"\"\"\n    output_file_path = Path(video_path).joinpath(video_id+'.wav')\n    if os.path.isfile(output_file_path): \n        return output_file_path\n\n    input_file_path = Path(video_path).joinpath(video_id+'.mp4').__str__()\n    try:\n        # Run ffmpeg command to convert MP4 to WAV\n        subprocess.run(['ffmpeg', '-i', input_file_path, output_file_path.__str__()], \n                       check=True, \n                       stdout=subprocess.PIPE, \n                       stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError as e:\n        print(e)\n        raise Exception(\"ERROR SUBPROCESS FFMPEG\")\n    return output_file_path\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/","title":"image","text":""},{"location":"codebase/EKEELVideoAnnotation/media/image/#image","title":"Image","text":"<p>Image processing module for video analysis.</p> <p>This module provides functionality for image processing, text extraction, face detection, and image comparison operations.</p> <p>Classes:</p> Name Description <code>FaceDetectorSingleton</code> <p>Singleton class for face detection using MediaPipe</p> <code>ImageClassifier</code> <p>Main class for image analysis and text extraction</p> <p>Functions:</p> Name Description <code>draw_bounding_boxes_on_image</code> <p>Draws bounding boxes on an image</p> <code>draw_bounding_boxes_on_image_classifier</code> <p>Draws bounding boxes on an ImageClassifier instance</p> <code>show_image</code> <p>Display an image using matplotlib</p>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.FaceDetectorSingleton","title":"<code>FaceDetectorSingleton</code>","text":"<p>               Bases: <code>object</code></p> <p>Singleton class for MediaPipe face detection.</p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>FaceDetectorSingleton</code> <p>Single instance of the detector</p> <code>_detector</code> <code>FaceDetector</code> <p>MediaPipe face detector instance</p> <p>Methods:</p> Name Description <code>detect</code> <p>Detect faces in the given image</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>class FaceDetectorSingleton(object):\n    \"\"\"\n    Singleton class for MediaPipe face detection.\n\n    Attributes\n    ----------\n    _instance : FaceDetectorSingleton\n        Single instance of the detector\n    _detector : MediaPipe.FaceDetector\n        MediaPipe face detector instance\n\n    Methods\n    -------\n    detect(image)\n        Detect faces in the given image\n    \"\"\"\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super(FaceDetectorSingleton, cls).__new__(cls)\n            options = mp.tasks.vision.FaceDetectorOptions(base_options=mp.tasks.BaseOptions(model_asset_path=MODEL_PATH))\n            cls._detector = mp.tasks.vision.FaceDetector.create_from_options(options)\n        return cls._instance\n\n    def detect(self, image):\n        return self._detector.detect(Image(image_format=ImageFormat.SRGB, data=image))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier","title":"<code>ImageClassifier</code>","text":"<p>Image analysis and classification wrapper.</p> <p>Attributes:</p> Name Type Description <code>_face_detector</code> <code>FaceDetectorSingleton</code> <p>Face detector instance</p> <code>_texts_with_contour</code> <code>list</code> <p>Detected text regions with bounding boxes</p> <code>_image</code> <code>ndarray</code> <p>Original image data</p> <code>_image_grayscaled</code> <code>ndarray</code> <p>Grayscale version of image</p> <p>Methods:</p> Name Description <code>copy</code> <p>Create a copy of the classifier</p> <code>detect_faces</code> <p>Detect faces in the image</p> <code>extract_text</code> <p>Extract text from image</p> <code>is_same_image</code> <p>Compare with another image</p> <code>has_changed_slide</code> <p>Check if slide has changed</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>class ImageClassifier:\n    \"\"\"\n    Image analysis and classification wrapper.\n\n    Attributes\n    ----------\n    _face_detector : FaceDetectorSingleton\n        Face detector instance\n    _texts_with_contour : list\n        Detected text regions with bounding boxes\n    _image : ndarray\n        Original image data\n    _image_grayscaled : ndarray\n        Grayscale version of image\n\n    Methods\n    -------\n    copy()\n        Create a copy of the classifier\n    detect_faces()\n        Detect faces in the image\n    extract_text(return_text=False, with_contours=False)\n        Extract text from image\n    is_same_image(other, threshold=3)\n        Compare with another image\n    has_changed_slide(other)\n        Check if slide has changed\n    \"\"\"\n    _face_detector = FaceDetectorSingleton()\n    _texts_with_contour:'list[tuple[str,tuple[int,int,int,int]]] | None' = None\n    _image = None\n    _image_grayscaled = None\n\n    def __init__(self,image) -&gt; None:\n        \"\"\"\n        Initialize image classifier.\n\n        Parameters\n        ----------\n        image : ndarray\n            Input image to analyze\n        \"\"\"\n        self._init_params_ = (image) \n        self._image = image\n\n    def copy(self):\n        \"\"\"\n        Create a deep copy of the classifier.\n\n        Returns\n        -------\n        ImageClassifier\n            New classifier instance with copied data\n        \"\"\"\n        new_img:ImageClassifier = ImageClassifier(self._image)\n        new_img._image_grayscaled = self._image_grayscaled\n        new_img._texts_with_contour = self._texts_with_contour\n        return new_img\n\n    def detect_faces(self):\n        \"\"\"\n        Detect faces in the image.\n\n        Returns\n        -------\n        list\n            List of MediaPipe face detection results\n\n        Raises\n        ------\n        Exception\n            If no image is loaded\n        \"\"\"\n        if self._image is None:\n            raise Exception(\"No Image to detect\")\n        return self._face_detector.detect(self._image).detections\n\n\n    def _convert_grayscale(self, new_axis=False):\n        \"\"\"\n        Convert image to grayscale.\n\n        Parameters\n        ----------\n        new_axis : bool, optional\n            Add channel dimension if True\n\n        Returns\n        -------\n        ndarray\n            Grayscale image\n        \"\"\"\n        self._image_grayscaled = cv2.cvtColor(self._image,cv2.COLOR_BGR2GRAY)\n        if new_axis:\n            self._image_grayscaled = self._image_grayscaled[:,:,None]\n        return self._image_grayscaled\n\n    def _preprocess_image(self,img_bw):\n        \"\"\"\n        Preprocess image for text detection.\n\n        Parameters\n        ----------\n        img_bw : ndarray\n            Binary image to process\n\n        Returns\n        -------\n        list\n            List of detected contours\n        \"\"\"\n        img_bw.flags.writeable = True\n        img_bw = img_bw.copy()\n        cv2.threshold(img_bw, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV,img_bw)\n        cv2.dilate(img_bw, cv2.getStructuringElement(cv2.MORPH_RECT, (6, 6)), img_bw,iterations = 3)\n        return cv2.findContours(img_bw, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)[0]\n\n    def _read_text_with_bbs(self, img, xywh_orig, conf=0) -&gt; List[Tuple[str,Tuple[int,int,int,int]]]:\n        '''\n        Read of text is made in this way:\n            - scan with pytesseract every word of the image (which is passed as cropped) and return as dict of words and other infos\n            - for every word in the structure i: check if is regognized with a confidence above conf, then save the delta line with respect to the previous\n\n            - if there's a delta line equal to zero i check if the previous line is ended ( -&gt; there's a new sentence)\n            - otherwise we are in the middle of a sentence so width of the sentence must be accumulated (accounting also for spaces)\n            - the heights and start Y of every sentence are calculated as the average of every word (sometimes there's noise or other elemens like mouse cursors)\n\n            - if instead there's a delta line &gt; 0 the sentence element is formed. \n            - New line is appended and bounding boxes are normalized with respect to the original size of the full image\n            - lastly if there's still some text in the structure but the iterator has ended, save it\n        '''\n        data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)\n        texts = data['text']\n        xs = data['left']\n        ys = data['top']\n        ws = data['width']\n        hs = data['height']\n        confs = data['conf']\n        lines = data['line_num']\n        texts_with_bb = []\n        x_off,y_off,img_w,img_h = xywh_orig\n\n        last_text_indx = len(texts)-1\n        text = ''\n        ended_line = True\n        for i, word in enumerate(texts):\n            next_line = lines[i+1] if i &lt; last_text_indx else lines[i]-1 \n            if confs[i]&gt;=conf:\n                # there won't be line change\n                if next_line-lines[i]==0:\n                    # first word of line: reset vars\n                    if ended_line:\n                        start_x = xs[i]\n                        ys_words = []\n                        hs_words = []\n                        cumul_w = 0\n                        ended_line = False\n                    # middle sentence word: add width for previous space\n                    else:\n                        cumul_w += xs[i]-(xs[i-1]+ws[i-1])\n                    text += word + ' '\n                    ys_words.append(ys[i])\n                    hs_words.append(hs[i])\n                    cumul_w += ws[i]\n                # there will be line change\n                else:\n                    # single word phrase: reset vars\n                    if ended_line:\n                        start_x = xs[i]\n                        ys_words = [ys[i]]\n                        hs_words = [hs[i]]; \n                        cumul_w = 0\n                    # last word of sentence before new line: add width for previous space\n                    else:\n                        cumul_w += xs[i]-(xs[i-1]+ws[i-1])\n                    text += word + '\\n'\n                    # if there's some text flush it\n                    if text.strip(): \n                        texts_with_bb.append((text,((start_x+x_off)/img_w,\n                                                    (mean(ys_words)+y_off)/img_h,\n                                                    (cumul_w + ws[i])/img_w,\n                                                    mean(hs_words)/img_h)))\n                    text = ''\n                    ended_line = True    \n        else:\n            # if there's still some text flush it\n            if not ended_line:\n                texts_with_bb.append((text,((start_x+x_off)/img_w,\n                                            (mean(ys_words)+y_off)/img_h,\n                                            (cumul_w + ws[i])/img_w,\n                                            mean(hs_words)/img_h)))\n\n        return texts_with_bb\n\n    def _scan_image_for_text_and_bounding_boxes(self):\n        '''\n        Image is preprocessed and cropped in multiple rectangles of texts, then these are singularly analyzed\\n\n        Firstly turns into black and white\\,\n        _preprocess_image() finds the contours of text\\n\n        for each contour a tuple of (text, bounding_boxes(x,y,w,h)) is read and insorted based on it's min Y value of the contours \n\n        Prerequisite\n        ------------\n        RGB, BGR but with len(image_shape) == 3 always\\n\n        '''\n        img_bw = self._convert_grayscale()\n        img_height,img_width = img_bw.shape\n        contours = self._preprocess_image(img_bw)\n        y_and_texts_with_bb = []\n        for cnt in contours:\n            x, y, w, h = cv2.boundingRect(cnt)\n            img_cropped = img_bw[y:y+h,x:x+w]\n            text_read = self._read_text_with_bbs(img_cropped,(x,y,img_width,img_height))\n            if text_read:\n                insort_left(y_and_texts_with_bb,(y,text_read))\n        self._texts_with_contour = [text_with_bb \n                                    for (_,texts_with_bb) in y_and_texts_with_bb\n                                    for text_with_bb in texts_with_bb]\n\n\n    def extract_text(self,return_text=False,with_contours=False):\n        \"\"\"\n        Extract text from image.\n\n        Parameters\n        ----------\n        return_text : bool, optional\n            Return extracted text if True\n        with_contours : bool, optional\n            Include bounding box coordinates if True\n\n        Returns\n        -------\n        bool or str or list\n            Depends on parameters:\n            - bool: if text found (default)\n            - str: extracted text if return_text=True\n            - list: text and contours if both True\n        \"\"\"\n        if self._image is None:\n            self._texts_with_contour = [[]]\n        self._scan_image_for_text_and_bounding_boxes()\n        if return_text and with_contours:\n            return self._texts_with_contour\n        elif return_text and not with_contours:\n            return ''.join([elem[0] for elem in self._texts_with_contour])\n        return bool(self._texts_with_contour)\n\n    def get_detected_text(self,with_contours=True):\n        \"\"\"\n        Get previously detected text.\n\n        Parameters\n        ----------\n        with_contours : bool, optional\n            Include bounding box coordinates\n\n        Returns\n        -------\n        list or str\n            Text with contours or just text\n        \"\"\"\n        assert self._texts_with_contour is not None\n        if with_contours: return self._texts_with_contour\n        else: return ''.join([elem[0] for elem in self._texts_with_contour])\n\n    def get_img_shape(self):\n        assert self._image is not None\n        return self._image.shape\n\n    def is_same_image(self,other:'ImageClassifier', threshold=3) -&gt; bool:\n        \"\"\"\n        Compare two images using MSE.\n\n        Parameters\n        ----------\n        other : ImageClassifier\n            Image to compare against\n        threshold : int, optional\n            MSE threshold for similarity\n\n        Returns\n        -------\n        bool\n            True if images are similar\n        \"\"\"\n        return np.mean((self._image - other._image)**2) &lt; threshold\n\n        comp_method = self._comp_method\n        if comp_method == DIST_MEAS_METHOD_COSINE_SIM:\n            return all(self.get_cosine_similarity(other_image) &gt;= self._similarity_threshold)\n        elif comp_method == DIST_MEAS_METHOD_MEAN_ABSOLUTE_DIST:\n            return all(self.get_mean_distance(other_image) &lt;= self._similarity_threshold)\n        else:\n            return False\n\n    def has_changed_slide(self, other:\"ImageClassifier\") -&gt; bool:\n        \"\"\"\n        Detect significant changes between images.\n\n        Parameters\n        ----------\n        other : ImageClassifier\n            Image to compare against\n\n        Returns\n        -------\n        bool\n            True if significant changes detected\n        \"\"\"\n        if self._image_grayscaled is None:\n            self._convert_grayscale()\n        if other._image_grayscaled is None:\n            other._convert_grayscale()\n\n        # Compute the absolute difference between the current frame and the previous frame\n        frame_diff = cv2.absdiff(self._image_grayscaled, other._image_grayscaled)\n\n        # Threshold the difference to get the regions with significant changes\n        _, thresh = cv2.threshold(frame_diff, 20, 255, cv2.THRESH_BINARY)\n\n        # Find contours in the thresholded image\n        return bool(len([cv2.boundingRect(contour) \n                         for contour in cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0] \n                         if cv2.contourArea(contour) &gt; img1_g.shape[0]*img2_g.shape[1]/20]))\n\n    def has_image(self):\n        return self._image is not None\n\n\n\n    def get_cosine_similarity(self,other:'ImageClassifier',on_histograms=True,rounding_decimals:int= 10):\n        \"\"\"\n        Calculate cosine similarity between images.\n\n        Parameters\n        ----------\n        other : ImageClassifier\n            Image to compare against\n        on_histograms : bool, optional\n            Use histogram comparison if True\n        rounding_decimals : int, optional\n            Decimal precision for result\n\n        Returns\n        -------\n        ndarray\n            Similarity scores per color channel\n        \"\"\"\n        assert self._image is not None and other._image is not None and self._image.shape == other._image.shape\n\n        if on_histograms:   # looks like it's faster\n            this_mat = self.get_hists(normalize=True)\n            other_mat = other.get_hists(normalize=True)\n        else:   # reshape to num_colors flatten rows, one for each color channel and normalize\n            this_image = self._image\n            other_image = other._image\n            this_mat = reshape(this_image,(this_image.shape[2],this_image.shape[0]*this_image.shape[1])).astype(float)\n            other_mat = reshape(other_image,(other_image.shape[2],other_image.shape[0]*other_image.shape[1])).astype(float)\n            cv2.normalize(this_mat,this_mat,0,1,cv2.NORM_MINMAX)\n            cv2.normalize(other_mat,other_mat,0,1,cv2.NORM_MINMAX)\n        cosine_sim = round( diag(dot(this_mat,other_mat.T))/(norm(this_mat,axis=1)*norm(other_mat,axis=1)), \n                            decimals=rounding_decimals)\n        return cosine_sim\n\n    def get_mean_distance(self,other:'ImageClassifier',on_histograms=True):\n        assert self._image is not None and other._image is not None\n        if on_histograms:\n            this_mat = self.get_hists(normalize=True)\n            other_mat = other.get_hists(normalize=True)\n            dists = abs(this_mat - other_mat)\n            return mean(dists,axis=1)\n        else:\n            this_mat = self._image.astype(int)\n            other_mat = other._image\n            dists = abs(this_mat - other_mat)\n            return mean(reshape(dists,(dists.shape[0]*dists.shape[1],dists.shape[2])),axis=0)\n\n    def _get_grayscaled_img(self):\n        '''\n        Converts to grayscale\n        '''\n        if self._color_scheme == COLOR_BGR:\n            return cv2.cvtColor(self._image, cv2.COLOR_BGR2GRAY)\n        elif self._color_scheme == COLOR_RGB:\n            return cv2.cvtColor(self._image, cv2.COLOR_RGB2GRAY)\n        else:\n            return self._image\n\n    def get_hists(self,normalize:bool=False,bins:int=256,grayscaled=False):\n        \"\"\"\n        Generate image histograms.\n\n        Parameters\n        ----------\n        normalize : bool, optional\n            Normalize histogram values\n        bins : int, optional\n            Number of histogram bins\n        grayscaled : bool, optional\n            Convert to grayscale first\n\n        Returns\n        -------\n        ndarray\n            Image histograms per channel\n        \"\"\"\n        assert self._image is not None\n        # CV2 calcHist is fast but can't calculate 3 channels at once \n        # so the fastest way is making a list of arrays and merging with cv2 merge\n        if grayscaled:\n            img = self._convert_grayscale(new_axis=True)\n        else:\n            img = self._image\n        img = cv2.split(img)\n        num_channels = len(img)\n        hists = []\n        for col_chan in range(num_channels):\n            hist = cv2.calcHist(img,channels=[col_chan],mask=None,histSize=[bins],ranges=[0,256])\n            if normalize:\n                cv2.normalize(hist,hist,0,1,cv2.NORM_MINMAX)\n            hists.append(hist)\n        hists = cv2.merge(hists)\n        if len(hists.shape) &gt; 2: hists = transpose(hists,(2,0,1))\n        return reshape(hists,(num_channels,bins))\n\n    def get_img(self, text_bounding_boxes=False):\n        if not text_bounding_boxes or not self._texts_with_contour:\n            return self._image\n        return draw_bounding_boxes_on_image(self._image,[elem[1] for elem in self._texts_with_contour])\n\n    def set_img(self,img):\n        \"\"\"\n        Set new image for analysis.\n\n        Parameters\n        ----------\n        img : ndarray\n            New image to analyze\n\n        Returns\n        -------\n        ImageClassifier\n            Self for method chaining\n        \"\"\"\n        self._image = img\n        self._image_grayscaled = None\n        self._texts_with_contour = None\n        return self\n\n    def set_color_scheme(self,color_scheme:int):\n        \"\"\"\n        Set color scheme for image processing.\n\n        Parameters\n        ----------\n        color_scheme : int\n            One of COLOR_BGR, COLOR_RGB, COLOR_GRAY\n\n        Returns\n        -------\n        ImageClassifier\n            Self for method chaining\n\n        Raises\n        ------\n        AssertionError\n            If invalid color scheme\n        \"\"\"\n        assert color_scheme == COLOR_BGR or color_scheme == COLOR_RGB or color_scheme == COLOR_GRAY \n        self._color_scheme = color_scheme\n        return self\n\n    def _debug_show_image(self,axis=None):\n        if self._image is not None:\n            if self._color_scheme == COLOR_BGR:\n                image = self._image.copy()\n                image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n            else:\n                image = self._image\n            if axis is not None:\n                axis.axis('off')\n                axis.imshow(image)\n            else:\n                from matplotlib import pyplot as plt\n                plt.axis('off')\n                plt.imshow(image)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.__init__","title":"<code>__init__(image)</code>","text":"<p>Initialize image classifier.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image to analyze</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def __init__(self,image) -&gt; None:\n    \"\"\"\n    Initialize image classifier.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image to analyze\n    \"\"\"\n    self._init_params_ = (image) \n    self._image = image\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.copy","title":"<code>copy()</code>","text":"<p>Create a deep copy of the classifier.</p> <p>Returns:</p> Type Description <code>ImageClassifier</code> <p>New classifier instance with copied data</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Create a deep copy of the classifier.\n\n    Returns\n    -------\n    ImageClassifier\n        New classifier instance with copied data\n    \"\"\"\n    new_img:ImageClassifier = ImageClassifier(self._image)\n    new_img._image_grayscaled = self._image_grayscaled\n    new_img._texts_with_contour = self._texts_with_contour\n    return new_img\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.detect_faces","title":"<code>detect_faces()</code>","text":"<p>Detect faces in the image.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of MediaPipe face detection results</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no image is loaded</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def detect_faces(self):\n    \"\"\"\n    Detect faces in the image.\n\n    Returns\n    -------\n    list\n        List of MediaPipe face detection results\n\n    Raises\n    ------\n    Exception\n        If no image is loaded\n    \"\"\"\n    if self._image is None:\n        raise Exception(\"No Image to detect\")\n    return self._face_detector.detect(self._image).detections\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.extract_text","title":"<code>extract_text(return_text=False, with_contours=False)</code>","text":"<p>Extract text from image.</p> <p>Parameters:</p> Name Type Description Default <code>return_text</code> <code>bool</code> <p>Return extracted text if True</p> <code>False</code> <code>with_contours</code> <code>bool</code> <p>Include bounding box coordinates if True</p> <code>False</code> <p>Returns:</p> Type Description <code>bool or str or list</code> <p>Depends on parameters: - bool: if text found (default) - str: extracted text if return_text=True - list: text and contours if both True</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def extract_text(self,return_text=False,with_contours=False):\n    \"\"\"\n    Extract text from image.\n\n    Parameters\n    ----------\n    return_text : bool, optional\n        Return extracted text if True\n    with_contours : bool, optional\n        Include bounding box coordinates if True\n\n    Returns\n    -------\n    bool or str or list\n        Depends on parameters:\n        - bool: if text found (default)\n        - str: extracted text if return_text=True\n        - list: text and contours if both True\n    \"\"\"\n    if self._image is None:\n        self._texts_with_contour = [[]]\n    self._scan_image_for_text_and_bounding_boxes()\n    if return_text and with_contours:\n        return self._texts_with_contour\n    elif return_text and not with_contours:\n        return ''.join([elem[0] for elem in self._texts_with_contour])\n    return bool(self._texts_with_contour)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.get_cosine_similarity","title":"<code>get_cosine_similarity(other, on_histograms=True, rounding_decimals=10)</code>","text":"<p>Calculate cosine similarity between images.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ImageClassifier</code> <p>Image to compare against</p> required <code>on_histograms</code> <code>bool</code> <p>Use histogram comparison if True</p> <code>True</code> <code>rounding_decimals</code> <code>int</code> <p>Decimal precision for result</p> <code>10</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Similarity scores per color channel</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def get_cosine_similarity(self,other:'ImageClassifier',on_histograms=True,rounding_decimals:int= 10):\n    \"\"\"\n    Calculate cosine similarity between images.\n\n    Parameters\n    ----------\n    other : ImageClassifier\n        Image to compare against\n    on_histograms : bool, optional\n        Use histogram comparison if True\n    rounding_decimals : int, optional\n        Decimal precision for result\n\n    Returns\n    -------\n    ndarray\n        Similarity scores per color channel\n    \"\"\"\n    assert self._image is not None and other._image is not None and self._image.shape == other._image.shape\n\n    if on_histograms:   # looks like it's faster\n        this_mat = self.get_hists(normalize=True)\n        other_mat = other.get_hists(normalize=True)\n    else:   # reshape to num_colors flatten rows, one for each color channel and normalize\n        this_image = self._image\n        other_image = other._image\n        this_mat = reshape(this_image,(this_image.shape[2],this_image.shape[0]*this_image.shape[1])).astype(float)\n        other_mat = reshape(other_image,(other_image.shape[2],other_image.shape[0]*other_image.shape[1])).astype(float)\n        cv2.normalize(this_mat,this_mat,0,1,cv2.NORM_MINMAX)\n        cv2.normalize(other_mat,other_mat,0,1,cv2.NORM_MINMAX)\n    cosine_sim = round( diag(dot(this_mat,other_mat.T))/(norm(this_mat,axis=1)*norm(other_mat,axis=1)), \n                        decimals=rounding_decimals)\n    return cosine_sim\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.get_detected_text","title":"<code>get_detected_text(with_contours=True)</code>","text":"<p>Get previously detected text.</p> <p>Parameters:</p> Name Type Description Default <code>with_contours</code> <code>bool</code> <p>Include bounding box coordinates</p> <code>True</code> <p>Returns:</p> Type Description <code>list or str</code> <p>Text with contours or just text</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def get_detected_text(self,with_contours=True):\n    \"\"\"\n    Get previously detected text.\n\n    Parameters\n    ----------\n    with_contours : bool, optional\n        Include bounding box coordinates\n\n    Returns\n    -------\n    list or str\n        Text with contours or just text\n    \"\"\"\n    assert self._texts_with_contour is not None\n    if with_contours: return self._texts_with_contour\n    else: return ''.join([elem[0] for elem in self._texts_with_contour])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.get_hists","title":"<code>get_hists(normalize=False, bins=256, grayscaled=False)</code>","text":"<p>Generate image histograms.</p> <p>Parameters:</p> Name Type Description Default <code>normalize</code> <code>bool</code> <p>Normalize histogram values</p> <code>False</code> <code>bins</code> <code>int</code> <p>Number of histogram bins</p> <code>256</code> <code>grayscaled</code> <code>bool</code> <p>Convert to grayscale first</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Image histograms per channel</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def get_hists(self,normalize:bool=False,bins:int=256,grayscaled=False):\n    \"\"\"\n    Generate image histograms.\n\n    Parameters\n    ----------\n    normalize : bool, optional\n        Normalize histogram values\n    bins : int, optional\n        Number of histogram bins\n    grayscaled : bool, optional\n        Convert to grayscale first\n\n    Returns\n    -------\n    ndarray\n        Image histograms per channel\n    \"\"\"\n    assert self._image is not None\n    # CV2 calcHist is fast but can't calculate 3 channels at once \n    # so the fastest way is making a list of arrays and merging with cv2 merge\n    if grayscaled:\n        img = self._convert_grayscale(new_axis=True)\n    else:\n        img = self._image\n    img = cv2.split(img)\n    num_channels = len(img)\n    hists = []\n    for col_chan in range(num_channels):\n        hist = cv2.calcHist(img,channels=[col_chan],mask=None,histSize=[bins],ranges=[0,256])\n        if normalize:\n            cv2.normalize(hist,hist,0,1,cv2.NORM_MINMAX)\n        hists.append(hist)\n    hists = cv2.merge(hists)\n    if len(hists.shape) &gt; 2: hists = transpose(hists,(2,0,1))\n    return reshape(hists,(num_channels,bins))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.has_changed_slide","title":"<code>has_changed_slide(other)</code>","text":"<p>Detect significant changes between images.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ImageClassifier</code> <p>Image to compare against</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if significant changes detected</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def has_changed_slide(self, other:\"ImageClassifier\") -&gt; bool:\n    \"\"\"\n    Detect significant changes between images.\n\n    Parameters\n    ----------\n    other : ImageClassifier\n        Image to compare against\n\n    Returns\n    -------\n    bool\n        True if significant changes detected\n    \"\"\"\n    if self._image_grayscaled is None:\n        self._convert_grayscale()\n    if other._image_grayscaled is None:\n        other._convert_grayscale()\n\n    # Compute the absolute difference between the current frame and the previous frame\n    frame_diff = cv2.absdiff(self._image_grayscaled, other._image_grayscaled)\n\n    # Threshold the difference to get the regions with significant changes\n    _, thresh = cv2.threshold(frame_diff, 20, 255, cv2.THRESH_BINARY)\n\n    # Find contours in the thresholded image\n    return bool(len([cv2.boundingRect(contour) \n                     for contour in cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0] \n                     if cv2.contourArea(contour) &gt; img1_g.shape[0]*img2_g.shape[1]/20]))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.is_same_image","title":"<code>is_same_image(other, threshold=3)</code>","text":"<p>Compare two images using MSE.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ImageClassifier</code> <p>Image to compare against</p> required <code>threshold</code> <code>int</code> <p>MSE threshold for similarity</p> <code>3</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if images are similar</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def is_same_image(self,other:'ImageClassifier', threshold=3) -&gt; bool:\n    \"\"\"\n    Compare two images using MSE.\n\n    Parameters\n    ----------\n    other : ImageClassifier\n        Image to compare against\n    threshold : int, optional\n        MSE threshold for similarity\n\n    Returns\n    -------\n    bool\n        True if images are similar\n    \"\"\"\n    return np.mean((self._image - other._image)**2) &lt; threshold\n\n    comp_method = self._comp_method\n    if comp_method == DIST_MEAS_METHOD_COSINE_SIM:\n        return all(self.get_cosine_similarity(other_image) &gt;= self._similarity_threshold)\n    elif comp_method == DIST_MEAS_METHOD_MEAN_ABSOLUTE_DIST:\n        return all(self.get_mean_distance(other_image) &lt;= self._similarity_threshold)\n    else:\n        return False\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.set_color_scheme","title":"<code>set_color_scheme(color_scheme)</code>","text":"<p>Set color scheme for image processing.</p> <p>Parameters:</p> Name Type Description Default <code>color_scheme</code> <code>int</code> <p>One of COLOR_BGR, COLOR_RGB, COLOR_GRAY</p> required <p>Returns:</p> Type Description <code>ImageClassifier</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If invalid color scheme</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def set_color_scheme(self,color_scheme:int):\n    \"\"\"\n    Set color scheme for image processing.\n\n    Parameters\n    ----------\n    color_scheme : int\n        One of COLOR_BGR, COLOR_RGB, COLOR_GRAY\n\n    Returns\n    -------\n    ImageClassifier\n        Self for method chaining\n\n    Raises\n    ------\n    AssertionError\n        If invalid color scheme\n    \"\"\"\n    assert color_scheme == COLOR_BGR or color_scheme == COLOR_RGB or color_scheme == COLOR_GRAY \n    self._color_scheme = color_scheme\n    return self\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.ImageClassifier.set_img","title":"<code>set_img(img)</code>","text":"<p>Set new image for analysis.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>New image to analyze</p> required <p>Returns:</p> Type Description <code>ImageClassifier</code> <p>Self for method chaining</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def set_img(self,img):\n    \"\"\"\n    Set new image for analysis.\n\n    Parameters\n    ----------\n    img : ndarray\n        New image to analyze\n\n    Returns\n    -------\n    ImageClassifier\n        Self for method chaining\n    \"\"\"\n    self._image = img\n    self._image_grayscaled = None\n    self._texts_with_contour = None\n    return self\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.draw_bounding_boxes_on_image","title":"<code>draw_bounding_boxes_on_image(img, bounding_boxes)</code>","text":"<p>Draw bounding boxes on an image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Input image</p> required <code>bounding_boxes</code> <code>list</code> <p>List of (x,y,w,h) bounding box coordinates</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image with drawn bounding boxes</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def draw_bounding_boxes_on_image(img, bounding_boxes:'list[tuple[(int,int,int,int)]]'):\n    \"\"\"\n    Draw bounding boxes on an image.\n\n    Parameters\n    ----------\n    img : ndarray\n        Input image\n    bounding_boxes : list\n        List of (x,y,w,h) bounding box coordinates\n\n    Returns\n    -------\n    ndarray\n        Image with drawn bounding boxes\n    \"\"\"\n    img = img.copy()\n    if len(img.shape) == 3:\n        img_h,img_w,_ = img.shape\n    else:\n        img_h,img_w = img.shape\n    for xywh in bounding_boxes:\n        # rescale bbs\n        x = int(xywh[0]*img_w); y = int(xywh[1]*img_h); w = int(xywh[2]*img_w); h = int(xywh[3]*img_h)\n        # draw\n        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 1)\n    return img\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.draw_bounding_boxes_on_image_classifier","title":"<code>draw_bounding_boxes_on_image_classifier(image)</code>","text":"<p>Draw detected text bounding boxes on ImageClassifier.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageClassifier</code> <p>Input image classifier with detected text</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image with drawn bounding boxes</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def draw_bounding_boxes_on_image_classifier(image:ImageClassifier):\n    \"\"\"\n    Draw detected text bounding boxes on ImageClassifier.\n\n    Parameters\n    ----------\n    image : ImageClassifier\n        Input image classifier with detected text\n\n    Returns\n    -------\n    ndarray\n        Image with drawn bounding boxes\n    \"\"\"\n    assert image.get_img() is not None and image.get_detected_text() is not None\n    return draw_bounding_boxes_on_image(image.get_img(),[bbs for _,bbs in image.get_detected_text()])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/image/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.image.show_image","title":"<code>show_image(image, color_scheme=COLOR_BGR)</code>","text":"<p>Display an image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image to display</p> required <code>color_scheme</code> <code>int</code> <p>Color scheme of the input image</p> <code>COLOR_BGR</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/image.py</code> <pre><code>def show_image(image,color_scheme=COLOR_BGR):\n    \"\"\"\n    Display an image using matplotlib.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image to display\n    color_scheme : int\n        Color scheme of the input image\n    \"\"\"\n    from matplotlib import pyplot as plt\n    if color_scheme == COLOR_BGR:\n        cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.show()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/","title":"segmentation","text":""},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#segmentation","title":"Segmentation","text":""},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.MAX_VIDEO_SECONDS","title":"<code>MAX_VIDEO_SECONDS = 18000</code>  <code>module-attribute</code>","text":"<p>variables to consider:     * punctuator net,     * bert model,     * cosine similarity threshold - c_threshold,     * summary model - bert vs sumy,     * min time to merge short clusters,     * add_sentence vs deprecated,     * color_histogram threshold and frame range,</p>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer","title":"<code>VideoAnalyzer</code>","text":"<p>This class analyzes videos from their ids in the path \"class\"-path/static/videos </p> <p>_testing_path is for changing the folder path but it's used just for testing</p> <pre><code>- It provides a method to get the transcript from the youtube link associated to the id provided\n\n- segment the transcript to obtain keyframes\n\n- Analyze the whole video stream or frames to segment slides,text and times on screen of those\n\n- Get the extracted text in different formats (tipically only one is used and the others are for debug)\n\n- Check if the video contains slides (for now this means text around the center of the screen) and the proportion with respect to the whole video len\n\n- Extract titles from the slides found\n\n- Create thumbnails based on the slides segmentation after having analyzed the text\n\n- Find definitions and in-depths of concepts expressed in the title of every slide\n</code></pre> <p>checks of valid session are performed sometimes to ensure user did not want to stop analysis</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>class VideoAnalyzer:\n    '''\n    This class analyzes videos from their ids in the path \"__class__\"-path/static/videos \\n\n    _testing_path is for changing the folder path but it's used just for testing\\n\\n\n        - It provides a method to get the transcript from the youtube link associated to the id provided\\n\n        - segment the transcript to obtain keyframes\\n\n        - Analyze the whole video stream or frames to segment slides,text and times on screen of those\\n\n        - Get the extracted text in different formats (tipically only one is used and the others are for debug)\\n\n        - Check if the video contains slides (for now this means text around the center of the screen) and the proportion with respect to the whole video len\\n\n        - Extract titles from the slides found\\n\n        - Create thumbnails based on the slides segmentation after having analyzed the text\\n\n        - Find definitions and in-depths of concepts expressed in the title of every slide\n    checks of valid session are performed sometimes to ensure user did not want to stop analysis\n    '''\n    url:str\n    video_id:str\n    images_path:list = None\n    data:dict = None\n    timed_subtitles:dict = None\n\n    _text_in_video: list[VideoSlide] | None = None\n    _cos_sim_img_threshold = None\n    _frames_to_analyze = None\n    _slide_startends = None\n    _slide_titles = None\n\n\n    def __init__(self, url:str,request_fields_from_db:list | None=None, _testing_path=None) -&gt; None:\n        if self.is_youtube_url(url):\n            url = self.standardize_url(url)\n            self.video_id = self.extract_video_id(url)\n            self.url = url\n        else:\n            raise Exception(\"This is not a Youtube video!\")\n\n        response = requests.get(url)\n        if response.status_code == 200 and (\"Video non disponibile\" in response.text or \"Video unavailable\" in response.text):\n            raise Exception(\"Video unavailable\")\n\n        self.data = mongo.get_video_data(self.video_id, request_fields_from_db)\n        if self.data is None:\n            self.data = {'video_id': self.video_id,\n                         'title': search(r'\"title\":\"(.*?)\"', response.text).group(1),\n                         'creator': search(r'\"ownerChannelName\":\"(.*?)\"', response.text).group(1),\n                         'duration': str(round(int(search(r'\"approxDurationMs\":\"(\\d+)\"', response.text).group(1)) / 1000, 2)),\n                         'upload_date': search(r'\"uploadDate\":\"(.*?)\"', response.text).group(1)\n                        }\n            self.identify_language()\n            if float(self.data[\"duration\"]) &gt; MAX_VIDEO_SECONDS:\n                raise Exception(\"The video is too long, please choose another video.\")\n\n        if _testing_path is None:\n            self.folder_path = VIDEOS_PATH.joinpath(self.video_id)\n        else:\n            self.folder_path = _testing_path\n\n    @staticmethod\n    def standardize_url(url: str) -&gt; str:\n        pattern = r'(?:=|\\/|&amp;)([A-Za-z0-9_\\-]{11})(?=[=/&amp;]|\\b)'\n        id = re.findall(pattern,url)[0]\n        return \"https://www.youtube.com/watch?v=\"+id\n\n    @staticmethod\n    def is_youtube_url(url:str) -&gt; bool:\n        youtube_video_regex = (\n            r'(https?://)?(www\\.)?'\n            '(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n            '(watch\\?v=|embed/|v/|.+\\?v=)?([^&amp;=%\\?]{11})')\n\n        return match(youtube_video_regex, url) is not None\n\n    @staticmethod\n    def extract_video_id(url:str):\n        '''\n        From a YouTube url extracts the video id\n        '''\n        video_link = url.split('&amp;')[0]\n        if '=' in video_link:\n            return video_link.split('=')[-1]\n        return video_link.split('/')[-1]\n\n    def download_video(self):\n        '''\n        Downloads the video (expected YouTube video)\\n\n        If the video has been removed from youtube, it attempts to remove the folder from both the drive and the database, then raises an Exception\\n\n\n        --------------\n        # NOTE\n        There are problems very often with YouTube, which breaks yt_dlp library.\\n \n        In case of errors try to update the library\n        '''\n        url = self.url\n        #video_link = url.split('&amp;')[0]\n        video_id = self.video_id\n        folder_path = self.folder_path\n\n        os.makedirs(folder_path, exist_ok=True)\n\n        if os.path.isfile(os.path.join(folder_path,video_id+'.mp4')):\n            return\n\n        # Both pafy and pytube seems to be not mantained anymore, only youtube_dlp is still alive\n\n        prev_cwd = os.getcwd()\n        os.chdir(folder_path)\n\n        try:\n            #print(\"using ytdl\")\n            #yt_dlp.YoutubeDL({'format': 'bestvideo[height&lt;=480]', 'quiet': True}).download([url])\n            yt_dlp.YoutubeDL({  'quiet': False,\n                                'format': 'bestvideo[height&lt;=720]+bestaudio/best[height&lt;=720]',\n                                'outtmpl': video_id+'.mp4',\n                                'merge_output_format': 'mp4',\n                                'ffmpeg_location': '/usr/bin/ffmpeg',\n                                'postprocessors': [{\n                                    'key': 'FFmpegVideoConvertor',\n                                    'preferedformat': 'mp4',  # Ensure the output is in mp4 format\n                                    }],\n                                  }).download([url])\n        except Exception as e:\n            print(e)\n\n        os.chdir(prev_cwd)\n\n        for file in os.listdir(folder_path):\n            if file.endswith(\".mp4\"):\n                os.rename(folder_path.joinpath(file),folder_path.joinpath(video_id+\".\"+file.split(\".\")[-1]))\n                vidcap = cv2.VideoCapture(folder_path.joinpath(video_id+\".\"+file.split(\".\")[-1]))\n                if not vidcap.isOpened() or not min((vidcap.get(cv2.CAP_PROP_FRAME_WIDTH),vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))) &gt;= 360:\n                    raise Exception(\"Video does not have enough definition to find text\")\n                break\n\n\n    def _create_keyframes(self,start_times,end_times,S,seconds_range, image_scale:float=1,create_thumbnails=True):\n        \"\"\"\n        Take a list of clusters and compute the color histogram on end and start of the cluster\n\n        Parameters\n        ----------\n        cluster_list :\n        S : scala per color histogram\n        seconds_range : Adjust the start and end of segments based on the difference in color histograms.\n        \"\"\"\n        assert len(start_times) == len(end_times)\n        vsm = VideoSpeedManager(self.video_id,output_colors=COLOR_RGB)\n        vid_ref = vsm.vid_ref\n        vidcap = vid_ref._vidcap\n        start_end_frames = [(vid_ref.get_num_frame_from_time(s_time),vid_ref.get_num_frame_from_time(e_time)) \n                                for s_time,e_time in zip(start_times,end_times)]\n\n        curr_num_frame = 0\n        vidcap.set(cv2.CAP_PROP_POS_FRAMES, curr_num_frame)\n        has_frame,curr_frame = vidcap.read()\n        next_frame_offset = 10*vid_ref.get_fps()\n        summation = 0\n        prev_hist = []\n        all_diffs = []\n        while has_frame:\n            curr_frame = cv2.resize(curr_frame,(240,180))\n            image = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)\n            hist = cv2.calcHist([image], [0], None, [32], [0, 128])\n            hist = cv2.normalize(hist, hist)\n\n            if curr_num_frame &gt; 0 :\n                diff = 0\n                for i, bin in enumerate(hist):\n                    diff += abs(bin[0] - prev_hist[i][0])\n                all_diffs.append(diff)\n                summation += diff\n\n            curr_num_frame += next_frame_offset\n            vidcap.set(cv2.CAP_PROP_POS_FRAMES, curr_num_frame)\n            has_frame,curr_frame = vidcap.read()\n            prev_hist = hist\n\n            #if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            #    break\n        threshold = S * (summation / curr_num_frame)\n\n        '''\n        checking if there is a scene change within a timeframe of \"seconds_range\" frames from the beginning and end of each cluster.\n        '''\n        fps = vid_ref.get_fps()\n\n        for j,(start_f,end_f) in enumerate(start_end_frames):\n            changes_found = [False,False]\n            for i in range(seconds_range):\n                diffs = [int(start_f / next_frame_offset),int(end_f / next_frame_offset)]\n                if not changes_found[0] and diffs[0] + i &lt; len(all_diffs) and all_diffs[diffs[0] + i] &gt; threshold:\n                    sec = (start_f + i) / fps\n                    start_times[j] = round(sec,2)\n                    changes_found[0] = True\n\n                if not changes_found[1] and diffs[1] + i &lt; len(all_diffs) and all_diffs[diffs[1] + i] &gt; threshold:\n                    sec = (end_f + i) / fps\n                    end_times[j] = round(sec,2)\n                    changes_found[1] = True\n\n                if all(changes_found): break\n\n        if not create_thumbnails:\n            return list(zip(start_times, end_times))\n\n        # saving images to show into the timeline\n        images_path = []\n        folder_path = VIDEOS_PATH.joinpath(self.video_id)\n        for i, start_end in enumerate(start_end_frames):\n            vidcap.set(cv2.CAP_PROP_POS_FRAMES, start_end[0])\n            ret, image = vidcap.read()\n            image = cv2.resize(image,(array([image.shape[1],image.shape[0]],dtype='f')*image_scale).astype(int))\n            name_file = folder_path.joinpath(str(i) + \".jpg\")\n            #print(saving_position)\n            #saving_position = \"videos\\\\\" + video_id + \"\\\\\" + str(start) + \".jpg\"\n            cv2.imwrite(name_file.__str__(), image)\n            images_path.append(\"videos/\" + self.video_id + \"/\" + str(i) + \".jpg\")\n        vsm.close()\n        #print(images_path)\n        self.images_path = images_path\n        return list(zip(start_times, end_times))\n\n\n    def request_transcript(self):\n        '''\n        Downloads the transcript associated with the video and returns also whether the transcript is automatically or manually generated \\n\n        Preferred manually generated \\n\n        Whisper transcription is implemented in transcribe.py called as external service and will replace youtube transcript for word level precision\\n\n        '''\n\n        if \"transcript_data\" in self.data.keys():\n            return\n\n        # moved as external service\n        #if extract_from_audio:\n        #    self.data[\"transcript_data\"] = {\"text\": WhisperTranscriber.transcribe(self.video_id,language), \n        #                                    \"is_autogenerated\": True,\n        #                                    \"is_whisper_transcribed\": True }\n        #    mongo.insert_video_data(self.data)\n        #    return\n\n\n        language = self.identify_language() \n        self.data[\"transcript_data\"] = {\"is_whisper_transcribed\": False}\n\n        transcripts = YTTranscriptApi.list_transcripts(self.video_id)\n        transcript:Transcript\n        try:\n            transcript = transcripts.find_manually_created_transcript([language])\n            self.data[\"transcript_data\"][\"is_autogenerated\"] = False\n        except:\n            transcript = transcripts.find_generated_transcript([language])\n            self.data[\"transcript_data\"][\"is_autogenerated\"] = True\n\n        subs_dict = transcript.fetch()\n        for sub in subs_dict: sub[\"end\"] = sub[\"start\"] + sub.pop(\"duration\")\n\n        timed_subtitles = []\n        for entry in subs_dict:\n            if not \"[\" in entry[\"text\"]:\n                word = {\"word\":\"\",\n                        \"start\":entry[\"start\"],\n                        \"end\": entry[\"end\"]}\n                words = []\n                for word_text in entry[\"text\"].split(\" \"):\n                    apostrophed_words = word_text.split(\"'\")\n                    if len(apostrophed_words) &gt; 1:\n                        word = word.copy()\n                        word[\"word\"] = apostrophed_words[0]+\"'\"\n                        words.append(word)\n                    if len(apostrophed_words[-1]):\n                        word = word.copy()\n                        word[\"word\"] = apostrophed_words[-1]\n                        words.append(word)\n                segment = {'text': entry[\"text\"], \n                           'start': entry['start'],\n                           'end': entry['end'],\n                           'words': words }\n                timed_subtitles.append(segment)\n        self.data[\"transcript_data\"][\"text\"] = timed_subtitles \n\n        mongo.insert_video_data(self.data)\n\n\n    def transcript_segmentation(self, c_threshold=0.22, sec_min=35, S=1, frame_range=15, create_thumbnails=True):\n        \"\"\"\n        :param c_threshold: threshold per la similarit\u00e0 tra frasi\n        :param sec_min: se un segmento \u00e8 minore di sec_min verr\u00e0 unito con il successivo\n        :param S: scala per color histogram\n        :param frame_range: aggiustare inizio e fine dei segmenti in base a differenza nel color histogram nel frame_range\n        :return: segments\n        \"\"\"\n        if \"video_data\" in self.data.keys() and \"segments\" in self.data[\"video_data\"].keys():\n            return \n        video_id = self.video_id\n        language = self.identify_language()\n\n        # get punctuated transcription from the conll in the db\n        #transcription:str = get_text(video_id)\n        #if transcription is None:\n        #    if self.timed_subtitles is None:\n        #        self.request_transcript()\n#\n        #    '''Get the transcription from the subtitles'''\n        #    transcription:str = \" \".join([sub[\"text\"] for sub in self.timed_subtitles[\"text\"]])\n        #    if language == Locale().get_pt1_from_full('English'):\n        #        transcription:str = transcription.replace('\\n', ' ').replace(\"&gt;&gt;\", \"\") \\\n        #                                         .replace(\"Dr.\",\"Dr\").replace(\"dr.\",\"dr\") \\\n        #                                         .replace(\"Mr.\",\"Mr\").replace(\"mr.\",\"mr\")\n        #print(\"Checking punctuation...\")\n        semantic_transcript = SemanticText(\" \".join(timed_sentence[\"text\"] for timed_sentence in self.data[\"transcript_data\"][\"text\"] if not \"[\" in timed_sentence['text']),language)\n\n        #video = mongo.get_video(video_id)\n\n        '''Divide into sentences the punctuated transcription'''\n        print(\"Extracting sentences..\")\n        sentences = [sent.replace(\" ,\",\",\").replace(\" .\",\".\") for sent in semantic_transcript.tokenize()]\n\n        '''For each sentence, add its start and end time obtained from the subtitles'''\n        timed_sentences = get_timed_sentences(self.data[\"transcript_data\"][\"text\"], sentences)\n\n        '''Define the BERT model for similarity'''\n        print(\"Creating embeddings..\")\n        #model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n        #model = SentenceTransformer('stsb-roberta-large')\n\n        '''Compute a vector of numbers (the embedding) to idenfity each sentence'''\n        #embeddings = model.encode(sentences, convert_to_tensor=True)\n        # All moved inside semantic transcript class\n        embeddings = semantic_transcript.get_embeddings()\n\n        '''Create clusters based on semantic textual similarity, using the BERT embeddings'''\n        print(\"Creating initials segments..\")\n        clusters = create_cluster_list(timed_sentences, embeddings, c_threshold)\n\n\n        '''Aggregate togheter clusters shorter than 40 seconds in total'''\n        refined_clusters = aggregate_short_clusters(clusters, sec_min)\n\n        start_times = []\n        end_times = []\n\n        '''Print the final result'''\n        for c in refined_clusters:\n            start_times.append(c.start_time)\n            end_times.append(c.end_time)\n        self.data[\"video_data\"] = {\"segments\": list(zip(start_times, end_times))}\n        mongo.insert_video_data(self.data)\n\n\n        print(\"Reached the part of finding clusters\")\n\n        '''Find and append to each cluster the 2 most relevant sentences'''\n        #num_sentences = 2\n        #sumy_summary(refined_clusters, num_sentences)\n        #self.transcript = semantic_transcript\n\n\n        '''Adjust end and start time of each cluster based on detected scene changes'''\n        #path = Path(__file__).parent.joinpath(\"static\", \"videos\", video_id)\n        #if not create_thumbnails:\n        #    self.data[\"video_data\"]['segments'] = self._create_keyframes(start_times, end_times, S, frame_range, create_thumbnails=False)\n        #    return\n#\n        #if not any(File.endswith(\".jpg\") for File in os.listdir(path)):\n        #    self.data[\"video_data\"]['segments'] = self._create_keyframes(start_times, end_times, S, frame_range)\n        #else:\n        #    print(\"keyframes already present\")\n        #    images = []\n        #    for File in os.listdir(path):\n        #        if File.endswith(\".jpg\"):\n        #            images.append(File.split(\".\")[0])\n        #    images.sort(key=int)\n        #    self.images_path = [\"videos/\" + video_id + \"/\" + im + \".jpg\" for im in images]\n        return\n\n\n    #def extract_keywords(self,maxWords:int=1,minFrequency:int=3) -&gt; None:\n    #    video_doc = mongo.get_video_metadata(self.video_id)\n    #    if video_doc is None:\n    #        if self.transcript is None:\n    #            self.transcript_segmentation_and_thumbnails(create_thumbnails=False)\n    #        self.data[\"extracted_keywords\"] = self.transcript.extract_keywords(maxWords, minFrequency)\n    #        return\n    #    self.data[\"extracted_keywords\"] = video_doc[\"extracted_keywords\"]\n\n\n#######################\n    def _preprocess_video(self, vsm:VideoSpeedManager,num_segments:int=150,estimate_threshold=False,_show_info=False):\n        '''\n        Split the video into `num_segments` windows frames, for every segment it's taken the frame that's far enough to guarantee a minimum sensibility\\n\n        The current frame is analyzed by XGBoost model to recognize the scene\\n\n        If there are two non-slide frames consecutively the resulting frame window is cut\\n\n        Bounds are both upper and lower inclusive to avoid a miss as much as possible\\n\n        Then both are compared in terms of cosine distance of their histograms (it's faster than flattening and computing on pure pixels)\\n\\n\n        Lastly the distance between wach frame is selected as either the average of values, either with fixed value.\\n\n        In this instance with videos that are mostly static, the threshold is set to 0.9999\n        #TODO further improvements: for more accuracy this algorithm could include frames similarity to classify a segment as slide (which is a generally very static)\n\n        Returns\n        ----------\n        The cosine similarity threshold and the list of frames to analyze (tuples of starting and ending frames)\n\n        ----------\n        Example\n        ----------\n        A video split into 10 segments:\\n\\n\n        slide_segments : 0,1,3,6,9,10\\n\n        non_slide_segments : 2,4,5,7,8\\n\n        results in segmentation = [(0,4),(5,7)(8,10)]\\n\n        with holes between segments 4-5 and 7-8\n        '''\n        num_frames = vsm.get_video().get_count_frames()\n        step = floor(num_frames / (num_segments))\n        vsm.lock_speed(step)\n        iterations_counter:int = 0\n        txt_cleaner = TextCleaner()\n        if estimate_threshold:\n            cos_sim_values = empty((num_segments,vsm.get_video().get_dim_frame()[2]))\n\n        # Optimization is performed by doing a first coarse-grained analysis with the XGBoost model predictor\n        # then set those windows inside the VideoSpeedManager\n        model = XGBoostModelAdapter(Path(__file__).parent.parent.joinpath(\"models\",\"xgboost500.sav\").__str__())\n        #XGBoostModelAdapter(os.path.dirname(os.path.realpath(__file__))+\"/xgboost/model/xgboost500.sav\")\n\n        start_frame_num = None\n        frames_to_analyze:List[Tuple[int,int]] = []\n        answ_queue = deque([False,False])\n        curr_frame = ImageClassifier(None)\n        prev_frame = curr_frame.copy()\n        frame_w,frame_h,num_colors = vsm.get_video().get_dim_frame()\n\n        # Loops through num segments and for every segment checks if is a slide\n        # When at least one \n        # Iterates over num segments\n        while iterations_counter &lt; num_segments:\n\n            # Stores two frames\n            prev_frame.set_img(vsm.get_frame())\n            curr_frame.set_img(vsm.get_following_frame())\n            if model.is_enough_slidish_like(prev_frame):\n                frame = prev_frame.get_img()\n\n                # validate slide in frame by slicing the image in a region that removes logos (that are usually in corners)\n                region = (slice(int(frame_h/11),int(frame_h*8/9)),slice(int(frame_w/8),int(frame_w*7/8)))\n                prev_frame.set_img(frame[region])\n\n                # double checks the text  \n                is_slide = bool(txt_cleaner.clean_text(prev_frame.extract_text(return_text=True)).strip())\n                #from matplotlib import pyplot as plt; plt.figure(\"Figure 1\"); plt.imshow(frame); plt.figure(\"Figure 2\"); plt.imshow(prev_frame.get_img())\n            else:\n                is_slide = False\n            answ_queue.appendleft(is_slide); answ_queue.pop()\n\n            # if there's more than 1 True discontinuity -&gt; cut the video\n            if any(answ_queue) and start_frame_num is None:\n                start_frame_num = int(clip(iterations_counter-1,0,num_segments))*step\n            elif not any(answ_queue) and start_frame_num is not None:\n                frames_to_analyze.append((start_frame_num,(iterations_counter-1)*step))\n                start_frame_num = None\n\n            if estimate_threshold:\n                cos_sim_values[iterations_counter,:] = prev_frame.get_cosine_similarity(curr_frame)\n            iterations_counter+=1\n            if _show_info: print(f\" Coarse-grained analysis: {ceil((iterations_counter)/num_segments * 100)}%\",end='\\r')\n        if start_frame_num is not None:\n            frames_to_analyze.append((start_frame_num,num_frames-1))\n\n        if estimate_threshold:\n            cos_sim_img_threshold = clip(average(cos_sim_values,axis=0)+var(cos_sim_values,axis=0)/2,0.9,0.9999)\n        else:\n            cos_sim_img_threshold = ones((1,num_colors))*0.9999\n\n        if _show_info:\n            if estimate_threshold:\n                print(f\"Estimated cosine similarity threshold: {cos_sim_img_threshold}\")\n            else:\n                print(f\"Cosine_similarity threshold: {cos_sim_img_threshold}\")\n            print(f\"Frames to analyze: {frames_to_analyze} of {num_frames} total frames\")\n        self.data[\"video_data\"][\"slides_percentage\"] = sum([frame_window[1] - frame_window[0] for frame_window in frames_to_analyze])/(num_frames-1)\n        self._cos_sim_img_threshold = cos_sim_img_threshold \n        self._frames_to_analyze = frames_to_analyze    \n\n\n    def analyze_video(self,_show_info:bool=True):\n        \"\"\"\n        Analyzes a video to identify and extract slides, transitioning between different states \n        (WAITING_OPENING, OPENING, CONTENT, ENDED) based on the content of the video frames.\n        It's based on the EduOpen format, so it will look for the logo and start looking for the text using a state machine based algorithm\n\n        Parameters:\n        _show_info (bool): Flag to control the display of processing information. Defaults to True.\n\n        Returns:\n        None\n        \"\"\"\n        if not self.is_slide_video() or \"slides\" in self.data[\"video_data\"].keys():\n            return\n\n        class State(Enum):\n            WAITING_OPENING = auto()\n            OPENING = auto()\n            CONTENT = auto()\n            ENDED = auto()\n\n\n        video = SimpleVideo(self.video_id)\n        video.set_step(video.get_fps())\n\n        slides:list[VideoSlide] = []\n\n        # We start looking for EduOpen\n        # Then transit to content\n        # Ending is optional (sometimes videos are cut)\n        state_machine = {\"state\": list(State)[0]}\n        next_state = { from_state:to_state for from_state, to_state in list(zip(list(State), list(State)[1:] + [None])) }\n        prev_frame = ImageClassifier(video.get_frame())\n        curr_frame = prev_frame.copy()\n        speed_up_coef = 0.35\n        fps = video.get_fps()\n        max_speed = fps * 10\n        curr_slide = None\n\n        while True:\n\n            # We have finished\n            if not curr_frame.has_image():\n                state_machine[\"state\"] = State.ENDED\n\n            curr_state = state_machine['state']\n\n            # We are looking for the edu (o) pen word (the o is not recognized)\n            if curr_state == State.WAITING_OPENING:\n                if not curr_frame.is_same_image(prev_frame):\n                    text = curr_frame.extract_text(return_text=True)\n                    if \"edu\" in text and \"pen\" in text:\n                        state_machine[\"state\"] = next_state[curr_state]\n                        video.set_step(video.get_fps())\n                else:\n                    video.set_step(np.clip(int(video._curr_step+2), 1, video.get_fps()*2, dtype=int))\n\n            # We are looking in a change in the text that won't have edu and open in the text\n            elif curr_state == State.OPENING:\n                text = curr_frame.extract_text(return_text=True)\n                if not \"edu\" in text or not \"pen\" in text or len(text) &gt; 8 :\n                    state_machine['state'] = next_state[curr_state]\n\n            # We start processing slides reading the text at increasing video speed (capped at max speed) \n            # as we find same text in the image\n            elif curr_state == State.CONTENT:\n                texts_with_bb = curr_frame.extract_text(return_text=True, with_contours=True)\n\n                # Found text\n                if any(texts_with_bb):\n\n                    # Create new slide\n                    if curr_slide is None:\n                        curr_slide = VideoSlide(texts_with_bb, (video.get_frame_index(), None))\n\n                    # Check if it's the same slide as the one cached\n                    else:\n                        new_slide = VideoSlide(texts_with_bb, (video.get_frame_index(),None))\n\n                        # If different append the previous slide setting it's end and reset the playback speed\n                        if new_slide != curr_slide:\n                            curr_slide.start_end_frames[-1] = (curr_slide.start_end_frames[-1][0], video.get_frame_index(True))\n                            slides.append(curr_slide)\n                            curr_slide = new_slide\n                            video.set_step(video.get_fps()//2)\n\n                        # If same slide increase playback speed\n                        else:\n                            video.set_step(int(np.clip(video._curr_step + speed_up_coef * max_speed, 0, max_speed)))\n\n                # Not found text\n                else:\n\n                    # If there is a slide save it, set it's end and reset playback speed\n                    if curr_slide is not None:\n                        curr_slide.start_end_frames[-1] = (curr_slide.start_end_frames[-1][0], video.get_frame_index(True))\n                        slides.append(curr_slide)\n                        curr_slide = None\n                        video.set_step(video.get_fps()//2)\n\n            # If the last slide has not an end_frame because the video ended before, we assign last frame\n            elif curr_state == State.ENDED:\n                if curr_slide is not None and len(slides) and curr_slide != slides[-1]:\n                    curr_slide.start_end_frames[-1] = (curr_slide.start_end_frames[-1][0], video.get_frame_index(True))\n                    slides.append(curr_slide)\n                break   \n\n            prev_frame.set_img(curr_frame.get_img())\n            curr_frame.set_img(video.get_frame())\n\n            if _show_info: print(f\"Doing {np.round((video._curr_frame_idx-video._curr_step)/video.get_count_frames()*100, 2)}%  curr step {video._curr_step} num slides {len(slides)}    \",end=\"\\r\")\n\n        # Cleaning doubles\n        # TODO need to implement method to remove gibberish\n        txt_classif = TextSimilarityClassifier(comp_methods={ComparisonMethods.FUZZY_PARTIAL_RATIO, ComparisonMethods.CHARS_COMMON_DISTRIB})\n        changed = True\n        n_iter = 0\n        while changed:\n            changed = False\n            for to_reverse in [False, True]:\n                for slide1,slide2 in pairwise(slides, None_tail=False, reversed=to_reverse):\n                    if txt_classif.is_partially_in(slide1,slide2):\n                        slide2:VideoSlide\n                        slide2.merge_frames(slide1)\n                        slides.remove(slide1)\n                        changed = True\n                    n_iter += 1\n\n            for slide1, slide2 in double_iterator(slides):\n                if txt_classif.is_partially_in(slide2,slide1):\n                    slide1:VideoSlide\n                    slide1.merge_frames(slide2)\n                    slides.remove(slide2)\n                    changed = True\n                n_iter += 1\n\n        # Now correct slides offsets to the first and last frame they appear\n        # TODO need to implement binary search because it's too slow\n        if False:\n            step = fps//5\n            total = sum([len(slide.start_end_frames) for slide in slides])\n            iter_ = 0\n            print()\n            for slide in slides:\n                this_slide_text = slide._full_text\n                for i, (start_frame, end_frame) in enumerate(slide.start_end_frames):\n\n                    # Shifting start frame backward\n                    video.rewind()\n                    video.roll(start_frame)\n                    video.set_step(-step)\n                    while True:\n                        other_frame_text = curr_frame.set_img(video.get_frame()).extract_text(return_text=True)\n\n                        if (i &gt; 0 and slide.start_end_frames[i-1][1] &gt;= start_frame-1) or \\\n                           (this_slide_text != other_frame_text and not slide.txt_sim_class.are_cosine_similar(this_slide_text, other_frame_text,confidence=0.4)):\n                            start_frame += step\n                            break\n\n                        start_frame -= step\n\n                    # Shifting end frame forward\n                    video.rewind()\n                    video.roll(end_frame)\n                    video.set_step(step)\n                    while True:\n                        other_frame_text = curr_frame.set_img(video.get_frame()).extract_text(return_text=True)\n\n                        if (i+1 &lt; len(slide.start_end_frames) and slide.start_end_frames[i+1][0] &lt;= end_frame+1) or \\\n                           (this_slide_text != other_frame_text and not slide.txt_sim_class.are_cosine_similar(this_slide_text, other_frame_text,confidence=0.4)):\n                            end_frame -= step\n                            break\n\n                        end_frame += step\n\n                    iter_ += 1\n                    print(f\"At {iter_/total*100}%\")\n                    # We reassign the frames\n                    slide.start_end_frames[i] = (start_frame, end_frame)\n\n        # Convert into seconds\n        for slide in slides:\n            for i, (start_frame, end_frame) in enumerate(slide.start_end_frames):\n                slide.start_end_frames[i] = (start_frame/fps, end_frame/fps)\n\n        self.data[\"video_data\"][\"slides\"] = [dict(tft) for tft in slides]\n\n        mongo.insert_video_data(self.data)\n\n\n#######################\n\n\n############ New Methods ###########\n    def identify_language(self, format:Literal['full','pt1']='pt1') -&gt; str:\n        '''\n        Recognizes the video language (currently implemented ita and eng so it raises exception if not one of these)\n        '''\n        if not 'language' in self.data.keys():\n            self.data['language'] = list(YTTranscriptApi.list_transcripts(self.video_id)._generated_transcripts.keys())[0] \n\n        locale = Locale()\n        if not locale.is_language_supported(self.data['language']):\n            raise Exception(f\"Language is not between supported ones: {locale.get_supported_languages()}\")\n        return self.data['language'] if format =='pt1' else locale.get_full_from_pt1(self.data['language'])\n\n    def analyze_transcript(self, _debug_reload_from_json=False):\n\n        #assert self.identify_language() == \"it\", \"implementation error cannot analyze other language transcripts here\"\n        if _debug_reload_from_json:\n            self.data[\"transcript_data\"].pop(\"ItaliaNLP_doc_id\", None)\n            from pathlib import Path\n            from json import load\n            with open(VIDEOS_PATH.joinpath(self.video_id,self.video_id+\".json\")) as f:\n                self.data[\"transcript_data\"][\"text\"] = load(f)[\"segments\"]\n        if \"ItaliaNLP_doc_id\" in self.data[\"transcript_data\"].keys():\n            return\n\n        timed_transcript = self.data[\"transcript_data\"][\"text\"].copy()        \n        language = self.identify_language()\n        doc_id, tagged_transcript = WhisperToPosTagged(language).request_tagged_transcript(self.video_id, timed_transcript)\n\n        #with open(\"transcript.json\",\"w\") as f:\n        #    json.dump({\"transcript\": self.data[\"transcript_data\"][\"text\"], \"lemmas\": self.data[\"transcript_data\"][\"lemmas\"]},f,indent=4)\n\n        self.data[\"transcript_data\"][\"text\"] = tagged_transcript\n        self.data[\"transcript_data\"].update({ \"ItaliaNLP_doc_id\":   doc_id })\n\n        mongo.insert_video_data(self.data)\n\n    def request_terms(self, _debug_recompute:bool=False):\n        if not _debug_recompute:\n            if \"terms\" in self.data[\"transcript_data\"].keys() and any(self.data[\"transcript_data\"][\"terms\"]):\n                return\n\n        language = self.identify_language()\n        if language == \"it\":\n            terms = ItaliaNLAPI().execute_term_extraction(self.data[\"transcript_data\"][\"ItaliaNLP_doc_id\"])\n        elif language == \"en\":\n            terms = extract_keywords_LEGACY(\" \".join(timed_sentence[\"text\"] for timed_sentence in self.data[\"transcript_data\"][\"text\"] if not \"[\" in timed_sentence['text']))\n        self.data[\"transcript_data\"].update({\"terms\": terms.to_dict('records')})\n        mongo.insert_video_data(self.data)\n\n    def filter_terms(self, filtering_opts:dict=None):\n        if filtering_opts is None:\n            filtering_opts = {\"domain_relevance_thresh\": 80}\n        terms = DataFrame(self.data[\"transcript_data\"][\"terms\"]) \n        if terms.empty:\n            terms = DataFrame(columns=[\"term\", \"domain_relevance\", \"frequency\"])\n        self.data[\"transcript_data\"][\"terms\"] = terms[terms[\"domain_relevance\"] &gt; filtering_opts[\"domain_relevance_thresh\"]].to_dict(\"records\")\n\n\n    def lemmatize_an_italian_term(self, term):\n        nlp = NLPSingleton()\n        doc = nlp.lemmatize(term[\"term\"],'it')\n        term[\"lemma\"] = term[\"term\"]\n        head = {}\n        for token in doc:\n            if token.dep_ == \"ROOT\":\n                head[\"text\"] = token.text\n                head[\"lemma\"] = token.lemma_\n                #head[\"gen\"] = token.morph.get(\"Gender\")[0] if len(token.morph.get(\"Gender\")) else \"\"\n                #head[\"num\"] = token.morph.get(\"Number\")[0] if len(token.morph.get(\"Number\")) else \"\"\n\n        # if the lemma of the head matches or there is a frequency of 1, use it as it is\n        if head[\"lemma\"] == head[\"text\"] or term[\"frequency\"] == 1:\n            return term\n\n        lemmas = [token.lemma_ for token in doc]\n\n        words = re.split(r\"(?: )|(?='[^ ]+)\", term[\"term\"])\n        for indx, word in enumerate(words):\n            if word.startswith(\"'\"):\n                words[indx-1] += \"'\"\n                words[indx] = word[1:]\n\n        # Counting term occurrences\n        curr_word_indx = 0\n        term_variants = [[]]\n        if not \"variants\" in term.keys():\n            for sentence in self.data[\"transcript_data\"][\"text\"]:\n                for word in sentence[\"words\"]:\n                    if curr_word_indx == len(lemmas):\n                        term_variants.append([])\n                        curr_word_indx = 0\n                    if word[\"lemma\"] == lemmas[curr_word_indx]:\n                        term_variants[-1].append(word[\"word\"])\n                        curr_word_indx += 1\n                    elif curr_word_indx &gt; 0 and word[\"lemma\"] != lemmas[curr_word_indx]:\n                        term_variants[-1] = []\n                        curr_word_indx = 0\n            term_variants = [\" \".join(occurence) for occurence in term_variants if len(occurence)]\n        else:\n            term_variants = term[\"variants\"]\n\n        # TODO probably a wrong lemmatization and mismatch between spacy and ItaliaNLP\n        # Keep the lemma as the term\n        if len(term_variants) == 0:\n            return term\n\n        # If i have a verb as single lemma and all it's variants are identified as VERB \n        # (minimize risk of context errors of spacy like \"accusa\" in a text like \"l'accusa decise di\" that becomes \"accusare\")\n        # TODO but may still happen in case of few occurrences\n        if len(lemmas) == 1 and doc[0].pos_ == \"VERB\" and len(term_variants) &gt; 0:\n            is_verb = True\n            for variant in term_variants:\n                doc = nlp.lemmatize(variant,'it')\n                if len(doc) != 1 or doc[0].pos_ != \"VERB\":\n                    is_verb = False\n                    break\n            if is_verb:\n                term[\"lemma\"] = lemmas[0]\n                return term            \n\n        # Taking the most recurrent version and the lemmatized version if tie\n        counts = Counter(term_variants).most_common()\n        print(counts, lemmas)\n        targetLemma = (counts[0][0],counts[0][1])\n\n        # Look for all the occurrences, if it's present an occurence equal to the lemmatized version, take that\n        for curr_lemma, curr_occurrences in counts:\n            if curr_lemma == \" \".join(lemmas):\n                term[\"lemma\"] = curr_lemma\n                return term\n\n\n        term[\"lemma\"] = targetLemma[0]\n        return term\n\n    def lemmatize_terms(self):\n        terms = self.data[\"transcript_data\"][\"terms\"]\n        lang = self.identify_language()\n        if lang == \"en\":\n            sem_text = SemanticText(\"\", language=lang)\n            return [\" \".join(sem_text.set_text(term[\"term\"]).lemmatize()).replace(\" \u2019\",\"\u2019\") for term in terms]\n        else:\n            # For every term check if the \n            for term in terms:\n                term.update(self.lemmatize_an_italian_term(term))\n            return [term[\"lemma\"] for term in terms]\n\n####################################\n\n\n    def get_extracted_text(self,format:Literal['str','list','list[text,box]','set[times]','list[text,time,box]','list[time,list[text,box]]','list[id,text,box]']='list'): \n        \"\"\"\n        Returns the text extracted from the video.\\n\n        Text can be cleaned from non-alphanumeric characters or not.\n\n        Parameters :\n        ------------\n        - format (str): The desired format of the output. Defaults to 'list[text_id, timed-tuple]'.\n            - 'str': single string with the text joined together.\\n\n            - 'list': list of VideoSlides\\n\n            - 'set[times] : list of unique texts' times (in seconds) (used for creation of thumbnails)\n            - 'list[time,list[text,box]]': a list of (times, list of (sentence, bounding-box))\n            - 'list[text,box]': list of texts with bounding boxes\n            - 'list[id,text,box]': list of tuple of id, text, and bounding boxes\n            - 'list[text,time,box]': list of repeated times (in seconds) for every text_bounding_boxed\n            - 'list[tuple(id,timed-text)]': list of tuples made of (startend times in seconds, text as string present in those frames)\n        \"\"\"\n        if self._text_in_video is None:\n            return None\n        if format=='list':\n            return self._text_in_video\n        elif format=='str':\n            return ' '.join([tft.get_full_text() for tft in self._text_in_video])\n        elif format=='set[times]':\n            out = []\n            video = LocalVideo(self.video_id)\n            for tft in self._text_in_video:\n                out.extend([(video.get_time_from_num_frame(st_en_frames[0]),video.get_time_from_num_frame(st_en_frames[1])) for st_en_frames in tft.start_end_frames])\n            return out\n        elif format=='list[text,box]':\n            out_lst = []\n            for tft in self._text_in_video:\n                out_lst.extend(tft.get_framed_sentences())\n            return out_lst\n        elif format=='list[id,text,box]':\n            timed_text_with_bb = []\n            _id = 0\n            for tft in self._text_in_video:\n                for sentence,bb in tft.get_framed_sentences():\n                    timed_text_with_bb.append((_id,sentence,bb))\n                    _id +=1\n            return timed_text_with_bb\n        elif format=='list[text,time,box]':\n            timed_text_with_bb = []\n            video = LocalVideo(self.video_id)\n            for tft in self._text_in_video:\n                st_en_frames = tft.start_end_frames[0]\n                for sentence,bb in tft.get_framed_sentences():\n                    timed_text_with_bb.append((sentence.strip('\\n'),(video.get_time_from_num_frame(st_en_frames[0]),video.get_time_from_num_frame(st_en_frames[1])),bb))\n            return timed_text_with_bb\n        elif format=='list[time,list[text,box]]':\n            video = LocalVideo(self.video_id)\n            timed_text = []\n            texts = self._text_in_video\n            for tft in texts:\n                for startend in tft.start_end_frames:\n                    insort_left(timed_text,((video.get_time_from_num_frame(startend[0]),video.get_time_from_num_frame(startend[1])), tft.get_full_text()))\n            return timed_text\n        elif format=='list[tuple(id,timed-text)]':\n            video = LocalVideo(self.video_id)\n            return [(id,(video.get_time_from_num_frame(startend[0]),video.get_time_from_num_frame(startend[1])), tft.get_full_text()) \n                            for id, tft in enumerate(self._text_in_video) \n                            for startend in tft.start_end_frames]\n\n\n    def is_slide_video(self,slide_frames_percent_threshold:float=0.5,_show_info=True):\n        '''\n        Computes a threshold against a value that can be calculated or passed as precomputed_value\n\n        Returns\n        -------\n        value and slide frames if return value is True\\n\n        else\\n\n        True and slide frames if percentage of recognized slidish frames is above the threshold\n        '''\n        if not \"slides_percentage\" in self.data[\"video_data\"].keys():\n            print(self.data[\"video_data\"])\n            self._preprocess_video(vsm=VideoSpeedManager(self.video_id,COLOR_RGB),_show_info=_show_info)\n            mongo.insert_video_data(self.data)\n        return self.data[\"video_data\"]['slides_percentage'] &gt; slide_frames_percent_threshold\n\n\n    def extract_slides_title(self,quant:float=.8,axis_for_outliers_detection:Literal['w','h','wh']='h',union=True,with_times=True) -&gt; list:\n        \"\"\"\n        Titles are extracted by performing statistics on the axis defined, computing a threshold based on quantiles\\n\n        and merging results based on union of results or intersection\\n\n        #TODO improvements: now the analysis is performed on the whole list of sentences, but can be performed:\\n\n            - per slide\\n\n            - then overall\\n\n        but i don't know if can be beneficial, depends on style of presentation. \n        For now the assumption is that there's uniform text across all the slides and titles are generally bigger than the other text\n\n        Then titles are further more filtered by choosing that text whose size is above the threshold, only if it's\n        the first sentence of the slide\\n\n\n        Prerequisites :\n        ------------\n        Must have runned analyze_video()ffmpeg -i /home/gaggio/Documents/Research/ekeel/EVA_apps/EKEELVideoAnnotation/static/videos/lskmIRldsyU/lskmIRldsyU.mp4 -o /home/gaggio/Documents/Research/ekeel/EVA_apps/EKEELVideoAnnotation/static/videos/lskmIRldsyU/lskmIRldsyU.wav\n\n        Parameters :\n        ----------\n        - quant : quantile as lower bound for outlier detection\n        - axis_for_outliers_detection : axis considered for analysis are 'width' or 'height' or both \n        - union : lastly it's perfomed a union of results (logical OR) or an intersection (logical AND)\n        - with_times : if with_times returns a list of tuples(startend_frames, text, bounding_box)\n                otherwise startend_frames are omitted\n\n        Returns :\n        ---------\n        List of all the detected titles\n        \"\"\"\n        assert axis_for_outliers_detection in {'w','h','wh'} and 0 &lt; quant &lt; 1 and self.get_extracted_text(format='list[text,time,box]') is not None\n        # convert input into columns slice for analysis\n        sliced_columns = {'w':slice(2,3),'h':slice(3,4),'wh':slice(2,4)}[axis_for_outliers_detection]\n        texts_with_bb = self.get_extracted_text(format='list[text,time,box]')\n\n        # select columns\n        axis_array = array([text_with_bb[2][sliced_columns] for text_with_bb in texts_with_bb],dtype=dtype('float','float'))\n\n        # compute statistical analysis on columns\n        indices_above_threshold = list(where((axis_array &gt; quantile(axis_array,quant,axis=0)).any(axis=1))[0]) if union else \\\n                                  list(where((axis_array &gt; quantile(axis_array,quant,axis=0)).all(axis=1))[0])\n\n        slides_group = []\n        # group by slide\n        # x[0] is one element of indices_above_threshold to compare, x[1] is another\n        # [1] accesses the startend seconds of that VideoSlide  [text, startend, bounding_boxes]\n        # [0] picks the start second of that object [start_second, end_second]\n        for _,g in groupby(enumerate(indices_above_threshold),lambda x: texts_with_bb[x[0]][1][0] - texts_with_bb[x[1]][1][0]):\n            slides_group.append(list(reversed(list(map(lambda x:x[1], g)))))\n        slides_group = list(reversed(slides_group))\n\n        # remove indices of text that are classified as titles but are just big text that's not at the top of every slide\n        for group in slides_group:\n            for indx_text in group:\n                if indx_text &gt; 0 and indx_text-1 not in group and (                        # if i'm not at the first index and the text of the previous index is not in the group\n                   texts_with_bb[indx_text-1][1][0] == texts_with_bb[indx_text][1][0] and  # and the text is in the same slide\n                   texts_with_bb[indx_text-1][2][1] &lt; texts_with_bb[indx_text][2][1]):     # and has a y value lower than my current text (there's another sentence before this one)\n                    indices_above_threshold.remove(indx_text)\n\n        # selects the texts from the list of texts\n        if len(indices_above_threshold) == 0:\n            return None\n\n        if not with_times:\n            if len(indices_above_threshold) &gt; 1:\n                return itemgetter(*indices_above_threshold)(list(zip(*list(zip(*texts_with_bb))[0:3:2])))\n            return [itemgetter(*indices_above_threshold)(list(zip(*list(zip(*texts_with_bb))[0:3:2])))]\n        if len(indices_above_threshold) &gt; 1:\n            return itemgetter(*indices_above_threshold)(texts_with_bb)\n        return [itemgetter(*indices_above_threshold)(texts_with_bb)]\n\n\n    def create_thumbnails(self):\n        '''\n        Create thumbnails of keyframes from slide segmentation\n        '''\n\n        #times = self._slide_startends\n        #if times is None:\n        #    times = sorted(self.get_extracted_text(format='set[times]'))\n        #else:\n        #    times = sorted(times)\n\n        images_path = []\n        path = VIDEOS_PATH.joinpath(self.video_id) #os.path.dirname(os.path.abspath(__file__))\n        if any(File.endswith(\".jpg\") for File in os.listdir(path)):\n            for file in sorted(os.listdir(path)):\n                if file.endswith(\".jpg\"):\n                    images_path.append(f\"videos/{self.video_id}/{file}\")\n            self.images_path = images_path\n            return\n\n        times = self.data[\"video_data\"][\"segments\"]\n        video = LocalVideo(self.video_id)\n        for i,(start_seconds,_) in enumerate(times):\n            video.set_num_frame(video.get_num_frame_from_time(start_seconds+0.5))\n            image = video.extract_next_frame()\n            file_name = str(i) + \".jpg\"\n            image_file_dir = path.joinpath(file_name)\n            cv2.imwrite(image_file_dir.__str__(), image)\n            images_path.append(f\"videos/{self.video_id}/{i}.jpg\")\n\n        self.images_path = images_path\n\n    @staticmethod\n    def seconds_to_h_mm_ss_dddddd(time:float):\n        millisec = str(time%1)[2:8]\n        millisec += '0'*(6-len(millisec))\n        seconds = str(int(time)%60)\n        seconds = '0'*(2-len(seconds)) + seconds\n        minutes = str(int(time/60))\n        minutes = '0'*(2-len(minutes)) + minutes\n        hours = str(int(time/3600))\n        return hours+':'+minutes+':'+seconds+'.'+millisec\n\n\n    def adjust_or_insert_definitions_and_indepth_times(self,burst_concepts:List[dict],definition_tol_seconds:float = 3,_show_output=False):\n        '''\n        This is an attempt to find definitions from timed sentences of the transcript and the timed titles of the slides.\\n\n        Heuristic is that if there's a keyword in the title of a slide (frontpage slide excluded)\n        find the first occurence of that keyword in the transcript within a tolerance seconds window before and after the appearance of the slide\n        set that as \"definition\" only if it contains the keyword of the title .\\n \n        Heuristic for the in-depth is that after definition there's an in-depth of the slide, this means that the concept is explained further there, until the slide with that title won't disappear.\\n\n\n        On the algorithmic side sentences of the transcript used by the heuristic are mapped to the conll version of the text,\n        cleaning operation is performed to remove errors (it groups the contiguous hit of every used sentence of the transcript to the conll by groups len and picks the biggest)\\n\n        Then the mapped sentences are aggregated by start and end sentence ids\\n\n        Lastly if the burst analysis has different definition times it is overwritten, if it does not contain the definition, this is appended at the end\n        '''\n        if self._slide_titles is None:\n            raise Exception('slide titles not set')\n\n\n        # extract definitions and in-depths in the transcript of every title based on slide show time and concept citation (especially with definition)\n        timed_sentences = get_timed_sentences(self.request_transcript()[0],[sent.data[\"text\"] for sent in parse(get_text(self.video_id,return_conll=True)[1])])\n\n        video_defs = {}\n        video_in_depths = {}\n        is_introductory_slide = True\n        for title in self._slide_titles:\n            if is_introductory_slide or title['start_end_seconds'] == start_end_times_introductory_slides: # TODO is there always an introductory slide?\n                start_end_times_introductory_slides = title['start_end_seconds']\n                is_introductory_slide = False\n            else:\n                start_time_title,end_time_title = title['start_end_seconds']\n                title_lowered = title[\"text\"].lower()\n                title_keyword = [burst_concept['concept'] for burst_concept in burst_concepts if burst_concept['concept'] in title_lowered]\n                if len(title_keyword) &gt; 0:\n                    title_keyword = title_keyword[0]\n                else:\n                    title_keyword = SemanticText(title['text'],self.data['language']).extract_keywords_from_title()[0]\n                for sent_id, timed_sentence in enumerate(timed_sentences):\n                    if title_keyword not in video_defs.keys() and \\\n                       abs(start_time_title - timed_sentence['start']) &lt; definition_tol_seconds and \\\n                       title_keyword in timed_sentence['text']:\n                        if _show_output:\n                            print()\n                            print('********** Here comes the definition of the following keyword *******')\n                            print(f'keyword from title: {title_keyword}')\n                            print(f\"time: {str(timed_sentence['start'])[:5]} : {str(timed_sentence['end'])[:5]}  |  sentence: {timed_sentence['text']}\")\n                            print()\n                        #timed_sentence['id'] = ts_id\n                        video_defs[title_keyword] = [(sent_id,timed_sentence)]\n\n                    # enlarge end time threshold to incorporate split slides with the same title\n                    if title_keyword in video_defs.keys() and \\\n                       end_time_title &gt; timed_sentence['end'] - 1 and \\\n                       timed_sentence['start'] &gt; video_defs[title_keyword][0][1]['start']: # and \\\n                       #txt_classif.is_partially_in_txt_version(title_keywords[0],timed_sentence['text']):\n                        if title_keyword not in video_in_depths.keys():\n                            if _show_output:\n                                print('********** Here comes the indepth of the following keyword *******')\n                                print(f'keyword from title: {title_keyword}')\n                                print(f\"time: {str(timed_sentence['start'])[:5]} : {str(timed_sentence['end'])[:5]}  |  sentence: {timed_sentence['text']}\")\n                            #timed_sentence['id'] = ts_id\n                            video_in_depths[title_keyword] = [(sent_id,timed_sentence)]\n\n                        elif not any([True for _,tmd_sentence in video_in_depths[title_keyword] if tmd_sentence['start'] == timed_sentence['start']]):\n                            #timed_sentence['id'] = ts_id\n                            video_in_depths[title_keyword].append((sent_id,timed_sentence))\n                            if _show_output:\n                                print(f\"time: {str(timed_sentence['start'])[:5]} : {str(timed_sentence['end'])[:5]}  |  sentence: {timed_sentence['text']}\")\n\n        # Creating or modifying burst_concept definition of the video results \n        added_concepts = []\n        concepts_used = {concept:False for concept in video_defs.keys()}\n        concept_description_type = \"Definition\"\n        for burst_concept in burst_concepts:\n            if burst_concept[\"concept\"] in video_defs.keys() and burst_concept[\"description_type\"] == concept_description_type:\n\n                if burst_concept[\"start_sent_id\"] != video_defs[burst_concept[\"concept\"]][0][0] or \\\n                   burst_concept[\"end_sent_id\"] != video_defs[burst_concept[\"concept\"]][-1][0]:\n                    burst_concept_name = burst_concept['concept']\n                    burst_concept['start_sent_id'] = video_defs[burst_concept_name][0][0]\n                    burst_concept['end_sent_id'] = video_defs[burst_concept_name][-1][0]\n                    burst_concept['start'] = self.seconds_to_h_mm_ss_dddddd(video_defs[burst_concept_name][0][1][\"start\"])\n                    burst_concept['end'] = self.seconds_to_h_mm_ss_dddddd(video_defs[burst_concept_name][-1][1][\"end\"])\n                    burst_concept['creator'] = \"Video_Analysis\"\n                    concepts_used[burst_concept_name] = True\n\n        for concept_name in video_defs.keys():\n            if not concepts_used[concept_name]:\n                burst_concepts.append({ 'concept':concept_name,\n                                        'start_sent_id':video_defs[concept_name][0][0],\n                                        'end_sent_id':video_defs[concept_name][-1][0],\n                                        'start':self.seconds_to_h_mm_ss_dddddd(video_defs[concept_name][0][1]['start']),\n                                        'end':self.seconds_to_h_mm_ss_dddddd(video_defs[concept_name][-1][1][\"end\"]),\n                                        'description_type':concept_description_type,\n                                        'creator':'Video_Analysis'})\n                if not concept_name in added_concepts:\n                    added_concepts.append(concept_name)\n\n        # In Depths must be managed differently since there can be more than one\n        concepts_used = {concept:False for concept in video_in_depths.keys()}\n        concept_description_type = \"In Depth\"\n        for video_concept_name in video_in_depths.keys():\n            most_proximal = {'found':False}\n            for id_, burst_concept in reversed(list(enumerate(burst_concepts))):\n\n                if burst_concept[\"concept\"] == video_concept_name and burst_concept[\"description_type\"] == concept_description_type:\n                    if not most_proximal[\"found\"]:\n                        most_proximal['found'] = True\n                        most_proximal[\"id\"] = id_\n                        most_proximal['diff_start_sent_id'] = abs(burst_concept['start_sent_id']-video_in_depths[video_concept_name][0][0])\n                        most_proximal['diff_end_sent_id'] = abs(burst_concept['end_sent_id']-video_in_depths[video_concept_name][-1][0])\n                    else:\n                        if most_proximal['diff_start_sent_id'] &gt; abs(burst_concept['start_sent_id']-video_in_depths[video_concept_name][0][0]):\n                            most_proximal[\"id\"] = id_\n                            most_proximal['diff_start_sent_id'] = abs(burst_concept['start_sent_id']-video_in_depths[video_concept_name][0][0])\n                            most_proximal['diff_end_sent_id'] = abs(burst_concept['end_sent_id']-video_in_depths[video_concept_name][-1][0])\n\n                elif most_proximal['found'] and burst_concept[\"concept\"] != video_concept_name:\n                    target_concept = burst_concepts[most_proximal['id']]\n                    target_concept['start'] = self.seconds_to_h_mm_ss_dddddd(video_in_depths[video_concept_name][0][1][\"start\"])\n                    target_concept['end'] = self.seconds_to_h_mm_ss_dddddd(video_in_depths[video_concept_name][-1][1][\"end\"])\n                    target_concept['start_sent_id'] = video_in_depths[video_concept_name][0][0]\n                    target_concept['end_sent_id'] = video_in_depths[video_concept_name][-1][0]\n                    target_concept['creator'] = 'Video_Analysis'\n                    break\n\n            if not most_proximal['found']:\n                burst_concepts.append({ 'concept':video_concept_name,\n                                        'start_sent_id':video_in_depths[video_concept_name][0][0],\n                                        'end_sent_id':video_in_depths[video_concept_name][-1][0],\n                                        'start':self.seconds_to_h_mm_ss_dddddd(video_in_depths[video_concept_name][0][1]['start']),\n                                        'end':self.seconds_to_h_mm_ss_dddddd(video_in_depths[video_concept_name][-1][1][\"end\"]),\n                                        'description_type':concept_description_type,\n                                        'creator':'Video_Analysis'})\n                if not concept_name in added_concepts:\n                    added_concepts.append(concept_name)\n\n        return added_concepts,burst_concepts\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.adjust_or_insert_definitions_and_indepth_times","title":"<code>adjust_or_insert_definitions_and_indepth_times(burst_concepts, definition_tol_seconds=3, _show_output=False)</code>","text":"<p>This is an attempt to find definitions from timed sentences of the transcript and the timed titles of the slides.</p> <p>Heuristic is that if there's a keyword in the title of a slide (frontpage slide excluded) find the first occurence of that keyword in the transcript within a tolerance seconds window before and after the appearance of the slide set that as \"definition\" only if it contains the keyword of the title .</p> <p>Heuristic for the in-depth is that after definition there's an in-depth of the slide, this means that the concept is explained further there, until the slide with that title won't disappear.</p> <p>On the algorithmic side sentences of the transcript used by the heuristic are mapped to the conll version of the text, cleaning operation is performed to remove errors (it groups the contiguous hit of every used sentence of the transcript to the conll by groups len and picks the biggest)</p> <p>Then the mapped sentences are aggregated by start and end sentence ids</p> <p>Lastly if the burst analysis has different definition times it is overwritten, if it does not contain the definition, this is appended at the end</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def adjust_or_insert_definitions_and_indepth_times(self,burst_concepts:List[dict],definition_tol_seconds:float = 3,_show_output=False):\n    '''\n    This is an attempt to find definitions from timed sentences of the transcript and the timed titles of the slides.\\n\n    Heuristic is that if there's a keyword in the title of a slide (frontpage slide excluded)\n    find the first occurence of that keyword in the transcript within a tolerance seconds window before and after the appearance of the slide\n    set that as \"definition\" only if it contains the keyword of the title .\\n \n    Heuristic for the in-depth is that after definition there's an in-depth of the slide, this means that the concept is explained further there, until the slide with that title won't disappear.\\n\n\n    On the algorithmic side sentences of the transcript used by the heuristic are mapped to the conll version of the text,\n    cleaning operation is performed to remove errors (it groups the contiguous hit of every used sentence of the transcript to the conll by groups len and picks the biggest)\\n\n    Then the mapped sentences are aggregated by start and end sentence ids\\n\n    Lastly if the burst analysis has different definition times it is overwritten, if it does not contain the definition, this is appended at the end\n    '''\n    if self._slide_titles is None:\n        raise Exception('slide titles not set')\n\n\n    # extract definitions and in-depths in the transcript of every title based on slide show time and concept citation (especially with definition)\n    timed_sentences = get_timed_sentences(self.request_transcript()[0],[sent.data[\"text\"] for sent in parse(get_text(self.video_id,return_conll=True)[1])])\n\n    video_defs = {}\n    video_in_depths = {}\n    is_introductory_slide = True\n    for title in self._slide_titles:\n        if is_introductory_slide or title['start_end_seconds'] == start_end_times_introductory_slides: # TODO is there always an introductory slide?\n            start_end_times_introductory_slides = title['start_end_seconds']\n            is_introductory_slide = False\n        else:\n            start_time_title,end_time_title = title['start_end_seconds']\n            title_lowered = title[\"text\"].lower()\n            title_keyword = [burst_concept['concept'] for burst_concept in burst_concepts if burst_concept['concept'] in title_lowered]\n            if len(title_keyword) &gt; 0:\n                title_keyword = title_keyword[0]\n            else:\n                title_keyword = SemanticText(title['text'],self.data['language']).extract_keywords_from_title()[0]\n            for sent_id, timed_sentence in enumerate(timed_sentences):\n                if title_keyword not in video_defs.keys() and \\\n                   abs(start_time_title - timed_sentence['start']) &lt; definition_tol_seconds and \\\n                   title_keyword in timed_sentence['text']:\n                    if _show_output:\n                        print()\n                        print('********** Here comes the definition of the following keyword *******')\n                        print(f'keyword from title: {title_keyword}')\n                        print(f\"time: {str(timed_sentence['start'])[:5]} : {str(timed_sentence['end'])[:5]}  |  sentence: {timed_sentence['text']}\")\n                        print()\n                    #timed_sentence['id'] = ts_id\n                    video_defs[title_keyword] = [(sent_id,timed_sentence)]\n\n                # enlarge end time threshold to incorporate split slides with the same title\n                if title_keyword in video_defs.keys() and \\\n                   end_time_title &gt; timed_sentence['end'] - 1 and \\\n                   timed_sentence['start'] &gt; video_defs[title_keyword][0][1]['start']: # and \\\n                   #txt_classif.is_partially_in_txt_version(title_keywords[0],timed_sentence['text']):\n                    if title_keyword not in video_in_depths.keys():\n                        if _show_output:\n                            print('********** Here comes the indepth of the following keyword *******')\n                            print(f'keyword from title: {title_keyword}')\n                            print(f\"time: {str(timed_sentence['start'])[:5]} : {str(timed_sentence['end'])[:5]}  |  sentence: {timed_sentence['text']}\")\n                        #timed_sentence['id'] = ts_id\n                        video_in_depths[title_keyword] = [(sent_id,timed_sentence)]\n\n                    elif not any([True for _,tmd_sentence in video_in_depths[title_keyword] if tmd_sentence['start'] == timed_sentence['start']]):\n                        #timed_sentence['id'] = ts_id\n                        video_in_depths[title_keyword].append((sent_id,timed_sentence))\n                        if _show_output:\n                            print(f\"time: {str(timed_sentence['start'])[:5]} : {str(timed_sentence['end'])[:5]}  |  sentence: {timed_sentence['text']}\")\n\n    # Creating or modifying burst_concept definition of the video results \n    added_concepts = []\n    concepts_used = {concept:False for concept in video_defs.keys()}\n    concept_description_type = \"Definition\"\n    for burst_concept in burst_concepts:\n        if burst_concept[\"concept\"] in video_defs.keys() and burst_concept[\"description_type\"] == concept_description_type:\n\n            if burst_concept[\"start_sent_id\"] != video_defs[burst_concept[\"concept\"]][0][0] or \\\n               burst_concept[\"end_sent_id\"] != video_defs[burst_concept[\"concept\"]][-1][0]:\n                burst_concept_name = burst_concept['concept']\n                burst_concept['start_sent_id'] = video_defs[burst_concept_name][0][0]\n                burst_concept['end_sent_id'] = video_defs[burst_concept_name][-1][0]\n                burst_concept['start'] = self.seconds_to_h_mm_ss_dddddd(video_defs[burst_concept_name][0][1][\"start\"])\n                burst_concept['end'] = self.seconds_to_h_mm_ss_dddddd(video_defs[burst_concept_name][-1][1][\"end\"])\n                burst_concept['creator'] = \"Video_Analysis\"\n                concepts_used[burst_concept_name] = True\n\n    for concept_name in video_defs.keys():\n        if not concepts_used[concept_name]:\n            burst_concepts.append({ 'concept':concept_name,\n                                    'start_sent_id':video_defs[concept_name][0][0],\n                                    'end_sent_id':video_defs[concept_name][-1][0],\n                                    'start':self.seconds_to_h_mm_ss_dddddd(video_defs[concept_name][0][1]['start']),\n                                    'end':self.seconds_to_h_mm_ss_dddddd(video_defs[concept_name][-1][1][\"end\"]),\n                                    'description_type':concept_description_type,\n                                    'creator':'Video_Analysis'})\n            if not concept_name in added_concepts:\n                added_concepts.append(concept_name)\n\n    # In Depths must be managed differently since there can be more than one\n    concepts_used = {concept:False for concept in video_in_depths.keys()}\n    concept_description_type = \"In Depth\"\n    for video_concept_name in video_in_depths.keys():\n        most_proximal = {'found':False}\n        for id_, burst_concept in reversed(list(enumerate(burst_concepts))):\n\n            if burst_concept[\"concept\"] == video_concept_name and burst_concept[\"description_type\"] == concept_description_type:\n                if not most_proximal[\"found\"]:\n                    most_proximal['found'] = True\n                    most_proximal[\"id\"] = id_\n                    most_proximal['diff_start_sent_id'] = abs(burst_concept['start_sent_id']-video_in_depths[video_concept_name][0][0])\n                    most_proximal['diff_end_sent_id'] = abs(burst_concept['end_sent_id']-video_in_depths[video_concept_name][-1][0])\n                else:\n                    if most_proximal['diff_start_sent_id'] &gt; abs(burst_concept['start_sent_id']-video_in_depths[video_concept_name][0][0]):\n                        most_proximal[\"id\"] = id_\n                        most_proximal['diff_start_sent_id'] = abs(burst_concept['start_sent_id']-video_in_depths[video_concept_name][0][0])\n                        most_proximal['diff_end_sent_id'] = abs(burst_concept['end_sent_id']-video_in_depths[video_concept_name][-1][0])\n\n            elif most_proximal['found'] and burst_concept[\"concept\"] != video_concept_name:\n                target_concept = burst_concepts[most_proximal['id']]\n                target_concept['start'] = self.seconds_to_h_mm_ss_dddddd(video_in_depths[video_concept_name][0][1][\"start\"])\n                target_concept['end'] = self.seconds_to_h_mm_ss_dddddd(video_in_depths[video_concept_name][-1][1][\"end\"])\n                target_concept['start_sent_id'] = video_in_depths[video_concept_name][0][0]\n                target_concept['end_sent_id'] = video_in_depths[video_concept_name][-1][0]\n                target_concept['creator'] = 'Video_Analysis'\n                break\n\n        if not most_proximal['found']:\n            burst_concepts.append({ 'concept':video_concept_name,\n                                    'start_sent_id':video_in_depths[video_concept_name][0][0],\n                                    'end_sent_id':video_in_depths[video_concept_name][-1][0],\n                                    'start':self.seconds_to_h_mm_ss_dddddd(video_in_depths[video_concept_name][0][1]['start']),\n                                    'end':self.seconds_to_h_mm_ss_dddddd(video_in_depths[video_concept_name][-1][1][\"end\"]),\n                                    'description_type':concept_description_type,\n                                    'creator':'Video_Analysis'})\n            if not concept_name in added_concepts:\n                added_concepts.append(concept_name)\n\n    return added_concepts,burst_concepts\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.analyze_video","title":"<code>analyze_video(_show_info=True)</code>","text":"<p>Analyzes a video to identify and extract slides, transitioning between different states  (WAITING_OPENING, OPENING, CONTENT, ENDED) based on the content of the video frames. It's based on the EduOpen format, so it will look for the logo and start looking for the text using a state machine based algorithm</p> <p>Parameters: _show_info (bool): Flag to control the display of processing information. Defaults to True.</p> <p>Returns: None</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def analyze_video(self,_show_info:bool=True):\n    \"\"\"\n    Analyzes a video to identify and extract slides, transitioning between different states \n    (WAITING_OPENING, OPENING, CONTENT, ENDED) based on the content of the video frames.\n    It's based on the EduOpen format, so it will look for the logo and start looking for the text using a state machine based algorithm\n\n    Parameters:\n    _show_info (bool): Flag to control the display of processing information. Defaults to True.\n\n    Returns:\n    None\n    \"\"\"\n    if not self.is_slide_video() or \"slides\" in self.data[\"video_data\"].keys():\n        return\n\n    class State(Enum):\n        WAITING_OPENING = auto()\n        OPENING = auto()\n        CONTENT = auto()\n        ENDED = auto()\n\n\n    video = SimpleVideo(self.video_id)\n    video.set_step(video.get_fps())\n\n    slides:list[VideoSlide] = []\n\n    # We start looking for EduOpen\n    # Then transit to content\n    # Ending is optional (sometimes videos are cut)\n    state_machine = {\"state\": list(State)[0]}\n    next_state = { from_state:to_state for from_state, to_state in list(zip(list(State), list(State)[1:] + [None])) }\n    prev_frame = ImageClassifier(video.get_frame())\n    curr_frame = prev_frame.copy()\n    speed_up_coef = 0.35\n    fps = video.get_fps()\n    max_speed = fps * 10\n    curr_slide = None\n\n    while True:\n\n        # We have finished\n        if not curr_frame.has_image():\n            state_machine[\"state\"] = State.ENDED\n\n        curr_state = state_machine['state']\n\n        # We are looking for the edu (o) pen word (the o is not recognized)\n        if curr_state == State.WAITING_OPENING:\n            if not curr_frame.is_same_image(prev_frame):\n                text = curr_frame.extract_text(return_text=True)\n                if \"edu\" in text and \"pen\" in text:\n                    state_machine[\"state\"] = next_state[curr_state]\n                    video.set_step(video.get_fps())\n            else:\n                video.set_step(np.clip(int(video._curr_step+2), 1, video.get_fps()*2, dtype=int))\n\n        # We are looking in a change in the text that won't have edu and open in the text\n        elif curr_state == State.OPENING:\n            text = curr_frame.extract_text(return_text=True)\n            if not \"edu\" in text or not \"pen\" in text or len(text) &gt; 8 :\n                state_machine['state'] = next_state[curr_state]\n\n        # We start processing slides reading the text at increasing video speed (capped at max speed) \n        # as we find same text in the image\n        elif curr_state == State.CONTENT:\n            texts_with_bb = curr_frame.extract_text(return_text=True, with_contours=True)\n\n            # Found text\n            if any(texts_with_bb):\n\n                # Create new slide\n                if curr_slide is None:\n                    curr_slide = VideoSlide(texts_with_bb, (video.get_frame_index(), None))\n\n                # Check if it's the same slide as the one cached\n                else:\n                    new_slide = VideoSlide(texts_with_bb, (video.get_frame_index(),None))\n\n                    # If different append the previous slide setting it's end and reset the playback speed\n                    if new_slide != curr_slide:\n                        curr_slide.start_end_frames[-1] = (curr_slide.start_end_frames[-1][0], video.get_frame_index(True))\n                        slides.append(curr_slide)\n                        curr_slide = new_slide\n                        video.set_step(video.get_fps()//2)\n\n                    # If same slide increase playback speed\n                    else:\n                        video.set_step(int(np.clip(video._curr_step + speed_up_coef * max_speed, 0, max_speed)))\n\n            # Not found text\n            else:\n\n                # If there is a slide save it, set it's end and reset playback speed\n                if curr_slide is not None:\n                    curr_slide.start_end_frames[-1] = (curr_slide.start_end_frames[-1][0], video.get_frame_index(True))\n                    slides.append(curr_slide)\n                    curr_slide = None\n                    video.set_step(video.get_fps()//2)\n\n        # If the last slide has not an end_frame because the video ended before, we assign last frame\n        elif curr_state == State.ENDED:\n            if curr_slide is not None and len(slides) and curr_slide != slides[-1]:\n                curr_slide.start_end_frames[-1] = (curr_slide.start_end_frames[-1][0], video.get_frame_index(True))\n                slides.append(curr_slide)\n            break   \n\n        prev_frame.set_img(curr_frame.get_img())\n        curr_frame.set_img(video.get_frame())\n\n        if _show_info: print(f\"Doing {np.round((video._curr_frame_idx-video._curr_step)/video.get_count_frames()*100, 2)}%  curr step {video._curr_step} num slides {len(slides)}    \",end=\"\\r\")\n\n    # Cleaning doubles\n    # TODO need to implement method to remove gibberish\n    txt_classif = TextSimilarityClassifier(comp_methods={ComparisonMethods.FUZZY_PARTIAL_RATIO, ComparisonMethods.CHARS_COMMON_DISTRIB})\n    changed = True\n    n_iter = 0\n    while changed:\n        changed = False\n        for to_reverse in [False, True]:\n            for slide1,slide2 in pairwise(slides, None_tail=False, reversed=to_reverse):\n                if txt_classif.is_partially_in(slide1,slide2):\n                    slide2:VideoSlide\n                    slide2.merge_frames(slide1)\n                    slides.remove(slide1)\n                    changed = True\n                n_iter += 1\n\n        for slide1, slide2 in double_iterator(slides):\n            if txt_classif.is_partially_in(slide2,slide1):\n                slide1:VideoSlide\n                slide1.merge_frames(slide2)\n                slides.remove(slide2)\n                changed = True\n            n_iter += 1\n\n    # Now correct slides offsets to the first and last frame they appear\n    # TODO need to implement binary search because it's too slow\n    if False:\n        step = fps//5\n        total = sum([len(slide.start_end_frames) for slide in slides])\n        iter_ = 0\n        print()\n        for slide in slides:\n            this_slide_text = slide._full_text\n            for i, (start_frame, end_frame) in enumerate(slide.start_end_frames):\n\n                # Shifting start frame backward\n                video.rewind()\n                video.roll(start_frame)\n                video.set_step(-step)\n                while True:\n                    other_frame_text = curr_frame.set_img(video.get_frame()).extract_text(return_text=True)\n\n                    if (i &gt; 0 and slide.start_end_frames[i-1][1] &gt;= start_frame-1) or \\\n                       (this_slide_text != other_frame_text and not slide.txt_sim_class.are_cosine_similar(this_slide_text, other_frame_text,confidence=0.4)):\n                        start_frame += step\n                        break\n\n                    start_frame -= step\n\n                # Shifting end frame forward\n                video.rewind()\n                video.roll(end_frame)\n                video.set_step(step)\n                while True:\n                    other_frame_text = curr_frame.set_img(video.get_frame()).extract_text(return_text=True)\n\n                    if (i+1 &lt; len(slide.start_end_frames) and slide.start_end_frames[i+1][0] &lt;= end_frame+1) or \\\n                       (this_slide_text != other_frame_text and not slide.txt_sim_class.are_cosine_similar(this_slide_text, other_frame_text,confidence=0.4)):\n                        end_frame -= step\n                        break\n\n                    end_frame += step\n\n                iter_ += 1\n                print(f\"At {iter_/total*100}%\")\n                # We reassign the frames\n                slide.start_end_frames[i] = (start_frame, end_frame)\n\n    # Convert into seconds\n    for slide in slides:\n        for i, (start_frame, end_frame) in enumerate(slide.start_end_frames):\n            slide.start_end_frames[i] = (start_frame/fps, end_frame/fps)\n\n    self.data[\"video_data\"][\"slides\"] = [dict(tft) for tft in slides]\n\n    mongo.insert_video_data(self.data)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.create_thumbnails","title":"<code>create_thumbnails()</code>","text":"<p>Create thumbnails of keyframes from slide segmentation</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def create_thumbnails(self):\n    '''\n    Create thumbnails of keyframes from slide segmentation\n    '''\n\n    #times = self._slide_startends\n    #if times is None:\n    #    times = sorted(self.get_extracted_text(format='set[times]'))\n    #else:\n    #    times = sorted(times)\n\n    images_path = []\n    path = VIDEOS_PATH.joinpath(self.video_id) #os.path.dirname(os.path.abspath(__file__))\n    if any(File.endswith(\".jpg\") for File in os.listdir(path)):\n        for file in sorted(os.listdir(path)):\n            if file.endswith(\".jpg\"):\n                images_path.append(f\"videos/{self.video_id}/{file}\")\n        self.images_path = images_path\n        return\n\n    times = self.data[\"video_data\"][\"segments\"]\n    video = LocalVideo(self.video_id)\n    for i,(start_seconds,_) in enumerate(times):\n        video.set_num_frame(video.get_num_frame_from_time(start_seconds+0.5))\n        image = video.extract_next_frame()\n        file_name = str(i) + \".jpg\"\n        image_file_dir = path.joinpath(file_name)\n        cv2.imwrite(image_file_dir.__str__(), image)\n        images_path.append(f\"videos/{self.video_id}/{i}.jpg\")\n\n    self.images_path = images_path\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.download_video","title":"<code>download_video()</code>","text":"<p>Downloads the video (expected YouTube video)</p> <p>If the video has been removed from youtube, it attempts to remove the folder from both the drive and the database, then raises an Exception</p>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.download_video--note","title":"NOTE","text":"<p>There are problems very often with YouTube, which breaks yt_dlp library.</p> <p>In case of errors try to update the library</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def download_video(self):\n    '''\n    Downloads the video (expected YouTube video)\\n\n    If the video has been removed from youtube, it attempts to remove the folder from both the drive and the database, then raises an Exception\\n\n\n    --------------\n    # NOTE\n    There are problems very often with YouTube, which breaks yt_dlp library.\\n \n    In case of errors try to update the library\n    '''\n    url = self.url\n    #video_link = url.split('&amp;')[0]\n    video_id = self.video_id\n    folder_path = self.folder_path\n\n    os.makedirs(folder_path, exist_ok=True)\n\n    if os.path.isfile(os.path.join(folder_path,video_id+'.mp4')):\n        return\n\n    # Both pafy and pytube seems to be not mantained anymore, only youtube_dlp is still alive\n\n    prev_cwd = os.getcwd()\n    os.chdir(folder_path)\n\n    try:\n        #print(\"using ytdl\")\n        #yt_dlp.YoutubeDL({'format': 'bestvideo[height&lt;=480]', 'quiet': True}).download([url])\n        yt_dlp.YoutubeDL({  'quiet': False,\n                            'format': 'bestvideo[height&lt;=720]+bestaudio/best[height&lt;=720]',\n                            'outtmpl': video_id+'.mp4',\n                            'merge_output_format': 'mp4',\n                            'ffmpeg_location': '/usr/bin/ffmpeg',\n                            'postprocessors': [{\n                                'key': 'FFmpegVideoConvertor',\n                                'preferedformat': 'mp4',  # Ensure the output is in mp4 format\n                                }],\n                              }).download([url])\n    except Exception as e:\n        print(e)\n\n    os.chdir(prev_cwd)\n\n    for file in os.listdir(folder_path):\n        if file.endswith(\".mp4\"):\n            os.rename(folder_path.joinpath(file),folder_path.joinpath(video_id+\".\"+file.split(\".\")[-1]))\n            vidcap = cv2.VideoCapture(folder_path.joinpath(video_id+\".\"+file.split(\".\")[-1]))\n            if not vidcap.isOpened() or not min((vidcap.get(cv2.CAP_PROP_FRAME_WIDTH),vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))) &gt;= 360:\n                raise Exception(\"Video does not have enough definition to find text\")\n            break\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.extract_slides_title","title":"<code>extract_slides_title(quant=0.8, axis_for_outliers_detection='h', union=True, with_times=True)</code>","text":"<p>Titles are extracted by performing statistics on the axis defined, computing a threshold based on quantiles</p> <p>and merging results based on union of results or intersection</p>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.extract_slides_title--todo-improvements-now-the-analysis-is-performed-on-the-whole-list-of-sentences-but-can-be-performed","title":"TODO improvements: now the analysis is performed on the whole list of sentences, but can be performed:","text":"<pre><code>- per slide\n\n- then overall\n</code></pre> <p>but i don't know if can be beneficial, depends on style of presentation.  For now the assumption is that there's uniform text across all the slides and titles are generally bigger than the other text</p> <p>Then titles are further more filtered by choosing that text whose size is above the threshold, only if it's the first sentence of the slide</p> Prerequisites : <p>Must have runned analyze_video()ffmpeg -i /home/gaggio/Documents/Research/ekeel/EVA_apps/EKEELVideoAnnotation/static/videos/lskmIRldsyU/lskmIRldsyU.mp4 -o /home/gaggio/Documents/Research/ekeel/EVA_apps/EKEELVideoAnnotation/static/videos/lskmIRldsyU/lskmIRldsyU.wav</p> Parameters : <ul> <li>quant : quantile as lower bound for outlier detection</li> <li>axis_for_outliers_detection : axis considered for analysis are 'width' or 'height' or both </li> <li>union : lastly it's perfomed a union of results (logical OR) or an intersection (logical AND)</li> <li>with_times : if with_times returns a list of tuples(startend_frames, text, bounding_box)         otherwise startend_frames are omitted</li> </ul> Returns : <p>List of all the detected titles</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def extract_slides_title(self,quant:float=.8,axis_for_outliers_detection:Literal['w','h','wh']='h',union=True,with_times=True) -&gt; list:\n    \"\"\"\n    Titles are extracted by performing statistics on the axis defined, computing a threshold based on quantiles\\n\n    and merging results based on union of results or intersection\\n\n    #TODO improvements: now the analysis is performed on the whole list of sentences, but can be performed:\\n\n        - per slide\\n\n        - then overall\\n\n    but i don't know if can be beneficial, depends on style of presentation. \n    For now the assumption is that there's uniform text across all the slides and titles are generally bigger than the other text\n\n    Then titles are further more filtered by choosing that text whose size is above the threshold, only if it's\n    the first sentence of the slide\\n\n\n    Prerequisites :\n    ------------\n    Must have runned analyze_video()ffmpeg -i /home/gaggio/Documents/Research/ekeel/EVA_apps/EKEELVideoAnnotation/static/videos/lskmIRldsyU/lskmIRldsyU.mp4 -o /home/gaggio/Documents/Research/ekeel/EVA_apps/EKEELVideoAnnotation/static/videos/lskmIRldsyU/lskmIRldsyU.wav\n\n    Parameters :\n    ----------\n    - quant : quantile as lower bound for outlier detection\n    - axis_for_outliers_detection : axis considered for analysis are 'width' or 'height' or both \n    - union : lastly it's perfomed a union of results (logical OR) or an intersection (logical AND)\n    - with_times : if with_times returns a list of tuples(startend_frames, text, bounding_box)\n            otherwise startend_frames are omitted\n\n    Returns :\n    ---------\n    List of all the detected titles\n    \"\"\"\n    assert axis_for_outliers_detection in {'w','h','wh'} and 0 &lt; quant &lt; 1 and self.get_extracted_text(format='list[text,time,box]') is not None\n    # convert input into columns slice for analysis\n    sliced_columns = {'w':slice(2,3),'h':slice(3,4),'wh':slice(2,4)}[axis_for_outliers_detection]\n    texts_with_bb = self.get_extracted_text(format='list[text,time,box]')\n\n    # select columns\n    axis_array = array([text_with_bb[2][sliced_columns] for text_with_bb in texts_with_bb],dtype=dtype('float','float'))\n\n    # compute statistical analysis on columns\n    indices_above_threshold = list(where((axis_array &gt; quantile(axis_array,quant,axis=0)).any(axis=1))[0]) if union else \\\n                              list(where((axis_array &gt; quantile(axis_array,quant,axis=0)).all(axis=1))[0])\n\n    slides_group = []\n    # group by slide\n    # x[0] is one element of indices_above_threshold to compare, x[1] is another\n    # [1] accesses the startend seconds of that VideoSlide  [text, startend, bounding_boxes]\n    # [0] picks the start second of that object [start_second, end_second]\n    for _,g in groupby(enumerate(indices_above_threshold),lambda x: texts_with_bb[x[0]][1][0] - texts_with_bb[x[1]][1][0]):\n        slides_group.append(list(reversed(list(map(lambda x:x[1], g)))))\n    slides_group = list(reversed(slides_group))\n\n    # remove indices of text that are classified as titles but are just big text that's not at the top of every slide\n    for group in slides_group:\n        for indx_text in group:\n            if indx_text &gt; 0 and indx_text-1 not in group and (                        # if i'm not at the first index and the text of the previous index is not in the group\n               texts_with_bb[indx_text-1][1][0] == texts_with_bb[indx_text][1][0] and  # and the text is in the same slide\n               texts_with_bb[indx_text-1][2][1] &lt; texts_with_bb[indx_text][2][1]):     # and has a y value lower than my current text (there's another sentence before this one)\n                indices_above_threshold.remove(indx_text)\n\n    # selects the texts from the list of texts\n    if len(indices_above_threshold) == 0:\n        return None\n\n    if not with_times:\n        if len(indices_above_threshold) &gt; 1:\n            return itemgetter(*indices_above_threshold)(list(zip(*list(zip(*texts_with_bb))[0:3:2])))\n        return [itemgetter(*indices_above_threshold)(list(zip(*list(zip(*texts_with_bb))[0:3:2])))]\n    if len(indices_above_threshold) &gt; 1:\n        return itemgetter(*indices_above_threshold)(texts_with_bb)\n    return [itemgetter(*indices_above_threshold)(texts_with_bb)]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.extract_video_id","title":"<code>extract_video_id(url)</code>  <code>staticmethod</code>","text":"<p>From a YouTube url extracts the video id</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>@staticmethod\ndef extract_video_id(url:str):\n    '''\n    From a YouTube url extracts the video id\n    '''\n    video_link = url.split('&amp;')[0]\n    if '=' in video_link:\n        return video_link.split('=')[-1]\n    return video_link.split('/')[-1]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.get_extracted_text","title":"<code>get_extracted_text(format='list')</code>","text":"<p>Returns the text extracted from the video.</p> <p>Text can be cleaned from non-alphanumeric characters or not.</p> Parameters : <ul> <li> <p>format (str): The desired format of the output. Defaults to 'list[text_id, timed-tuple]'.</p> <ul> <li> <p>'str': single string with the text joined together.</p> </li> <li> <p>'list': list of VideoSlides</p> </li> <li> <p>'set[times] : list of unique texts' times (in seconds) (used for creation of thumbnails)</p> </li> <li>'list[time,list[text,box]]': a list of (times, list of (sentence, bounding-box))</li> <li>'list[text,box]': list of texts with bounding boxes</li> <li>'list[id,text,box]': list of tuple of id, text, and bounding boxes</li> <li>'list[text,time,box]': list of repeated times (in seconds) for every text_bounding_boxed</li> <li>'list[tuple(id,timed-text)]': list of tuples made of (startend times in seconds, text as string present in those frames)</li> </ul> </li> </ul> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def get_extracted_text(self,format:Literal['str','list','list[text,box]','set[times]','list[text,time,box]','list[time,list[text,box]]','list[id,text,box]']='list'): \n    \"\"\"\n    Returns the text extracted from the video.\\n\n    Text can be cleaned from non-alphanumeric characters or not.\n\n    Parameters :\n    ------------\n    - format (str): The desired format of the output. Defaults to 'list[text_id, timed-tuple]'.\n        - 'str': single string with the text joined together.\\n\n        - 'list': list of VideoSlides\\n\n        - 'set[times] : list of unique texts' times (in seconds) (used for creation of thumbnails)\n        - 'list[time,list[text,box]]': a list of (times, list of (sentence, bounding-box))\n        - 'list[text,box]': list of texts with bounding boxes\n        - 'list[id,text,box]': list of tuple of id, text, and bounding boxes\n        - 'list[text,time,box]': list of repeated times (in seconds) for every text_bounding_boxed\n        - 'list[tuple(id,timed-text)]': list of tuples made of (startend times in seconds, text as string present in those frames)\n    \"\"\"\n    if self._text_in_video is None:\n        return None\n    if format=='list':\n        return self._text_in_video\n    elif format=='str':\n        return ' '.join([tft.get_full_text() for tft in self._text_in_video])\n    elif format=='set[times]':\n        out = []\n        video = LocalVideo(self.video_id)\n        for tft in self._text_in_video:\n            out.extend([(video.get_time_from_num_frame(st_en_frames[0]),video.get_time_from_num_frame(st_en_frames[1])) for st_en_frames in tft.start_end_frames])\n        return out\n    elif format=='list[text,box]':\n        out_lst = []\n        for tft in self._text_in_video:\n            out_lst.extend(tft.get_framed_sentences())\n        return out_lst\n    elif format=='list[id,text,box]':\n        timed_text_with_bb = []\n        _id = 0\n        for tft in self._text_in_video:\n            for sentence,bb in tft.get_framed_sentences():\n                timed_text_with_bb.append((_id,sentence,bb))\n                _id +=1\n        return timed_text_with_bb\n    elif format=='list[text,time,box]':\n        timed_text_with_bb = []\n        video = LocalVideo(self.video_id)\n        for tft in self._text_in_video:\n            st_en_frames = tft.start_end_frames[0]\n            for sentence,bb in tft.get_framed_sentences():\n                timed_text_with_bb.append((sentence.strip('\\n'),(video.get_time_from_num_frame(st_en_frames[0]),video.get_time_from_num_frame(st_en_frames[1])),bb))\n        return timed_text_with_bb\n    elif format=='list[time,list[text,box]]':\n        video = LocalVideo(self.video_id)\n        timed_text = []\n        texts = self._text_in_video\n        for tft in texts:\n            for startend in tft.start_end_frames:\n                insort_left(timed_text,((video.get_time_from_num_frame(startend[0]),video.get_time_from_num_frame(startend[1])), tft.get_full_text()))\n        return timed_text\n    elif format=='list[tuple(id,timed-text)]':\n        video = LocalVideo(self.video_id)\n        return [(id,(video.get_time_from_num_frame(startend[0]),video.get_time_from_num_frame(startend[1])), tft.get_full_text()) \n                        for id, tft in enumerate(self._text_in_video) \n                        for startend in tft.start_end_frames]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.identify_language","title":"<code>identify_language(format='pt1')</code>","text":"<p>Recognizes the video language (currently implemented ita and eng so it raises exception if not one of these)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def identify_language(self, format:Literal['full','pt1']='pt1') -&gt; str:\n    '''\n    Recognizes the video language (currently implemented ita and eng so it raises exception if not one of these)\n    '''\n    if not 'language' in self.data.keys():\n        self.data['language'] = list(YTTranscriptApi.list_transcripts(self.video_id)._generated_transcripts.keys())[0] \n\n    locale = Locale()\n    if not locale.is_language_supported(self.data['language']):\n        raise Exception(f\"Language is not between supported ones: {locale.get_supported_languages()}\")\n    return self.data['language'] if format =='pt1' else locale.get_full_from_pt1(self.data['language'])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.is_slide_video","title":"<code>is_slide_video(slide_frames_percent_threshold=0.5, _show_info=True)</code>","text":"<p>Computes a threshold against a value that can be calculated or passed as precomputed_value</p> <p>Returns:</p> Type Description <code>value and slide frames if return value is True</code> <code>else</code> <code>True and slide frames if percentage of recognized slidish frames is above the threshold</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def is_slide_video(self,slide_frames_percent_threshold:float=0.5,_show_info=True):\n    '''\n    Computes a threshold against a value that can be calculated or passed as precomputed_value\n\n    Returns\n    -------\n    value and slide frames if return value is True\\n\n    else\\n\n    True and slide frames if percentage of recognized slidish frames is above the threshold\n    '''\n    if not \"slides_percentage\" in self.data[\"video_data\"].keys():\n        print(self.data[\"video_data\"])\n        self._preprocess_video(vsm=VideoSpeedManager(self.video_id,COLOR_RGB),_show_info=_show_info)\n        mongo.insert_video_data(self.data)\n    return self.data[\"video_data\"]['slides_percentage'] &gt; slide_frames_percent_threshold\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.request_transcript","title":"<code>request_transcript()</code>","text":"<p>Downloads the transcript associated with the video and returns also whether the transcript is automatically or manually generated </p> <p>Preferred manually generated </p> <p>Whisper transcription is implemented in transcribe.py called as external service and will replace youtube transcript for word level precision</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>def request_transcript(self):\n    '''\n    Downloads the transcript associated with the video and returns also whether the transcript is automatically or manually generated \\n\n    Preferred manually generated \\n\n    Whisper transcription is implemented in transcribe.py called as external service and will replace youtube transcript for word level precision\\n\n    '''\n\n    if \"transcript_data\" in self.data.keys():\n        return\n\n    # moved as external service\n    #if extract_from_audio:\n    #    self.data[\"transcript_data\"] = {\"text\": WhisperTranscriber.transcribe(self.video_id,language), \n    #                                    \"is_autogenerated\": True,\n    #                                    \"is_whisper_transcribed\": True }\n    #    mongo.insert_video_data(self.data)\n    #    return\n\n\n    language = self.identify_language() \n    self.data[\"transcript_data\"] = {\"is_whisper_transcribed\": False}\n\n    transcripts = YTTranscriptApi.list_transcripts(self.video_id)\n    transcript:Transcript\n    try:\n        transcript = transcripts.find_manually_created_transcript([language])\n        self.data[\"transcript_data\"][\"is_autogenerated\"] = False\n    except:\n        transcript = transcripts.find_generated_transcript([language])\n        self.data[\"transcript_data\"][\"is_autogenerated\"] = True\n\n    subs_dict = transcript.fetch()\n    for sub in subs_dict: sub[\"end\"] = sub[\"start\"] + sub.pop(\"duration\")\n\n    timed_subtitles = []\n    for entry in subs_dict:\n        if not \"[\" in entry[\"text\"]:\n            word = {\"word\":\"\",\n                    \"start\":entry[\"start\"],\n                    \"end\": entry[\"end\"]}\n            words = []\n            for word_text in entry[\"text\"].split(\" \"):\n                apostrophed_words = word_text.split(\"'\")\n                if len(apostrophed_words) &gt; 1:\n                    word = word.copy()\n                    word[\"word\"] = apostrophed_words[0]+\"'\"\n                    words.append(word)\n                if len(apostrophed_words[-1]):\n                    word = word.copy()\n                    word[\"word\"] = apostrophed_words[-1]\n                    words.append(word)\n            segment = {'text': entry[\"text\"], \n                       'start': entry['start'],\n                       'end': entry['end'],\n                       'words': words }\n            timed_subtitles.append(segment)\n    self.data[\"transcript_data\"][\"text\"] = timed_subtitles \n\n    mongo.insert_video_data(self.data)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/segmentation/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.segmentation.VideoAnalyzer.transcript_segmentation","title":"<code>transcript_segmentation(c_threshold=0.22, sec_min=35, S=1, frame_range=15, create_thumbnails=True)</code>","text":"<p>:param c_threshold: threshold per la similarit\u00e0 tra frasi :param sec_min: se un segmento \u00e8 minore di sec_min verr\u00e0 unito con il successivo :param S: scala per color histogram :param frame_range: aggiustare inizio e fine dei segmenti in base a differenza nel color histogram nel frame_range :return: segments</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/segmentation.py</code> <pre><code>    def transcript_segmentation(self, c_threshold=0.22, sec_min=35, S=1, frame_range=15, create_thumbnails=True):\n        \"\"\"\n        :param c_threshold: threshold per la similarit\u00e0 tra frasi\n        :param sec_min: se un segmento \u00e8 minore di sec_min verr\u00e0 unito con il successivo\n        :param S: scala per color histogram\n        :param frame_range: aggiustare inizio e fine dei segmenti in base a differenza nel color histogram nel frame_range\n        :return: segments\n        \"\"\"\n        if \"video_data\" in self.data.keys() and \"segments\" in self.data[\"video_data\"].keys():\n            return \n        video_id = self.video_id\n        language = self.identify_language()\n\n        # get punctuated transcription from the conll in the db\n        #transcription:str = get_text(video_id)\n        #if transcription is None:\n        #    if self.timed_subtitles is None:\n        #        self.request_transcript()\n#\n        #    '''Get the transcription from the subtitles'''\n        #    transcription:str = \" \".join([sub[\"text\"] for sub in self.timed_subtitles[\"text\"]])\n        #    if language == Locale().get_pt1_from_full('English'):\n        #        transcription:str = transcription.replace('\\n', ' ').replace(\"&gt;&gt;\", \"\") \\\n        #                                         .replace(\"Dr.\",\"Dr\").replace(\"dr.\",\"dr\") \\\n        #                                         .replace(\"Mr.\",\"Mr\").replace(\"mr.\",\"mr\")\n        #print(\"Checking punctuation...\")\n        semantic_transcript = SemanticText(\" \".join(timed_sentence[\"text\"] for timed_sentence in self.data[\"transcript_data\"][\"text\"] if not \"[\" in timed_sentence['text']),language)\n\n        #video = mongo.get_video(video_id)\n\n        '''Divide into sentences the punctuated transcription'''\n        print(\"Extracting sentences..\")\n        sentences = [sent.replace(\" ,\",\",\").replace(\" .\",\".\") for sent in semantic_transcript.tokenize()]\n\n        '''For each sentence, add its start and end time obtained from the subtitles'''\n        timed_sentences = get_timed_sentences(self.data[\"transcript_data\"][\"text\"], sentences)\n\n        '''Define the BERT model for similarity'''\n        print(\"Creating embeddings..\")\n        #model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n        #model = SentenceTransformer('stsb-roberta-large')\n\n        '''Compute a vector of numbers (the embedding) to idenfity each sentence'''\n        #embeddings = model.encode(sentences, convert_to_tensor=True)\n        # All moved inside semantic transcript class\n        embeddings = semantic_transcript.get_embeddings()\n\n        '''Create clusters based on semantic textual similarity, using the BERT embeddings'''\n        print(\"Creating initials segments..\")\n        clusters = create_cluster_list(timed_sentences, embeddings, c_threshold)\n\n\n        '''Aggregate togheter clusters shorter than 40 seconds in total'''\n        refined_clusters = aggregate_short_clusters(clusters, sec_min)\n\n        start_times = []\n        end_times = []\n\n        '''Print the final result'''\n        for c in refined_clusters:\n            start_times.append(c.start_time)\n            end_times.append(c.end_time)\n        self.data[\"video_data\"] = {\"segments\": list(zip(start_times, end_times))}\n        mongo.insert_video_data(self.data)\n\n\n        print(\"Reached the part of finding clusters\")\n\n        '''Find and append to each cluster the 2 most relevant sentences'''\n        #num_sentences = 2\n        #sumy_summary(refined_clusters, num_sentences)\n        #self.transcript = semantic_transcript\n\n\n        '''Adjust end and start time of each cluster based on detected scene changes'''\n        #path = Path(__file__).parent.joinpath(\"static\", \"videos\", video_id)\n        #if not create_thumbnails:\n        #    self.data[\"video_data\"]['segments'] = self._create_keyframes(start_times, end_times, S, frame_range, create_thumbnails=False)\n        #    return\n#\n        #if not any(File.endswith(\".jpg\") for File in os.listdir(path)):\n        #    self.data[\"video_data\"]['segments'] = self._create_keyframes(start_times, end_times, S, frame_range)\n        #else:\n        #    print(\"keyframes already present\")\n        #    images = []\n        #    for File in os.listdir(path):\n        #        if File.endswith(\".jpg\"):\n        #            images.append(File.split(\".\")[0])\n        #    images.sort(key=int)\n        #    self.images_path = [\"videos/\" + video_id + \"/\" + im + \".jpg\" for im in images]\n        return\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/","title":"video","text":""},{"location":"codebase/EKEELVideoAnnotation/media/video/#video","title":"Video","text":"<p>Video processing module.</p> <p>This module provides classes for handling video files, managing playback speed, and extracting frames for analysis.</p> <p>Classes:</p> Name Description <code>LocalVideo</code> <p>Basic video file handler with frame extraction capabilities</p> <code>SimpleVideo</code> <p>Basic video player with simple frame controls</p> <code>VideoSpeedManager</code> <p>Advanced video processor with adaptive frame skipping</p>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo","title":"<code>LocalVideo</code>","text":"<p>Local video file handler with frame extraction capabilities.</p> <p>Attributes:</p> Name Type Description <code>_vidcap</code> <code>VideoCapture</code> <p>OpenCV video capture object</p> <code>_output_colors</code> <code>int</code> <p>Color scheme for output frames</p> <code>_num_colors</code> <code>int</code> <p>Number of color channels</p> <code>_frame_size</code> <code>tuple</code> <p>Forced frame size (width, height)</p> <code>_vid_id</code> <code>str</code> <p>Video identifier</p> <p>Methods:</p> Name Description <code>extract_next_frame</code> <p>Extract next frame from video</p> <code>get_count_frames</code> <p>Get total number of frames</p> <code>get_dim_frame</code> <p>Get frame dimensions</p> <code>get_fps</code> <p>Get frames per second</p> <code>get_id_vid</code> <p>Get video identifier</p> <code>get_time_from_num_frame</code> <p>Convert frame number to time in seconds</p> <code>get_num_frame_from_time</code> <p>Convert time to frame number</p> <code>set_num_frame</code> <p>Set current frame number</p> <code>set_frame_size</code> <p>Set frame dimensions</p> <code>close</code> <p>Release video capture resources</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>class LocalVideo:\n    \"\"\"\n    Local video file handler with frame extraction capabilities.\n\n    Attributes\n    ----------\n    _vidcap : cv2.VideoCapture\n        OpenCV video capture object\n    _output_colors : int\n        Color scheme for output frames\n    _num_colors : int\n        Number of color channels\n    _frame_size : tuple\n        Forced frame size (width, height)\n    _vid_id : str\n        Video identifier\n\n    Methods\n    -------\n    extract_next_frame()\n        Extract next frame from video\n    get_count_frames()\n        Get total number of frames\n    get_dim_frame()\n        Get frame dimensions\n    get_fps()\n        Get frames per second\n    get_id_vid()\n        Get video identifier\n    get_time_from_num_frame(num_frame, decimals)\n        Convert frame number to time in seconds\n    get_num_frame_from_time(seconds)\n        Convert time to frame number\n    set_num_frame(num_frame)\n        Set current frame number\n    set_frame_size(value)\n        Set frame dimensions\n    close()\n        Release video capture resources\n    \"\"\"\n    def __init__(self,video_id:str,output_colors:int=COLOR_BGR,forced_frame_size:'tuple[int,int] | None'= None,_testing_path=None):\n        \"\"\"\n        Initialize video file handler.\n\n        Parameters\n        ----------\n        video_id : str\n            Identifier for the video file\n        output_colors : int, optional\n            Color scheme for output frames\n        forced_frame_size : tuple or None, optional\n            Force specific frame dimensions (width, height)\n        _testing_path : str, optional\n            Override path for testing\n        \"\"\"\n        if output_colors!=COLOR_BGR and output_colors!=COLOR_RGB and output_colors!=COLOR_GRAY:\n            raise Exception(f\"Wrong parameter ouput_color value must be a COLOR_ value present in image.py\")\n        else:\n            self._output_colors = output_colors\n            if output_colors == COLOR_GRAY:\n                self._num_colors = 1\n            else:\n                self._num_colors = 3\n        self._frame_size = forced_frame_size\n        self._vid_id = video_id\n        if _testing_path is None:\n            video_file_folder = VIDEOS_PATH.joinpath(video_id)\n        else:\n            video_file_folder = _testing_path\n        self._vidcap = cv2.VideoCapture(os.path.join(video_file_folder,video_id+'.mp4'))\n        #self._vidcap = cv2.VideoCapture(os.path.join(class_path, \"static\", \"videos\", video_id,f\"{video_id}.mkv\"))\n        if not self._vidcap.isOpened():\n            raise Exception(f\"Can't find video: {video_id}\")\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._vidcap.relase()\n\n    def close(self):\n        \"\"\"\n        Release video capture resources.\n        \"\"\"\n        self._vidcap.release()\n\n    def extract_next_frame(self):\n        \"\"\"\n        Extract next frame from video.\n\n        Returns\n        -------\n        ndarray or None\n            Image array if frame exists, None otherwise\n        \"\"\"\n        has_frame, image = self._vidcap.read()\n        if not has_frame:\n            return None\n        if self._frame_size is not None:\n            image = cv2.resize(image,self._frame_size,interpolation=cv2.INTER_AREA)\n        if self._output_colors != COLOR_BGR:\n            image = cv2.cvtColor(image, self._output_colors)\n        return reshape(image,(image.shape[0],image.shape[1],self._num_colors))\n\n    def get_count_frames(self) -&gt; int:\n        \"\"\"\n        Get total number of frames in video.\n\n        Returns\n        -------\n        int\n            Total frame count\n        \"\"\"\n        return int(self._vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    def get_dim_frame(self):\n        \"\"\"\n        Get frame dimensions.\n\n        Returns\n        -------\n        tuple\n            (width, height, num_colors)\n        \"\"\"\n        if self._frame_size is not None:\n            return (*self._frame_size,self._num_colors)\n\n        return  int(self._vidcap.get(cv2.CAP_PROP_FRAME_WIDTH)), \\\n                int(self._vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT)), \\\n                self._num_colors\n\n    def get_fps(self) -&gt; int:\n        \"\"\"\n        Get video frames per second.\n\n        Returns\n        -------\n        int\n            Frames per second\n        \"\"\"\n        return int(self._vidcap.get(cv2.CAP_PROP_FPS))\n\n    def get_id_vid(self) -&gt; str:\n        \"\"\"\n        Get video identifier.\n\n        Returns\n        -------\n        str\n            Video ID\n        \"\"\"\n        return self._vid_id\n\n    def get_time_from_num_frame(self,num_frame:int,decimals:int=1):       #fr / (fr / s) = s\n        \"\"\"\n        Convert frame number to time in seconds.\n\n        Parameters\n        ----------\n        num_frame : int\n            Frame number\n        decimals : int, optional\n            Number of decimal places\n\n        Returns\n        -------\n        float\n            Time in seconds\n        \"\"\"\n        return round(num_frame/self.get_fps(), decimals=decimals)\n\n    def get_num_frame_from_time(self,seconds:float):\n        \"\"\"\n        Convert time to frame number.\n\n        Parameters\n        ----------\n        seconds : float\n            Time in seconds\n\n        Returns\n        -------\n        int\n            Frame number\n        \"\"\"\n        return int(seconds*self.get_fps())\n\n    def set_num_frame(self,num_frame:int):\n        \"\"\"\n        Set current frame number.\n\n        Parameters\n        ----------\n        num_frame : int\n            Frame number to set\n        \"\"\"\n        self._vidcap.set(cv2.CAP_PROP_POS_FRAMES,num_frame)\n\n    def set_frame_size(self,value:'tuple[int,int]'):\n        \"\"\"\n        Set frame dimensions.\n\n        Parameters\n        ----------\n        value : tuple\n            New frame dimensions (width, height)\n        \"\"\"\n        self._frame_size = value\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.__init__","title":"<code>__init__(video_id, output_colors=COLOR_BGR, forced_frame_size=None, _testing_path=None)</code>","text":"<p>Initialize video file handler.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video file</p> required <code>output_colors</code> <code>int</code> <p>Color scheme for output frames</p> <code>COLOR_BGR</code> <code>forced_frame_size</code> <code>tuple or None</code> <p>Force specific frame dimensions (width, height)</p> <code>None</code> <code>_testing_path</code> <code>str</code> <p>Override path for testing</p> <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def __init__(self,video_id:str,output_colors:int=COLOR_BGR,forced_frame_size:'tuple[int,int] | None'= None,_testing_path=None):\n    \"\"\"\n    Initialize video file handler.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video file\n    output_colors : int, optional\n        Color scheme for output frames\n    forced_frame_size : tuple or None, optional\n        Force specific frame dimensions (width, height)\n    _testing_path : str, optional\n        Override path for testing\n    \"\"\"\n    if output_colors!=COLOR_BGR and output_colors!=COLOR_RGB and output_colors!=COLOR_GRAY:\n        raise Exception(f\"Wrong parameter ouput_color value must be a COLOR_ value present in image.py\")\n    else:\n        self._output_colors = output_colors\n        if output_colors == COLOR_GRAY:\n            self._num_colors = 1\n        else:\n            self._num_colors = 3\n    self._frame_size = forced_frame_size\n    self._vid_id = video_id\n    if _testing_path is None:\n        video_file_folder = VIDEOS_PATH.joinpath(video_id)\n    else:\n        video_file_folder = _testing_path\n    self._vidcap = cv2.VideoCapture(os.path.join(video_file_folder,video_id+'.mp4'))\n    #self._vidcap = cv2.VideoCapture(os.path.join(class_path, \"static\", \"videos\", video_id,f\"{video_id}.mkv\"))\n    if not self._vidcap.isOpened():\n        raise Exception(f\"Can't find video: {video_id}\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.close","title":"<code>close()</code>","text":"<p>Release video capture resources.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def close(self):\n    \"\"\"\n    Release video capture resources.\n    \"\"\"\n    self._vidcap.release()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.extract_next_frame","title":"<code>extract_next_frame()</code>","text":"<p>Extract next frame from video.</p> <p>Returns:</p> Type Description <code>ndarray or None</code> <p>Image array if frame exists, None otherwise</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def extract_next_frame(self):\n    \"\"\"\n    Extract next frame from video.\n\n    Returns\n    -------\n    ndarray or None\n        Image array if frame exists, None otherwise\n    \"\"\"\n    has_frame, image = self._vidcap.read()\n    if not has_frame:\n        return None\n    if self._frame_size is not None:\n        image = cv2.resize(image,self._frame_size,interpolation=cv2.INTER_AREA)\n    if self._output_colors != COLOR_BGR:\n        image = cv2.cvtColor(image, self._output_colors)\n    return reshape(image,(image.shape[0],image.shape[1],self._num_colors))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.get_count_frames","title":"<code>get_count_frames()</code>","text":"<p>Get total number of frames in video.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total frame count</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_count_frames(self) -&gt; int:\n    \"\"\"\n    Get total number of frames in video.\n\n    Returns\n    -------\n    int\n        Total frame count\n    \"\"\"\n    return int(self._vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.get_dim_frame","title":"<code>get_dim_frame()</code>","text":"<p>Get frame dimensions.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>(width, height, num_colors)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_dim_frame(self):\n    \"\"\"\n    Get frame dimensions.\n\n    Returns\n    -------\n    tuple\n        (width, height, num_colors)\n    \"\"\"\n    if self._frame_size is not None:\n        return (*self._frame_size,self._num_colors)\n\n    return  int(self._vidcap.get(cv2.CAP_PROP_FRAME_WIDTH)), \\\n            int(self._vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT)), \\\n            self._num_colors\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.get_fps","title":"<code>get_fps()</code>","text":"<p>Get video frames per second.</p> <p>Returns:</p> Type Description <code>int</code> <p>Frames per second</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_fps(self) -&gt; int:\n    \"\"\"\n    Get video frames per second.\n\n    Returns\n    -------\n    int\n        Frames per second\n    \"\"\"\n    return int(self._vidcap.get(cv2.CAP_PROP_FPS))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.get_id_vid","title":"<code>get_id_vid()</code>","text":"<p>Get video identifier.</p> <p>Returns:</p> Type Description <code>str</code> <p>Video ID</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_id_vid(self) -&gt; str:\n    \"\"\"\n    Get video identifier.\n\n    Returns\n    -------\n    str\n        Video ID\n    \"\"\"\n    return self._vid_id\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.get_num_frame_from_time","title":"<code>get_num_frame_from_time(seconds)</code>","text":"<p>Convert time to frame number.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>Time in seconds</p> required <p>Returns:</p> Type Description <code>int</code> <p>Frame number</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_num_frame_from_time(self,seconds:float):\n    \"\"\"\n    Convert time to frame number.\n\n    Parameters\n    ----------\n    seconds : float\n        Time in seconds\n\n    Returns\n    -------\n    int\n        Frame number\n    \"\"\"\n    return int(seconds*self.get_fps())\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.get_time_from_num_frame","title":"<code>get_time_from_num_frame(num_frame, decimals=1)</code>","text":"<p>Convert frame number to time in seconds.</p> <p>Parameters:</p> Name Type Description Default <code>num_frame</code> <code>int</code> <p>Frame number</p> required <code>decimals</code> <code>int</code> <p>Number of decimal places</p> <code>1</code> <p>Returns:</p> Type Description <code>float</code> <p>Time in seconds</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_time_from_num_frame(self,num_frame:int,decimals:int=1):       #fr / (fr / s) = s\n    \"\"\"\n    Convert frame number to time in seconds.\n\n    Parameters\n    ----------\n    num_frame : int\n        Frame number\n    decimals : int, optional\n        Number of decimal places\n\n    Returns\n    -------\n    float\n        Time in seconds\n    \"\"\"\n    return round(num_frame/self.get_fps(), decimals=decimals)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.set_frame_size","title":"<code>set_frame_size(value)</code>","text":"<p>Set frame dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>tuple</code> <p>New frame dimensions (width, height)</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def set_frame_size(self,value:'tuple[int,int]'):\n    \"\"\"\n    Set frame dimensions.\n\n    Parameters\n    ----------\n    value : tuple\n        New frame dimensions (width, height)\n    \"\"\"\n    self._frame_size = value\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.LocalVideo.set_num_frame","title":"<code>set_num_frame(num_frame)</code>","text":"<p>Set current frame number.</p> <p>Parameters:</p> Name Type Description Default <code>num_frame</code> <code>int</code> <p>Frame number to set</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def set_num_frame(self,num_frame:int):\n    \"\"\"\n    Set current frame number.\n\n    Parameters\n    ----------\n    num_frame : int\n        Frame number to set\n    \"\"\"\n    self._vidcap.set(cv2.CAP_PROP_POS_FRAMES,num_frame)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo","title":"<code>SimpleVideo</code>","text":"<p>Basic video player with simple frame controls.</p> <p>Attributes:</p> Name Type Description <code>video</code> <code>VideoCapture</code> <p>OpenCV video capture object</p> <code>_curr_step</code> <code>int</code> <p>Current frame step size</p> <code>_curr_frame_idx</code> <code>int</code> <p>Current frame index</p> <p>Methods:</p> Name Description <code>close</code> <p>Release video resources</p> <code>get_count_frames</code> <p>Get total number of frames</p> <code>get_fps</code> <p>Get frames per second</p> <code>get_frame</code> <p>Get next frame based on step size</p> <code>roll</code> <p>Move frame position by offset</p> <code>rewind</code> <p>Return to start of video</p> <code>set_step</code> <p>Set frame step size</p> <code>get_frame_index</code> <p>Get current frame index</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>class SimpleVideo:\n    \"\"\"\n    Basic video player with simple frame controls.\n\n    Attributes\n    ----------\n    video : cv2.VideoCapture\n        OpenCV video capture object\n    _curr_step : int\n        Current frame step size\n    _curr_frame_idx : int\n        Current frame index\n\n    Methods\n    -------\n    close()\n        Release video resources\n    get_count_frames()\n        Get total number of frames\n    get_fps()\n        Get frames per second\n    get_frame()\n        Get next frame based on step size\n    roll(offset)\n        Move frame position by offset\n    rewind()\n        Return to start of video\n    set_step(step)\n        Set frame step size\n    get_frame_index(one_step_back)\n        Get current frame index\n    \"\"\"\n    def __init__(self, video_id: str):\n        \"\"\"\n        Initialize simple video player.\n\n        Parameters\n        ----------\n        video_id : str\n            Identifier for the video file\n        \"\"\"\n        self.video = cv2.VideoCapture(Path(__file__).parent.joinpath(\"static\",\"videos\",video_id,video_id+\".mp4\").__str__())\n        if not self.video.isOpened():\n            raise Exception(\"Error loading video in SimpleVideo\")\n        self._curr_step = 1\n        self._curr_frame_idx = 0\n\n    def close(self):\n        \"\"\"\n        Release video capture resources.\n        \"\"\"\n        self.video.release()\n\n    def get_count_frames(self) -&gt; int:\n        \"\"\"\n        Get total number of frames in video.\n\n        Returns\n        -------\n        int\n            Total frame count\n        \"\"\"\n        return int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    def get_fps(self) -&gt; int:\n        \"\"\"\n        Get video frames per second.\n\n        Returns\n        -------\n        int\n            Frames per second\n        \"\"\"\n        return int(self.video.get(cv2.CAP_PROP_FPS))\n\n    def get_frame(self):\n        \"\"\"\n        Get next frame based on current step size.\n\n        Returns\n        -------\n        ndarray\n            Next video frame\n        \"\"\"\n        self._curr_frame_idx += self._curr_step\n        self.video.set(cv2.CAP_PROP_POS_FRAMES,self._curr_frame_idx)\n        return self.video.read()[1]\n\n    def roll(self, offset: int):\n        \"\"\"\n        Move frame position by given offset.\n\n        Parameters\n        ----------\n        offset : int\n            Number of frames to move (positive or negative)\n        \"\"\"\n        self._curr_frame_idx += offset\n        self.video.set(cv2.CAP_PROP_POS_FRAMES,self._curr_frame_idx)\n\n    def rewind(self):\n        \"\"\"\n        Return to first frame of video.\n        \"\"\"\n        self.roll( - self._curr_frame_idx )\n\n    def set_step(self, step):\n        \"\"\"\n        Set frame step size.\n\n        Parameters\n        ----------\n        step : int\n            Number of frames to skip between each get_frame call\n        \"\"\"\n        self._curr_step = step\n\n    def get_frame_index(self, one_step_back: bool = False):\n        \"\"\"\n        Get current frame index.\n\n        Parameters\n        ----------\n        one_step_back : bool, optional\n            If True, returns index minus one step\n\n        Returns\n        -------\n        int\n            Current frame index\n        \"\"\"\n        return self._curr_frame_idx - one_step_back * self._curr_step\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.__init__","title":"<code>__init__(video_id)</code>","text":"<p>Initialize simple video player.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video file</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def __init__(self, video_id: str):\n    \"\"\"\n    Initialize simple video player.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video file\n    \"\"\"\n    self.video = cv2.VideoCapture(Path(__file__).parent.joinpath(\"static\",\"videos\",video_id,video_id+\".mp4\").__str__())\n    if not self.video.isOpened():\n        raise Exception(\"Error loading video in SimpleVideo\")\n    self._curr_step = 1\n    self._curr_frame_idx = 0\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.close","title":"<code>close()</code>","text":"<p>Release video capture resources.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def close(self):\n    \"\"\"\n    Release video capture resources.\n    \"\"\"\n    self.video.release()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.get_count_frames","title":"<code>get_count_frames()</code>","text":"<p>Get total number of frames in video.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total frame count</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_count_frames(self) -&gt; int:\n    \"\"\"\n    Get total number of frames in video.\n\n    Returns\n    -------\n    int\n        Total frame count\n    \"\"\"\n    return int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.get_fps","title":"<code>get_fps()</code>","text":"<p>Get video frames per second.</p> <p>Returns:</p> Type Description <code>int</code> <p>Frames per second</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_fps(self) -&gt; int:\n    \"\"\"\n    Get video frames per second.\n\n    Returns\n    -------\n    int\n        Frames per second\n    \"\"\"\n    return int(self.video.get(cv2.CAP_PROP_FPS))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.get_frame","title":"<code>get_frame()</code>","text":"<p>Get next frame based on current step size.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Next video frame</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_frame(self):\n    \"\"\"\n    Get next frame based on current step size.\n\n    Returns\n    -------\n    ndarray\n        Next video frame\n    \"\"\"\n    self._curr_frame_idx += self._curr_step\n    self.video.set(cv2.CAP_PROP_POS_FRAMES,self._curr_frame_idx)\n    return self.video.read()[1]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.get_frame_index","title":"<code>get_frame_index(one_step_back=False)</code>","text":"<p>Get current frame index.</p> <p>Parameters:</p> Name Type Description Default <code>one_step_back</code> <code>bool</code> <p>If True, returns index minus one step</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Current frame index</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_frame_index(self, one_step_back: bool = False):\n    \"\"\"\n    Get current frame index.\n\n    Parameters\n    ----------\n    one_step_back : bool, optional\n        If True, returns index minus one step\n\n    Returns\n    -------\n    int\n        Current frame index\n    \"\"\"\n    return self._curr_frame_idx - one_step_back * self._curr_step\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.rewind","title":"<code>rewind()</code>","text":"<p>Return to first frame of video.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def rewind(self):\n    \"\"\"\n    Return to first frame of video.\n    \"\"\"\n    self.roll( - self._curr_frame_idx )\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.roll","title":"<code>roll(offset)</code>","text":"<p>Move frame position by given offset.</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Number of frames to move (positive or negative)</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def roll(self, offset: int):\n    \"\"\"\n    Move frame position by given offset.\n\n    Parameters\n    ----------\n    offset : int\n        Number of frames to move (positive or negative)\n    \"\"\"\n    self._curr_frame_idx += offset\n    self.video.set(cv2.CAP_PROP_POS_FRAMES,self._curr_frame_idx)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.SimpleVideo.set_step","title":"<code>set_step(step)</code>","text":"<p>Set frame step size.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>Number of frames to skip between each get_frame call</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def set_step(self, step):\n    \"\"\"\n    Set frame step size.\n\n    Parameters\n    ----------\n    step : int\n        Number of frames to skip between each get_frame call\n    \"\"\"\n    self._curr_step = step\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager","title":"<code>VideoSpeedManager</code>","text":"<p>Advanced video processor with adaptive frame skipping.</p> <p>This class implements an adaptation of TCP fast retransmit algorithm for smart frame sampling.</p> <p>The principle is the following:  </p> <p>With exponential growth of the frame skip rate, the algorithm tries to skip many identical frames until a threshold where the growth becomes linear.</p> <p>When a collision is detected (the text is changed with respect to the cached one), it rolls back to the first frame of that text occurrence and sets a linear growth of the frame skip rate. If no text is detected, the speed of skip will be exponential again.</p> <p>Attributes:</p> Name Type Description <code>vid_ref</code> <code>LocalVideo</code> <p>Reference to video file handler</p> <code>_color_scheme</code> <code>int</code> <p>Color scheme for output frames</p> <code>_curr_num_frame</code> <code>int</code> <p>Current frame number</p> <code>_is_video_ended</code> <code>bool</code> <p>Flag indicating video end</p> <code>_is_forced_speed</code> <code>bool</code> <p>Flag for forced speed mode</p> <p>Methods:</p> Name Description <code>get_frame</code> <p>Get next frame based on current speed</p> <code>collide_and_get_fixed_num_frame</code> <p>Handle text detection collision</p> <code>lock_speed</code> <p>Lock frame skip rate</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>class VideoSpeedManager:\n    \"\"\"\n    Advanced video processor with adaptive frame skipping.\n\n    This class implements an adaptation of TCP fast retransmit algorithm\n    for smart frame sampling.\\n\n    The principle is the following: ![TCP](https://encyclopedia.pub/media/common/202107/blobid20-60f5467ae42de.jpeg) \\n\n    With exponential growth of the frame skip rate, the algorithm tries to skip many identical frames until a threshold where the growth becomes linear.\\n\n    When a collision is detected (the text is changed with respect to the cached one), it rolls back to the first frame of that text occurrence and sets a linear growth of the frame skip rate.\n    If no text is detected, the speed of skip will be exponential again.\\n\n\n    Attributes\n    ----------\n    vid_ref : LocalVideo\n        Reference to video file handler\n    _color_scheme : int\n        Color scheme for output frames\n    _curr_num_frame : int\n        Current frame number\n    _is_video_ended : bool\n        Flag indicating video end\n    _is_forced_speed : bool\n        Flag for forced speed mode\n\n    Methods\n    -------\n    get_frame()\n        Get next frame based on current speed\n    collide_and_get_fixed_num_frame()\n        Handle text detection collision\n    lock_speed(num_frames_skipped)\n        Lock frame skip rate\n    \"\"\"\n    def __init__(self,\n                 video_id:str, \n                 output_colors:int=COLOR_BGR, \n                 max_dim_frame:Tuple[int,int]=(640,360),\n                 time_decimals_accuracy:int=1,\n                 exp_base:float=1.4,\n                 lin_factor:float=2,\n                 max_seconds_exp_window:float=5,\n                 ratio_lin_exp_window_size:float=1.5,\n                 _testing_path:str=None):\n        self._init_params = (video_id,output_colors,max_dim_frame,time_decimals_accuracy,exp_base,lin_factor,max_seconds_exp_window,ratio_lin_exp_window_size)\n        vid_ref = LocalVideo(video_id=video_id,output_colors=output_colors,_testing_path=_testing_path)\n        frame_dim = vid_ref.get_dim_frame()[:2]\n        max_scale_factor = max(divmod(frame_dim,max_dim_frame)[0])\n        if max_scale_factor &gt; 1: vid_ref.set_frame_size(tuple((array(frame_dim)/max_scale_factor).astype(int)))\n\n        max_size_exp_window_frames = int(vid_ref.get_fps()*max_seconds_exp_window)\n        max_size_lin_window_frames = int(max_size_exp_window_frames*ratio_lin_exp_window_size)\n        start_sample_rate = ceil(clip(vid_ref.get_fps()*(10**(-time_decimals_accuracy)),1,vid_ref.get_fps()))\n\n        self.vid_ref = vid_ref\n        self._color_scheme = output_colors\n        self._curr_num_frame = -start_sample_rate\n        self._curr_x = 0\n        self._frames = None\n        self._curr_start_end_frames = None\n        self._min_window_frame_size = start_sample_rate\n        self._max_size_exp_window_frames = max_size_exp_window_frames\n        self._max_size_lin_window_frames = max_size_lin_window_frames\n        self._exp_base = exp_base\n        self._y0_exp = start_sample_rate\n        self._m = lin_factor\n        self._y0_lin = max_size_exp_window_frames-lin_factor*(log2(max_size_lin_window_frames)/log2(exp_base))\n        self._is_cong_avoid = False\n        self._is_collided = False\n        self._is_video_ended = False\n        self._is_forced_speed = False\n\n    def _exp_step(self,value:int): # (base^x - 1) + y0\n        \"\"\"\n        Calculate exponential step size.\n\n        Parameters\n        ----------\n        value : int\n            Current step value\n\n        Returns\n        -------\n        float\n            Exponential step size\n        \"\"\"\n        return (self._exp_base**value-1) + self._y0_exp\n\n    def _lin_step(self,value:int):\n        \"\"\"\n        Calculate linear step size.\n\n        Parameters\n        ----------\n        value : int\n            Current step value\n\n        Returns\n        -------\n        float\n            Linear step size\n        \"\"\"\n        return self._m*value + self._y0_lin\n\n    def is_video_ended(self):\n        return self._is_video_ended\n\n    def close(self):\n        self.vid_ref.close()\n\n    def get_video(self):\n        \"\"\"\n        Get reference to video handler.\n\n        Returns\n        -------\n        LocalVideo\n            Reference to video handler object\n        \"\"\"\n        return self.vid_ref\n\n    def get_curr_num_frame(self):\n        return self._curr_num_frame\n\n    def get_prev_num_frame(self):\n        return self._curr_num_frame - self._curr_window_frame_size\n\n    def get_frame_from_num(self,num_frame:int):\n        \"\"\"\n        Get specific frame by number.\n\n        Parameters\n        ----------\n        num_frame : int\n            Frame number to retrieve\n\n        Returns\n        -------\n        ndarray\n            Requested video frame\n        \"\"\"\n        prev_num_frame = self._curr_num_frame\n        self.vid_ref.set_num_frame(num_frame)\n        frame = self.vid_ref.extract_next_frame()\n        self.vid_ref.set_num_frame(prev_num_frame)\n        return frame\n\n    def get_percentage_progression(self) -&gt; int:\n        \"\"\"\n        Calculate video processing progress.\n\n        Returns\n        -------\n        int\n            Percentage of video processed (0-100)\n        \"\"\"\n        if self._frames is not None:\n            if len(self._frames) &gt; 0:\n                return ceil(self._curr_num_frame/self._frames[0][1] * 100)\n            else:\n                assert self._curr_start_end_frames is not None\n                return ceil(self._curr_num_frame/self._curr_start_end_frames[1] * 100)\n        return ceil(self._curr_num_frame/self.vid_ref.get_count_frames()*100)\n\n    def is_full_video(self,frames):\n        return len(frames)==1 and frames[0][0]==0 and frames[0][1]==self.vid_ref.get_count_frames()-1\n\n    def _debug_get_speed(self):\n        if self._curr_window_frame_size is not None:\n            return self._curr_window_frame_size\n        return 0\n\n    def _get_num_last_frame(self,vid_ref: LocalVideo):\n        curr_start_end_frames = self._curr_start_end_frames\n        if curr_start_end_frames is not None:\n            return curr_start_end_frames[1] - 1\n        else:\n            return vid_ref.get_count_frames() - 1\n\n    def _get_frame_from_internal_frames(self):\n        curr_num_frame = self._curr_num_frame\n        curr_start_end_frames = self._curr_start_end_frames\n        if curr_start_end_frames is None or curr_num_frame + self._curr_window_frame_size &gt;= curr_start_end_frames[1]:\n            try:\n                assert self._frames is not None\n                self._curr_start_end_frames = self._frames.pop()\n                return self._curr_start_end_frames[0]\n            except IndexError:\n                self._frames = []\n                return self.vid_ref.get_count_frames()\n\n        return self._curr_num_frame\n\n    def get_frame(self):\n        \"\"\"\n        Get next frame based on current speed settings.\n\n        The frame skip rate adapts based on current phase:\n        - Collision: minimum skip rate\n        - Exponential: exponentially increasing skip\n        - Linear: linearly increasing skip\n        - Forced: fixed skip rate\n\n        Returns\n        -------\n        ndarray\n            Next video frame\n        \"\"\"\n        if self._is_forced_speed:\n            next_size_window_frame = self._curr_window_frame_size \n        else:\n            if self._is_collided:\n                next_size_window_frame = self._min_window_frame_size\n            else:\n                if self._is_cong_avoid:\n                    # linear step\n                    next_size_window_frame = clip(  self._lin_step(self._curr_x),\n                                                    self._min_window_frame_size,\n                                                    self._max_size_lin_window_frames )\n                else:\n                    # exponential step\n                    next_size_window_frame = clip(  self._exp_step(self._curr_x),\n                                                    self._min_window_frame_size,\n                                                    self._max_size_exp_window_frames )\n                    if next_size_window_frame == self._max_size_exp_window_frames:\n                        self._is_cong_avoid = True\n                self._curr_x += 1\n\n        self._curr_window_frame_size = int(next_size_window_frame)\n\n        if self._frames is not None:\n            self._curr_num_frame = self._get_frame_from_internal_frames()\n\n        vid_ref = self.vid_ref\n        self._curr_num_frame += self._curr_window_frame_size \n        vid_ref.set_num_frame(self._curr_num_frame)\n        frame = vid_ref.extract_next_frame()\n        if frame is None:\n            self._is_video_ended = True\n            num_last_frame = self._get_num_last_frame(vid_ref)\n            vid_ref.set_num_frame(num_last_frame)\n            frame = vid_ref.extract_next_frame()\n            self._curr_num_frame = num_last_frame + 1 \n        return frame\n\n    def _bin_search(self, min_offset:int, max_offset:int, step_size:int):\n        L = min_offset\n        R = max_offset\n        vid_ref = self.vid_ref\n        frame = ImageClassifier().set_color_scheme(vid_ref._output_colors)\n        while L &lt;= R:\n            m = floor((L+R)/2)\n            vid_ref.set_num_frame(m)\n            if frame.set_img(vid_ref.extract_next_frame()).extract_text():\n                R = m - 1\n            else:\n                L = m + 1\n        else:\n            m = ceil((L+R)/2)\n        return m + m%step_size # align to the step_size\n\n    def collide_and_get_fixed_num_frame(self):\n        \"\"\"\n        Handle text detection collision by finding first frame with text.\n\n        Uses binary search to find the first frame where text appears\n        between current position and last known position.\n\n        Returns\n        -------\n        int\n            Frame number where text first appears\n        \"\"\"\n        curr_num_frame = self._curr_num_frame\n        max_rollback_frame = int(clip(curr_num_frame - self._curr_window_frame_size, 0, curr_num_frame))\n        if self._curr_start_end_frames is not None:\n            max_rollback_frame = int(clip(max_rollback_frame, self._curr_start_end_frames[0], max_rollback_frame + self._curr_window_frame_size))\n        self._curr_num_frame = self._bin_search(    max_rollback_frame,\n                                                    curr_num_frame,\n                                                    self._min_window_frame_size )\n        self._is_collided = True\n        return self._curr_num_frame\n\n    def end_collision(self):\n        \"\"\"\n        Reset collision state and prepare for linear growth phase.\n\n        Sets internal flags to transition from collision handling\n        to linear growth phase.\n        \"\"\"\n        self._is_collided = False\n        self._is_cong_avoid = True\n        self._curr_x = 0\n        self._y0_lin = self._min_window_frame_size\n\n    def rewind_to(self,num_frame:int):\n        \"\"\"\n        Rewind video to specific frame.\n\n        Parameters\n        ----------\n        num_frame : int\n            Target frame number\n        \"\"\"\n        if not self._is_forced_speed:\n            if self._is_collided:\n                prev_size_window_frame = self._min_window_frame_size\n            elif self._is_cong_avoid:\n                prev_size_window_frame = clip(  self._lin_step(self._curr_x-1),\n                                                self._min_window_frame_size,\n                                                self._max_size_exp_window_frames )\n            else:\n                prev_size_window_frame = clip(  self._exp_step(self._curr_x-1),\n                                                self._min_window_frame_size,\n                                                self._max_size_exp_window_frames )\n        else:\n            prev_size_window_frame = self._curr_window_frame_size\n        self._curr_num_frame = num_frame - int(prev_size_window_frame)\n\n    def reset(self):\n        self.__init__(*self._init_params)\n\n    def lock_speed(self,num_frames_skipped:'int | None'= None):\n        \"\"\"\n        Lock frame skip rate to fixed value.\n\n        Parameters\n        ----------\n        num_frames_skipped : int or None, optional\n            Number of frames to skip, uses minimum if None\n        \"\"\"\n        self._is_forced_speed = True\n        if num_frames_skipped is None:\n            self._curr_window_frame_size = self._min_window_frame_size\n        else:\n            self._curr_window_frame_size = num_frames_skipped\n            if self._curr_num_frame &lt; 0:\n                self._curr_num_frame = -num_frames_skipped\n\n    def _get_frame_offset(self,offset:int):\n        \"\"\"\n        Returns the i+offset frame with respect to the one set inside the structure\n        \"\"\"\n        prev_speed = self._curr_window_frame_size\n        was_forced = self._is_forced_speed\n        self._is_forced_speed = True\n        self._curr_window_frame_size = offset\n        frame = self.get_frame()\n        self._curr_window_frame_size = prev_speed\n        self._is_forced_speed = was_forced\n        return frame\n\n    def get_following_frame(self):\n        \"\"\"\n        Get next sequential frame.\n\n        Returns\n        -------\n        ndarray\n            Next frame in sequence\n        \"\"\"\n        if self._is_video_ended or self._curr_num_frame + self._min_window_frame_size &gt; self.get_video().get_count_frames()-1:\n            self._curr_num_frame -= (self._min_window_frame_size+1)\n        return self._get_frame_offset(self._min_window_frame_size)\n\n    def set_analysis_frames(self,frames:'list[tuple[int,int]]'):\n        \"\"\"\n        Set specific frame ranges for analysis.\n\n        Parameters\n        ----------\n        frames : list of tuple\n            List of (start_frame, end_frame) ranges\n        \"\"\"\n        self._frames = sorted(frames,reverse=True)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.collide_and_get_fixed_num_frame","title":"<code>collide_and_get_fixed_num_frame()</code>","text":"<p>Handle text detection collision by finding first frame with text.</p> <p>Uses binary search to find the first frame where text appears between current position and last known position.</p> <p>Returns:</p> Type Description <code>int</code> <p>Frame number where text first appears</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def collide_and_get_fixed_num_frame(self):\n    \"\"\"\n    Handle text detection collision by finding first frame with text.\n\n    Uses binary search to find the first frame where text appears\n    between current position and last known position.\n\n    Returns\n    -------\n    int\n        Frame number where text first appears\n    \"\"\"\n    curr_num_frame = self._curr_num_frame\n    max_rollback_frame = int(clip(curr_num_frame - self._curr_window_frame_size, 0, curr_num_frame))\n    if self._curr_start_end_frames is not None:\n        max_rollback_frame = int(clip(max_rollback_frame, self._curr_start_end_frames[0], max_rollback_frame + self._curr_window_frame_size))\n    self._curr_num_frame = self._bin_search(    max_rollback_frame,\n                                                curr_num_frame,\n                                                self._min_window_frame_size )\n    self._is_collided = True\n    return self._curr_num_frame\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.end_collision","title":"<code>end_collision()</code>","text":"<p>Reset collision state and prepare for linear growth phase.</p> <p>Sets internal flags to transition from collision handling to linear growth phase.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def end_collision(self):\n    \"\"\"\n    Reset collision state and prepare for linear growth phase.\n\n    Sets internal flags to transition from collision handling\n    to linear growth phase.\n    \"\"\"\n    self._is_collided = False\n    self._is_cong_avoid = True\n    self._curr_x = 0\n    self._y0_lin = self._min_window_frame_size\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.get_following_frame","title":"<code>get_following_frame()</code>","text":"<p>Get next sequential frame.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Next frame in sequence</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_following_frame(self):\n    \"\"\"\n    Get next sequential frame.\n\n    Returns\n    -------\n    ndarray\n        Next frame in sequence\n    \"\"\"\n    if self._is_video_ended or self._curr_num_frame + self._min_window_frame_size &gt; self.get_video().get_count_frames()-1:\n        self._curr_num_frame -= (self._min_window_frame_size+1)\n    return self._get_frame_offset(self._min_window_frame_size)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.get_frame","title":"<code>get_frame()</code>","text":"<p>Get next frame based on current speed settings.</p> <p>The frame skip rate adapts based on current phase: - Collision: minimum skip rate - Exponential: exponentially increasing skip - Linear: linearly increasing skip - Forced: fixed skip rate</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Next video frame</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_frame(self):\n    \"\"\"\n    Get next frame based on current speed settings.\n\n    The frame skip rate adapts based on current phase:\n    - Collision: minimum skip rate\n    - Exponential: exponentially increasing skip\n    - Linear: linearly increasing skip\n    - Forced: fixed skip rate\n\n    Returns\n    -------\n    ndarray\n        Next video frame\n    \"\"\"\n    if self._is_forced_speed:\n        next_size_window_frame = self._curr_window_frame_size \n    else:\n        if self._is_collided:\n            next_size_window_frame = self._min_window_frame_size\n        else:\n            if self._is_cong_avoid:\n                # linear step\n                next_size_window_frame = clip(  self._lin_step(self._curr_x),\n                                                self._min_window_frame_size,\n                                                self._max_size_lin_window_frames )\n            else:\n                # exponential step\n                next_size_window_frame = clip(  self._exp_step(self._curr_x),\n                                                self._min_window_frame_size,\n                                                self._max_size_exp_window_frames )\n                if next_size_window_frame == self._max_size_exp_window_frames:\n                    self._is_cong_avoid = True\n            self._curr_x += 1\n\n    self._curr_window_frame_size = int(next_size_window_frame)\n\n    if self._frames is not None:\n        self._curr_num_frame = self._get_frame_from_internal_frames()\n\n    vid_ref = self.vid_ref\n    self._curr_num_frame += self._curr_window_frame_size \n    vid_ref.set_num_frame(self._curr_num_frame)\n    frame = vid_ref.extract_next_frame()\n    if frame is None:\n        self._is_video_ended = True\n        num_last_frame = self._get_num_last_frame(vid_ref)\n        vid_ref.set_num_frame(num_last_frame)\n        frame = vid_ref.extract_next_frame()\n        self._curr_num_frame = num_last_frame + 1 \n    return frame\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.get_frame_from_num","title":"<code>get_frame_from_num(num_frame)</code>","text":"<p>Get specific frame by number.</p> <p>Parameters:</p> Name Type Description Default <code>num_frame</code> <code>int</code> <p>Frame number to retrieve</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Requested video frame</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_frame_from_num(self,num_frame:int):\n    \"\"\"\n    Get specific frame by number.\n\n    Parameters\n    ----------\n    num_frame : int\n        Frame number to retrieve\n\n    Returns\n    -------\n    ndarray\n        Requested video frame\n    \"\"\"\n    prev_num_frame = self._curr_num_frame\n    self.vid_ref.set_num_frame(num_frame)\n    frame = self.vid_ref.extract_next_frame()\n    self.vid_ref.set_num_frame(prev_num_frame)\n    return frame\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.get_percentage_progression","title":"<code>get_percentage_progression()</code>","text":"<p>Calculate video processing progress.</p> <p>Returns:</p> Type Description <code>int</code> <p>Percentage of video processed (0-100)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_percentage_progression(self) -&gt; int:\n    \"\"\"\n    Calculate video processing progress.\n\n    Returns\n    -------\n    int\n        Percentage of video processed (0-100)\n    \"\"\"\n    if self._frames is not None:\n        if len(self._frames) &gt; 0:\n            return ceil(self._curr_num_frame/self._frames[0][1] * 100)\n        else:\n            assert self._curr_start_end_frames is not None\n            return ceil(self._curr_num_frame/self._curr_start_end_frames[1] * 100)\n    return ceil(self._curr_num_frame/self.vid_ref.get_count_frames()*100)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.get_video","title":"<code>get_video()</code>","text":"<p>Get reference to video handler.</p> <p>Returns:</p> Type Description <code>LocalVideo</code> <p>Reference to video handler object</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def get_video(self):\n    \"\"\"\n    Get reference to video handler.\n\n    Returns\n    -------\n    LocalVideo\n        Reference to video handler object\n    \"\"\"\n    return self.vid_ref\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.lock_speed","title":"<code>lock_speed(num_frames_skipped=None)</code>","text":"<p>Lock frame skip rate to fixed value.</p> <p>Parameters:</p> Name Type Description Default <code>num_frames_skipped</code> <code>int or None</code> <p>Number of frames to skip, uses minimum if None</p> <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def lock_speed(self,num_frames_skipped:'int | None'= None):\n    \"\"\"\n    Lock frame skip rate to fixed value.\n\n    Parameters\n    ----------\n    num_frames_skipped : int or None, optional\n        Number of frames to skip, uses minimum if None\n    \"\"\"\n    self._is_forced_speed = True\n    if num_frames_skipped is None:\n        self._curr_window_frame_size = self._min_window_frame_size\n    else:\n        self._curr_window_frame_size = num_frames_skipped\n        if self._curr_num_frame &lt; 0:\n            self._curr_num_frame = -num_frames_skipped\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.rewind_to","title":"<code>rewind_to(num_frame)</code>","text":"<p>Rewind video to specific frame.</p> <p>Parameters:</p> Name Type Description Default <code>num_frame</code> <code>int</code> <p>Target frame number</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def rewind_to(self,num_frame:int):\n    \"\"\"\n    Rewind video to specific frame.\n\n    Parameters\n    ----------\n    num_frame : int\n        Target frame number\n    \"\"\"\n    if not self._is_forced_speed:\n        if self._is_collided:\n            prev_size_window_frame = self._min_window_frame_size\n        elif self._is_cong_avoid:\n            prev_size_window_frame = clip(  self._lin_step(self._curr_x-1),\n                                            self._min_window_frame_size,\n                                            self._max_size_exp_window_frames )\n        else:\n            prev_size_window_frame = clip(  self._exp_step(self._curr_x-1),\n                                            self._min_window_frame_size,\n                                            self._max_size_exp_window_frames )\n    else:\n        prev_size_window_frame = self._curr_window_frame_size\n    self._curr_num_frame = num_frame - int(prev_size_window_frame)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/media/video/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.media.video.VideoSpeedManager.set_analysis_frames","title":"<code>set_analysis_frames(frames)</code>","text":"<p>Set specific frame ranges for analysis.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>list of tuple</code> <p>List of (start_frame, end_frame) ranges</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/media/video.py</code> <pre><code>def set_analysis_frames(self,frames:'list[tuple[int,int]]'):\n    \"\"\"\n    Set specific frame ranges for analysis.\n\n    Parameters\n    ----------\n    frames : list of tuple\n        List of (start_frame, end_frame) ranges\n    \"\"\"\n    self._frames = sorted(frames,reverse=True)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/","title":"agreement","text":""},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#agreement","title":"Agreement","text":"<p>Agreement metrics module for concept maps.</p> <p>This module provides functionality for computing various agreement metrics between annotators and creating gold standard annotations from multiple annotators' inputs.</p> <p>Functions:</p> Name Description <code>create_gold</code> <p>Creates gold standard from multiple annotators</p> <code>createAllComb</code> <p>Generates all possible concept pairs</p> <code>createUserRel</code> <p>Creates user relationship pairs from annotations</p> <code>check_trans</code> <p>Checks for transitive relationships</p> <code>creaCoppieAnnot</code> <p>Creates annotation pairs and counts agreements</p> <code>computeK</code> <p>Computes kappa coefficient</p> <code>computeFleiss</code> <p>Computes Fleiss' kappa for multiple raters</p> <code>computeKappaFleiss</code> <p>Helper function for Fleiss' kappa computation</p> <code>checkEachLineCount</code> <p>Validates rating consistency across lines</p>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.checkEachLineCount","title":"<code>checkEachLineCount(mat)</code>","text":"<p>Check that each line has same number of ratings.</p> <p>Parameters:</p> Name Type Description Default <code>mat</code> <code>list</code> <p>Matrix of ratings to check</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of ratings per line</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If lines have different rating counts</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def checkEachLineCount(mat):\n    \"\"\"\n    Check that each line has same number of ratings.\n\n    Parameters\n    ----------\n    mat : list\n        Matrix of ratings to check\n\n    Returns\n    -------\n    int\n        Number of ratings per line\n\n    Raises\n    ------\n    AssertionError\n        If lines have different rating counts\n    \"\"\"\n    \"\"\" Assert that each line has a constant number of ratings\n        @param mat The matrix checked\n        @return The number of ratings\n        @throws AssertionError If lines contain different number of ratings \"\"\"\n    n = sum(mat[0])\n\n\n    assert all(sum(line) == n for line in mat[1:]), \"Line count != %d (n value).\" % n\n\n    return n\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.check_trans","title":"<code>check_trans(rater, term_pairs_tuple, pair)</code>","text":"<p>Check for transitive relationships in annotations.</p> <p>Parameters:</p> Name Type Description Default <code>rater</code> <code>str</code> <p>Rater identifier</p> required <code>term_pairs_tuple</code> <code>dict</code> <p>Dictionary of term pairs by rater</p> required <code>pair</code> <code>str</code> <p>Concept pair to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if transitive relationship exists</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def check_trans(rater, term_pairs_tuple, pair):\n    \"\"\"\n    Check for transitive relationships in annotations.\n\n    Parameters\n    ----------\n    rater : str\n        Rater identifier\n    term_pairs_tuple : dict\n        Dictionary of term pairs by rater\n    pair : str\n        Concept pair to check\n\n    Returns\n    -------\n    bool\n        True if transitive relationship exists\n    \"\"\"\n    # print(pair)\n    # print(rater2)\n    g = networkx.DiGraph(term_pairs_tuple[rater])\n    if pair.split(\"/-/\")[0] in g and pair.split(\"/-/\")[1] in g:\n        if networkx.has_path(g, source=pair.split(\"/-/\")[0], target=pair.split(\"/-/\")[1]):\n            return True\n    else:\n        return False\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.computeFleiss","title":"<code>computeFleiss(term_pairs, all_combs)</code>","text":"<p>Compute Fleiss' kappa for multiple raters.</p> <p>Parameters:</p> Name Type Description Default <code>term_pairs</code> <code>dict</code> <p>Dictionary of term pairs by rater</p> required <code>all_combs</code> <code>list</code> <p>List of all possible combinations</p> required <p>Returns:</p> Type Description <code>float</code> <p>Fleiss' kappa coefficient</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def computeFleiss(term_pairs, all_combs):\n    \"\"\"\n    Compute Fleiss' kappa for multiple raters.\n\n    Parameters\n    ----------\n    term_pairs : dict\n        Dictionary of term pairs by rater\n    all_combs : list\n        List of all possible combinations\n\n    Returns\n    -------\n    float\n        Fleiss' kappa coefficient\n    \"\"\"\n    matrix_fleiss = []\n\n    for item in all_combs:\n\n        countZero = 0\n        countOne = 0\n        for rater, values in term_pairs.items():\n            lista = []\n            if item not in values:\n                countZero = countZero + 1\n            if item in values:\n                countOne = countOne + 1\n        lista.insert(0, countZero)\n        lista.insert(1, countOne)\n        matrix_fleiss.append(lista)\n\n    return computeKappaFleiss(matrix_fleiss)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.computeK","title":"<code>computeK(conteggio, pairs)</code>","text":"<p>Compute kappa coefficient for inter-rater agreement.</p> <p>Parameters:</p> Name Type Description Default <code>conteggio</code> <code>dict</code> <p>Agreement counts dictionary</p> required <code>pairs</code> <code>list</code> <p>List of all possible pairs</p> required <p>Returns:</p> Type Description <code>float</code> <p>Kappa coefficient</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def computeK(conteggio, pairs):\n    \"\"\"\n    Compute kappa coefficient for inter-rater agreement.\n\n    Parameters\n    ----------\n    conteggio : dict\n        Agreement counts dictionary\n    pairs : list\n        List of all possible pairs\n\n    Returns\n    -------\n    float\n        Kappa coefficient\n    \"\"\"\n    Po = (conteggio[\"1,1\"] + conteggio[\"0,0\"]) / float(len(pairs))\n    Pe1 = ((conteggio[\"1,1\"] + conteggio[\"1,0\"]) / float(len(pairs))) * (\n                (conteggio[\"1,1\"] + conteggio[\"0,1\"]) / float(len(pairs)))\n    Pe2 = ((conteggio[\"0,1\"] + conteggio[\"0,0\"]) / float(len(pairs))) * (\n                (conteggio[\"1,0\"] + conteggio[\"0,0\"]) / float(len(pairs)))\n    Pe = Pe1 + Pe2\n    k = (Po - Pe) / float(1 - Pe)\n    return k\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.computeKappaFleiss","title":"<code>computeKappaFleiss(mat)</code>","text":"<p>Compute Fleiss' kappa from rating matrix.</p> <p>Parameters:</p> Name Type Description Default <code>mat</code> <code>list</code> <p>Matrix of ratings subjects</p> required <p>Returns:</p> Type Description <code>float</code> <p>Fleiss' kappa coefficient</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def computeKappaFleiss(mat):\n    \"\"\"\n    Compute Fleiss' kappa from rating matrix.\n\n    Parameters\n    ----------\n    mat : list\n        Matrix of ratings [subjects][categories]\n\n    Returns\n    -------\n    float\n        Fleiss' kappa coefficient\n    \"\"\"\n    \"\"\" Computes the Kappa value\n        @param n Number of rating per subjects (number of human raters)\n        @param mat Matrix[subjects][categories]\n        @return The Kappa value \"\"\"\n    print(mat)\n    n = checkEachLineCount(mat)  # PRE : every line count must be equal to n\n    print(n)\n    N = len(mat)\n    k = len(mat[0])\n\n    # Computing p[] (accordo sugli 0 e accordo sugli 1)\n    p = [0.0] * k\n    for j in range(k):\n        p[j] = 0.0\n        for i in range(N):\n            p[j] += mat[i][j]\n        p[j] /= N * n\n\n    # Computing P[]  (accordo su ogni singola coppia di concetti)\n    P = [0.0] * N\n    for i in range(N):\n        P[i] = 0.0\n        for j in range(k):\n            P[i] += mat[i][j] * mat[i][j]\n        P[i] = (P[i] - n) / (n * (n - 1))\n\n    # Computing Pbar (accordo osservato)\n    Pbar = sum(P) / N\n\n    # Computing PbarE (accordo dovuto al caso)\n    PbarE = 0.0\n    for pj in p:\n        PbarE += pj * pj\n\n    kappa = (Pbar - PbarE) / (1 - PbarE)\n\n    return kappa\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.creaCoppieAnnot","title":"<code>creaCoppieAnnot(rater1, rater2, term_pairs, pairs, term_pairs_tuple)</code>","text":"<p>Create annotation pairs and compute agreement counts.</p> <p>Parameters:</p> Name Type Description Default <code>rater1</code> <code>str</code> <p>First rater identifier</p> required <code>rater2</code> <code>str</code> <p>Second rater identifier</p> required <code>term_pairs</code> <code>dict</code> <p>Dictionary of term pairs by rater</p> required <code>pairs</code> <code>list</code> <p>List of all possible pairs</p> required <code>term_pairs_tuple</code> <code>dict</code> <p>Dictionary of term pair tuples</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(annotation pairs, agreement counts)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def creaCoppieAnnot(rater1, rater2, term_pairs, pairs, term_pairs_tuple):\n    \"\"\"\n    Create annotation pairs and compute agreement counts.\n\n    Parameters\n    ----------\n    rater1 : str\n        First rater identifier\n    rater2 : str\n        Second rater identifier\n    term_pairs : dict\n        Dictionary of term pairs by rater\n    pairs : list\n        List of all possible pairs\n    term_pairs_tuple : dict\n        Dictionary of term pair tuples\n\n    Returns\n    -------\n    tuple\n        (annotation pairs, agreement counts)\n    \"\"\"\n    coppieannot = {}\n    conteggio = {\"1,1\": 0, \"1,0\": 0, \"0,1\": 0, \"0,0\": 0}\n    for pair in pairs:\n        # per ogni concept pair controllo fra le coppie E i paths di r1\n        if pair in term_pairs[rater1] or check_trans(rater1, term_pairs_tuple, pair):\n            # se presente, controllo fra coppie e paths di r2 e incremento i contatori\n            if pair in term_pairs[rater2] or check_trans(rater2, term_pairs_tuple, pair):\n                coppieannot[pair] = \"1,1\"\n                conteggio[\"1,1\"] += 2  # inv_pt1: scelgo di considerare le coppie inverse come both agree\n                conteggio[\"0,0\"] -= 1  # inv_pt2: compenso la scelta di tenenre conto le inverse in both agree\n            # conteggio[\"1,1\"]+=1 #no_inv: le coppie inverse valgolo come both diagree\n            else:\n                coppieannot[pair] = \"1,0\"\n                conteggio[\"1,0\"] += 1\n        # altrimenti, se manca coppia e percorso in r1 e r2 o solo in r1, incrementa questi contatori\n        elif pair not in term_pairs[rater1]:\n            if pair not in term_pairs[rater2] and not check_trans(rater2, term_pairs_tuple, pair):\n                coppieannot[pair] = \"0,0\"\n                conteggio[\"0,0\"] += 1\n            else:\n                coppieannot[pair] = \"0,1\"\n                conteggio[\"0,1\"] += 1\n    return coppieannot, conteggio\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.createAllComb","title":"<code>createAllComb(words)</code>","text":"<p>Create all possible concept pairs from word list.</p> <p>Parameters:</p> Name Type Description Default <code>words</code> <code>list</code> <p>List of concepts/words</p> required <p>Returns:</p> Type Description <code>list</code> <p>All possible unique concept pairs</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def createAllComb(words):\n    \"\"\"\n    Create all possible concept pairs from word list.\n\n    Parameters\n    ----------\n    words : list\n        List of concepts/words\n\n    Returns\n    -------\n    list\n        All possible unique concept pairs\n    \"\"\"\n    #creo tutte le possibili coppie di concetti\n    all_combs=[]\n    for term in words:\n        for i in range(len(words)):\n            if term != words[i]:\n                combination = term+\"/-/\"+words[i]\n                combination_inv = words[i]+\"/-/\"+term\n                if combination_inv not in all_combs:\n                    all_combs.append(combination)\n    return all_combs\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.createUserRel","title":"<code>createUserRel(file, all_combs)</code>","text":"<p>Create user relationship pairs from annotations.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>list</code> <p>List of annotation relationships</p> required <code>all_combs</code> <code>list</code> <p>List of all possible combinations</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(relationships, updated combinations, relationship tuples)</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def createUserRel(file, all_combs):\n    \"\"\"\n    Create user relationship pairs from annotations.\n\n    Parameters\n    ----------\n    file : list\n        List of annotation relationships\n    all_combs : list\n        List of all possible combinations\n\n    Returns\n    -------\n    tuple\n        (relationships, updated combinations, relationship tuples)\n    \"\"\"\n    temp = []\n    term_pairs_tuple = []\n    for annot_pairs in file:\n        concept_pair=annot_pairs[\"prerequisite\"]+\"/-/\"+annot_pairs[\"target\"]\n        if(concept_pair not in all_combs):\n            all_combs.append(concept_pair)\n        temp.append(concept_pair)\n\n        tupla = (annot_pairs[\"prerequisite\"], annot_pairs[\"target\"])\n        term_pairs_tuple.append(tupla)\n\n\n    return temp, all_combs, term_pairs_tuple\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/agreement/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.agreement.create_gold","title":"<code>create_gold(video, annotators, combination_criteria, name)</code>","text":"<p>Create gold standard annotations from multiple annotators.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>str</code> <p>Video identifier</p> required <code>annotators</code> <code>list</code> <p>List of annotator identifiers</p> required <code>combination_criteria</code> <code>str</code> <p>Criteria for combining annotations ('union' supported)</p> required <code>name</code> <code>str</code> <p>Name for the gold standard</p> required <p>Returns:</p> Type Description <code>None</code> <p>Stores results in database</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/agreement.py</code> <pre><code>def create_gold(video, annotators, combination_criteria, name):\n    \"\"\"\n    Create gold standard annotations from multiple annotators.\n\n    Parameters\n    ----------\n    video : str\n        Video identifier\n    annotators : list\n        List of annotator identifiers\n    combination_criteria : str\n        Criteria for combining annotations ('union' supported)\n    name : str\n        Name for the gold standard\n\n    Returns\n    -------\n    None\n        Stores results in database\n    \"\"\"\n    # Function to merge dictionaries\n    def mergeDictionary(d1, d2):\n       d3 = {**d1, **d2}\n       for key in d3.keys():\n           if key in d1 and key in d2:\n                d3[key] = list(set(d3[key]+d1[key]))\n       return d3\n\n    relations = []\n    definitions = []\n    conceptVocabulary = {}\n    if combination_criteria == \"union\":\n        for annotator in annotators:\n            relations += mongo.get_concept_map(annotator, video)\n            definitions += mongo.get_definitions(annotator, video)\n            #db_conceptVocabulary = db_mongo.get_vocabulary(annotator, video) take from db\n            db_conceptVocabulary = None # to start empty\n            if(db_conceptVocabulary != None):\n                conceptVocabulary = mergeDictionary(conceptVocabulary, db_conceptVocabulary)\n\n        # If the concept vocabulary is new (empty) then initialize it to empty synonyms\n        if(conceptVocabulary == {}) :\n            for i in mongo.get_concepts(annotators[0], video):\n                conceptVocabulary[i] = []\n\n        annotations = {\"relations\":relations, \"definitions\":definitions, \"id\":video}\n        _, jsonld = annotations_to_jsonLD(annotations, isAutomatic=True)\n\n        data = jsonld.copy()\n        data[\"video_id\"] = video\n        data[\"graph_type\"] = \"gold standard\"\n        data[\"gold_name\"] = name\n        data[\"conceptVocabulary\"] = create_skos_dictionary(conceptVocabulary, video,\"auto\")\n\n        mongo.insert_gold(data)\n\n\n    print(relations)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/","title":"analysis","text":""},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#analysis","title":"Analysis","text":"<p>Analysis module for concept maps and annotations.</p> <p>This module provides comprehensive functionality for analyzing concept maps, including statistical analysis, agreement computation between annotators, and linguistic analysis of annotations. It includes tools for:</p> <ul> <li>Computing summary statistics of concept maps</li> <li>Calculating inter-annotator agreement metrics</li> <li>Performing linguistic analysis on annotations</li> <li>Detecting transitive relationships</li> <li>Evaluating annotations against gold standards</li> <li>Graph-based analysis of concept relationships</li> </ul> <p>Functions:</p> Name Description <code>compute_data_summary</code> <p>Generate statistical summary of concept maps and definitions</p> <code>compute_agreement</code> <p>Calculate agreement between two concept maps</p> <code>fleiss</code> <p>Compute Fleiss' kappa for multiple annotators</p> <code>linguistic_analysis</code> <p>Analyze linguistic properties of annotations</p> <code>detect_transitive_edges</code> <p>Find transitive relations in concept maps</p> <code>scores</code> <p>Calculate evaluation metrics against gold standard</p> <code>BFS</code> <p>Perform breadth-first search on concept relationships</p> <p>Classes:</p> Name Description <code>Graph</code> <p>Simple directed graph implementation using adjacency lists</p>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.analysis.Graph","title":"<code>Graph</code>","text":"<p>A simple graph implementation using adjacency lists.</p> <p>Attributes:</p> Name Type Description <code>graph</code> <code>dict</code> <p>Dictionary storing adjacency lists for each node</p> <code>nodes</code> <code>list</code> <p>List of all nodes in the graph</p> <p>Methods:</p> Name Description <code>add_edge</code> <p>Add a directed edge from node u to node v</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/analysis.py</code> <pre><code>class Graph:\n    \"\"\"\n    A simple graph implementation using adjacency lists.\n\n    Attributes\n    ----------\n    graph : dict\n        Dictionary storing adjacency lists for each node\n    nodes : list\n        List of all nodes in the graph\n\n    Methods\n    -------\n    add_edge(u, v)\n        Add a directed edge from node u to node v\n    \"\"\"\n    def __init__(self):\n        self.graph = {}\n        self.nodes = []\n\n    def add_edge(self, u, v):\n\n        if u not in self.nodes:\n            self.nodes.append(u)\n            self.graph[u] = []\n\n        if v not in self.nodes:\n            self.nodes.append(v)\n            self.graph[v] = []\n\n        self.graph[u].append(v)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.analysis.BFS","title":"<code>BFS(from_, to_, relations, cut=None)</code>","text":"<p>Perform breadth-first search on concept map relationships.</p> <p>Parameters:</p> Name Type Description Default <code>from_</code> <code>str</code> <p>Starting concept</p> required <code>to_</code> <code>str</code> <p>Target concept</p> required <code>relations</code> <code>list</code> <p>List of concept map relationships</p> required <code>cut</code> <code>int</code> <p>Maximum search depth</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if path exists between concepts, False otherwise</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/analysis.py</code> <pre><code>def BFS(from_, to_, relations, cut=None):\n    \"\"\"\n    Perform breadth-first search on concept map relationships.\n\n    Parameters\n    ----------\n    from_ : str\n        Starting concept\n    to_ : str\n        Target concept\n    relations : list\n        List of concept map relationships\n    cut : int, optional\n        Maximum search depth\n\n    Returns\n    -------\n    bool\n        True if path exists between concepts, False otherwise\n    \"\"\"\n    \"\"\"\n    Breath First Search in concept map\n    \"\"\"\n\n    queue = [from_]\n    already_visited = [from_]\n    count = 0\n\n    targets = {}\n\n    for rel in relations:\n        if rel[\"prerequisite\"] not in targets:\n            targets[rel[\"prerequisite\"]] = []\n\n        targets[rel[\"prerequisite\"]].append(rel[\"target\"])\n\n\n    while len(queue) &gt; 0:\n\n        if cut is not None:\n            if count &gt; cut:\n                return False\n            count += 1\n\n        curr = queue.pop()\n        if curr in targets:\n            next_level = targets[curr]\n        else:\n            next_level = []\n\n        if to_ in next_level:\n            return True\n        else:\n            for i in range(0, len(next_level)):\n                if next_level[i] not in already_visited:\n                    queue.append(next_level[i])\n                    already_visited.append(next_level[i])\n\n    # not found\n    return False\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.analysis.compute_agreement","title":"<code>compute_agreement(concept_map1, concept_map2)</code>","text":"<p>Compute agreement statistics between two concept maps.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map1</code> <code>list</code> <p>First concept map relationships</p> required <code>concept_map2</code> <code>list</code> <p>Second concept map relationships</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Agreement statistics including kappa coefficient</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/analysis.py</code> <pre><code>def compute_agreement(concept_map1, concept_map2):\n    \"\"\"\n    Compute agreement statistics between two concept maps.\n\n    Parameters\n    ----------\n    concept_map1 : list\n        First concept map relationships\n    concept_map2 : list\n        Second concept map relationships\n\n    Returns\n    -------\n    dict\n        Agreement statistics including kappa coefficient\n    \"\"\"\n    # concept_map1 = db_mongo.get_concept_map(user1, video)\n    # concept_map2 = db_mongo.get_concept_map(user2, video)\n    words = []\n    user1 = \"first\"\n    user2 = \"second\"\n\n    for rel in concept_map1:\n        if rel[\"prerequisite\"] not in words:\n            words.append(rel[\"prerequisite\"])\n\n        if rel[\"target\"] not in words:\n            words.append(rel[\"target\"])\n\n    for rel in concept_map2:\n        if rel[\"prerequisite\"] not in words:\n            words.append(rel[\"prerequisite\"])\n\n        if rel[\"target\"] not in words:\n            words.append(rel[\"target\"])\n\n    all_combs = agreement.createAllComb(words)\n\n    # Calcolo agreement kappa no-inv all paths\n    term_pairs = {user1: [], user2: []}\n    term_pairs_tuple = {user1: [], user2: []}\n    term_pairs[user1], all_combs, term_pairs_tuple[user1] = agreement.createUserRel(concept_map1, all_combs)\n    term_pairs[user2], all_combs, term_pairs_tuple[user2] = agreement.createUserRel(concept_map2, all_combs)\n\n    coppieannotate, conteggio = agreement.creaCoppieAnnot(user1, user2, term_pairs, all_combs, term_pairs_tuple)\n\n\n    results = {\"analysis_type\": \"agreement\", \"agreement\":round(agreement.computeK(conteggio, all_combs), 3) if len(all_combs) else 0}\n\n    return results\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.analysis.compute_data_summary","title":"<code>compute_data_summary(video_id, concept_map, definitions)</code>","text":"<p>Compute summary statistics for a concept map and its definitions.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier of the video</p> required <code>concept_map</code> <code>list</code> <p>List of concept map relationships</p> required <code>definitions</code> <code>list</code> <p>List of concept definitions</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Summary statistics including counts of relations, concepts and descriptions</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/analysis.py</code> <pre><code>def compute_data_summary(video_id, concept_map, definitions):\n    \"\"\"\n    Compute summary statistics for a concept map and its definitions.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier of the video\n    concept_map : list\n        List of concept map relationships\n    definitions : list\n        List of concept definitions\n\n    Returns\n    -------\n    dict\n        Summary statistics including counts of relations, concepts and descriptions\n    \"\"\"\n    unique_relations = []\n    strong_relations = []\n    weak_relations = []\n    concepts = []\n\n    G = nx.DiGraph()\n\n    for rel in concept_map:\n\n        G.add_edge(rel[\"prerequisite\"], rel[\"target\"])\n\n        r = {\"prerequisite\":rel[\"prerequisite\"], \"target\": rel[\"target\"]}\n\n        if r not in unique_relations:\n            unique_relations.append(r)\n\n        if rel[\"weight\"] == \"Strong\":\n            strong_relations.append(rel)\n        else:\n            weak_relations.append(rel)\n\n        if rel[\"prerequisite\"] not in concepts:\n            concepts.append(rel[\"prerequisite\"])\n\n        if rel[\"target\"] not in concepts:\n            concepts.append(rel[\"target\"])\n\n    defs = 0\n    depth = 0\n    for d in definitions:\n        if d[\"concept\"] not in concepts:\n            concepts.append(d[\"concept\"])\n\n        if d[\"description_type\"] == \"Definition\":\n            defs += 1\n        else:\n            depth += 1\n\n    results = {\"analysis_type\": \"data_summary\", \"concept_map\": concept_map,\n               \"num_rels\": len(concept_map), \"num_weak\": len(weak_relations),\"num_strong\": len(strong_relations),\n               \"num_unique\": len(unique_relations), \"num_descriptions\":len(definitions), \"num_definitions\":defs,\n               \"num_depth\":depth, \"num_concepts\": len(concepts),\n               \"num_transitives\": len(detect_transitive_edges(G,10))}\n\n    return results\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.analysis.detect_transitive_edges","title":"<code>detect_transitive_edges(graph, cutoff)</code>","text":"<p>Detect transitive relations in a concept map graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DiGraph</code> <p>Directed graph representing concept map</p> required <code>cutoff</code> <code>int</code> <p>Maximum path length to consider</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of tuples containing transitive edges</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/analysis.py</code> <pre><code>def detect_transitive_edges(graph, cutoff):\n    \"\"\"\n    Detect transitive relations in a concept map graph.\n\n    Parameters\n    ----------\n    graph : networkx.DiGraph\n        Directed graph representing concept map\n    cutoff : int\n        Maximum path length to consider\n\n    Returns\n    -------\n    list\n        List of tuples containing transitive edges\n    \"\"\"\n\n    transitives = []\n\n    for source_node in graph.nodes():\n        other_nodes = list(graph.nodes())\n        other_nodes.remove(source_node)\n\n        for target_node in other_nodes:\n            paths = nx.all_simple_paths(graph, source_node, target_node, cutoff)\n\n            for path in paths:\n                if len(path) &gt; 2 and graph.has_edge(source_node, target_node):\n                    if (source_node, target_node) not in transitives:\n                        transitives.append((source_node, target_node))\n\n    return transitives\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.analysis.fleiss","title":"<code>fleiss(video_id)</code>","text":"<p>Compute Fleiss' kappa for multiple annotators.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier of the video</p> required <p>Returns:</p> Type Description <code>float</code> <p>Fleiss' kappa coefficient rounded to 3 decimal places</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/analysis.py</code> <pre><code>def fleiss(video_id):\n    \"\"\"\n    Compute Fleiss' kappa for multiple annotators.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier of the video\n\n    Returns\n    -------\n    float\n        Fleiss' kappa coefficient rounded to 3 decimal places\n    \"\"\"\n    users = mongo.get_graphs_info(video_id)[\"annotators\"]\n\n    words = []\n    concept_maps = {}\n    for user in users:\n        concept_map = mongo.get_concept_map(user[\"id\"], video_id)\n        for rel in concept_map:\n            if rel[\"prerequisite\"] not in words:\n                words.append(rel[\"prerequisite\"])\n\n            if rel[\"target\"] not in words:\n                words.append(rel[\"target\"])\n\n        concept_maps[user[\"id\"]] = concept_map\n\n    all_combs = agreement.createAllComb(words)\n\n    term_pairs = {}\n    for id in concept_maps:\n        term_pairs[id] = agreement.createUserRel(concept_maps[id], all_combs)[0]\n\n    try:\n        fleiss = agreement.computeFleiss(term_pairs, all_combs)\n    except:\n        fleiss = 1\n\n    return round(fleiss, 3)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.analysis.linguistic_analysis","title":"<code>linguistic_analysis(annotator, video_id)</code>","text":"<p>Perform linguistic analysis on annotated concept maps.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>Identifier of the annotator</p> required <code>video_id</code> <code>str</code> <p>Identifier of the video</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Linguistic analysis results including concepts, sentences and CoNLL data</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/analysis.py</code> <pre><code>def linguistic_analysis(annotator, video_id):\n    \"\"\"\n    Perform linguistic analysis on annotated concept maps.\n\n    Parameters\n    ----------\n    annotator : str\n        Identifier of the annotator\n    video_id : str\n        Identifier of the video\n\n    Returns\n    -------\n    dict\n        Linguistic analysis results including concepts, sentences and CoNLL data\n    \"\"\"\n    concept_map = mongo.get_concept_map(annotator, video_id)\n\n    conll = mongo.get_conll(video_id)\n    #print(conll)\n    parsed_conll = parse(conll)\n\n    sent_list = []\n    processed_conll = []\n\n    for sent in parsed_conll:\n        sent_list.append(sent.metadata[\"text\"])\n\n        for word in sent:\n            data = {}\n            data['tok_id'] = word[\"id\"]\n            data['sent_id'] = sent.metadata[\"sent_id\"]\n            data['forma'] = word[\"form\"]\n            data['lemma'] = word[\"lemma\"]\n            data['pos_coarse'] = word[\"upos\"]\n            data['pos_fine'] = word[\"xpos\"]\n\n            processed_conll.append(data)\n\n    concepts = []\n\n    for rel in concept_map:\n        rel[\"sentence\"] = parsed_conll[int(rel[\"sent_id\"])-1].metadata[\"text\"]\n        if rel[\"prerequisite\"] not in concepts:\n            concepts.append(rel[\"prerequisite\"])\n\n        if rel[\"target\"] not in concepts:\n            concepts.append(rel[\"target\"])\n\n\n    results = {\"analysis_type\": \"linguistic\",\"concept_map\": concept_map, \"concepts\": concepts, \"sentences\": sent_list,\n               \"conll\": processed_conll}\n\n    return results\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/analysis/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.analysis.scores","title":"<code>scores(annotation, annotation_gold, concepts)</code>","text":"<p>Calculate evaluation metrics comparing annotation to gold standard.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>list</code> <p>Concept map relationships from annotator</p> required <code>annotation_gold</code> <code>list</code> <p>Gold standard concept map relationships</p> required <code>concepts</code> <code>list</code> <p>List of all concepts</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(accuracy, precision, recall, f1_score) rounded to 3 decimal places</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/analysis.py</code> <pre><code>def scores(annotation, annotation_gold, concepts):\n    \"\"\"\n    Calculate evaluation metrics comparing annotation to gold standard.\n\n    Parameters\n    ----------\n    annotation : list\n        Concept map relationships from annotator\n    annotation_gold : list\n        Gold standard concept map relationships\n    concepts : list\n        List of all concepts\n\n    Returns\n    -------\n    tuple\n        (accuracy, precision, recall, f1_score) rounded to 3 decimal places\n    \"\"\"\n    TP = 0\n    TN = 0\n    FP = 0\n    FN = 0\n\n    paths_gold = []\n    paths_ann = []\n    negative_relations = []\n\n    G_ann = nx.DiGraph()\n    G_gold = nx.DiGraph()\n\n    for rel in annotation:\n\n        rel[\"prerequisite\"] = rel[\"prerequisite\"].replace(\"-\", \" \")\n        rel[\"target\"] = rel[\"target\"].replace(\"-\", \" \")\n\n        G_ann.add_edge(rel[\"prerequisite\"], rel[\"target\"])\n\n    for rel in annotation_gold:\n\n        rel[\"prerequisite\"] = rel[\"prerequisite\"].replace(\"-\", \" \")\n        rel[\"target\"] = rel[\"target\"].replace(\"-\", \" \")\n\n        G_gold.add_edge(rel[\"prerequisite\"], rel[\"target\"])\n\n\n    for c1 in concepts:\n        for c2 in concepts:\n            # se esiste un percorso tra due concetti\n            if c1 in G_gold and c2 in G_gold and nx.has_path(G_gold, c1, c2): #BFS(c1, c2, annotation_gold, cut=300):\n                paths_gold.append((c1, c2))\n            else:\n                negative_relations.append((c1, c2))\n\n            if c1 in G_ann and c2 in G_ann and nx.has_path(G_ann, c1, c2): #BFS(c1, c2, annotation, cut=300):\n                paths_ann.append((c1, c2))\n\n    for r in paths_gold:\n        if r in paths_ann:\n            TP += 1\n        else:\n            FN += 1\n\n    for r in paths_ann:\n        if r not in paths_gold:\n            FP += 1\n\n    for r in random.sample(negative_relations, len(paths_gold)):\n        if r not in paths_ann:\n            TN += 1\n\n\n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n\n    if TP + FP != 0:\n        precision = TP / (TP + FP)\n    else:\n        precision = 0\n\n    if TP + FN != 0:\n        recall = TP / (TP + FN)\n    else:\n        recall = 0\n\n    if precision != 0 or recall != 0:\n        F1 = 2 * (precision * recall) / (precision + recall)\n    else:\n        F1 = 0.0\n\n    return round(accuracy, 3), round(precision, 3), round(recall, 3), round(F1, 3)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/","title":"metrics","text":""},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/#metrics","title":"Metrics","text":"<p>Metrics module for comparing concept maps.</p> <p>This module provides various metrics to compare and evaluate concept maps, particularly focusing on the similarity between automatically generated and manually created concept maps.</p> <p>Functions:</p> Name Description <code>create_graph</code> <p>Creates a directed NetworkX graph from a concept map</p> <code>create_i_graph</code> <p>Creates an iGraph from a dataset and terminology</p> <code>GED_similarity</code> <p>Calculates Graph Edit Distance similarity</p> <code>pageRank_similarity</code> <p>Computes PageRank correlation between graphs</p> <code>edge_overlap</code> <p>Calculates edge overlap similarity between concept maps</p> <code>LO_PN</code> <p>Computes Learning Outcome and Prerequisite Network correlations</p> <code>calculate_metrics</code> <p>Main function that calculates all available metrics between two concept maps</p>"},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.metrics.GED_similarity","title":"<code>GED_similarity(concept_map1, concept_map2)</code>","text":"<p>Calculate Graph Edit Distance similarity between two concept maps.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map1</code> <code>list</code> <p>First concept map as list of relationship dictionaries.</p> required <code>concept_map2</code> <code>list</code> <p>Second concept map as list of relationship dictionaries.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The optimized edit distance between the two graphs.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/metrics.py</code> <pre><code>def GED_similarity(concept_map1, concept_map2):\n    \"\"\"\n    Calculate Graph Edit Distance similarity between two concept maps.\n\n    Parameters\n    ----------\n    concept_map1 : list\n        First concept map as list of relationship dictionaries.\n    concept_map2 : list\n        Second concept map as list of relationship dictionaries.\n\n    Returns\n    -------\n    int\n        The optimized edit distance between the two graphs.\n    \"\"\"\n\n    rater1_graph = create_graph(concept_map1)\n    rater2_graph = create_graph(concept_map2)\n    optimized_edit_distance = 0\n\n    for v in nx.optimize_graph_edit_distance(rater1_graph, rater2_graph,\n                                             node_match=lambda node1, node2: node1['label'] == node2['label']):\n        #print(v)\n        optimized_edit_distance = v\n        break\n\n    #print(\"optimized_edit_distance\", optimized_edit_distance)\n\n    return optimized_edit_distance\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.metrics.LO_PN","title":"<code>LO_PN(graphs, terminology)</code>","text":"<p>Calculate Learning Outcome (LO) and Prerequisite Network (PN) correlations.</p> <p>Parameters:</p> Name Type Description Default <code>graphs</code> <code>list</code> <p>List of dictionaries containing graph and author information.</p> required <code>terminology</code> <code>list</code> <p>List of terms to be considered.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(LO, PN) correlation coefficients as floats.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/metrics.py</code> <pre><code>def LO_PN(graphs, terminology):\n    \"\"\"\n    Calculate Learning Outcome (LO) and Prerequisite Network (PN) correlations.\n\n    Parameters\n    ----------\n    graphs : list\n        List of dictionaries containing graph and author information.\n    terminology : list\n        List of terms to be considered.\n\n    Returns\n    -------\n    tuple\n        (LO, PN) correlation coefficients as floats.\n    \"\"\"\n    # intialise data of leafs\n    temporal_leafs_df = pd.DataFrame(columns=['concept', 'Burst', 'Annotator'])\n\n    # intialise data of roots\n    temporal_roots_df = pd.DataFrame(columns=['concept', 'Burst', 'Annotator'])\n\n    temporal_leafs_roots_df = pd.DataFrame(\n        columns=['concept', 'Burst', 'Annotator'])\n\n    temporal_leafs_df['concept'] = list(terminology)\n    leafs_df = temporal_leafs_df.set_index('concept', verify_integrity=True)\n    temporal_roots_df['concept'] = list(terminology)\n    roots_df = temporal_roots_df.set_index('concept', verify_integrity=True)\n    temporal_leafs_roots_df['concept'] = list(terminology)\n    leafs_roots_df = temporal_leafs_roots_df.set_index('concept', verify_integrity=True)\n\n    for graphs in graphs:\n        networkx_graph = graphs[\"graph\"]\n        rater = graphs[\"author\"]\n\n        leafs = [{x: networkx_graph.in_degree(x)} for x in networkx_graph.nodes() if networkx_graph.out_degree(x) == 0\n                 and networkx_graph.in_degree(x) &gt;= 1]\n\n        for item in leafs:\n            leafs_df.loc[next(iter(item)), rater] = item[next(iter(item))]\n            leafs_roots_df.loc[next(iter(item)), rater] = item[next(iter(item))]\n\n        roots = [{x: networkx_graph.out_degree(x)} for x in networkx_graph.nodes() if networkx_graph.in_degree(x) == 0\n                 and networkx_graph.out_degree(x) &gt;= 1]\n\n        for item in roots:\n            # set negative numbers for the # of out-degree arcs\n            roots_df.loc[next(iter(item)), rater] = item[next(iter(item))]\n            leafs_roots_df.loc[next(iter(item)), rater] = -item[next(iter(item))]\n\n    leafs_df.astype(float)\n    df_0 = leafs_df.fillna(0)\n    LO = df_0.corr(method='pearson').loc[\"Burst\", \"Annotator\"]\n\n    roots_df.astype(float)\n    df_roots_0 = roots_df.fillna(0)\n    PN = df_roots_0.corr(method='pearson').loc[\"Burst\", \"Annotator\"]\n\n    return round(LO,3), round(PN,3)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.metrics.calculate_metrics","title":"<code>calculate_metrics(automatic_map, manual_map, terminology)</code>","text":"<p>Calculate various similarity metrics between automatic and manual concept maps.</p> <p>Parameters:</p> Name Type Description Default <code>automatic_map</code> <code>list</code> <p>Automatically generated concept map as list of relationships.</p> required <code>manual_map</code> <code>list</code> <p>Manually created concept map as list of relationships.</p> required <code>terminology</code> <code>list</code> <p>List of terms used in the concept maps.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(edge_overlap, pagerank_similarity, LO, PN, ged_similarity) metrics.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/metrics.py</code> <pre><code>def calculate_metrics(automatic_map, manual_map, terminology):\n    \"\"\"\n    Calculate various similarity metrics between automatic and manual concept maps.\n\n    Parameters\n    ----------\n    automatic_map : list\n        Automatically generated concept map as list of relationships.\n    manual_map : list\n        Manually created concept map as list of relationships.\n    terminology : list\n        List of terms used in the concept maps.\n\n    Returns\n    -------\n    tuple\n        (edge_overlap, pagerank_similarity, LO, PN, ged_similarity) metrics.\n    \"\"\"\n\n    print(\"***** EKEEL - Video Annotation: metrics.py::calculate_metrics() ******\")\n\n    automatic_graph = create_graph(automatic_map)\n    annotator_graph = create_graph(manual_map)\n\n    graphs = [\n        {\n            \"author\": \"Burst\",\n            \"graph\": automatic_graph,\n        },\n        {\n            \"author\": \"Annotator\",\n            \"graph\": annotator_graph,\n        }\n    ]\n\n    pageRank = pageRank_similarity(graphs, terminology)\n\n    LO, PN = LO_PN(graphs, terminology)\n\n    eo = edge_overlap(automatic_map, manual_map)\n\n    ged_sim = GED_similarity(automatic_map, manual_map)\n\n\n    if np.isnan(pageRank):\n        pageRank = 0\n\n    if np.isnan(LO):\n        LO = 0\n\n    if np.isnan(PN):\n        PN = 0\n\n\n    return eo, pageRank, LO, PN, ged_sim\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.metrics.create_graph","title":"<code>create_graph(concept_map)</code>","text":"<p>Create a directed graph from a concept map.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map</code> <code>list</code> <p>List of dictionaries containing prerequisite and target relationships.</p> required <p>Returns:</p> Type Description <code>DiGraph</code> <p>A directed graph representation of the concept map.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/metrics.py</code> <pre><code>def create_graph(concept_map):\n    \"\"\"\n    Create a directed graph from a concept map.\n\n    Parameters\n    ----------\n    concept_map : list\n        List of dictionaries containing prerequisite and target relationships.\n\n    Returns\n    -------\n    networkx.DiGraph\n        A directed graph representation of the concept map.\n    \"\"\"\n\n    G_nx = nx.DiGraph()\n\n    for rel in concept_map:\n        G_nx.add_node(rel[\"prerequisite\"], label=rel[\"prerequisite\"])\n        G_nx.add_node(rel[\"target\"], label=rel[\"target\"])\n        G_nx.add_edge(rel[\"prerequisite\"], rel[\"target\"])\n\n    return G_nx\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.metrics.create_i_graph","title":"<code>create_i_graph(dataset, terminology)</code>","text":"<p>Create an igraph Graph from dataset and terminology.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>list</code> <p>List of dictionaries containing prerequisite and target relationships.</p> required <code>terminology</code> <code>list</code> <p>List of terms/vertices to be included in the graph.</p> required <p>Returns:</p> Type Description <code>Graph</code> <p>An igraph Graph object representing the concept relationships.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/metrics.py</code> <pre><code>def create_i_graph(dataset, terminology):\n    \"\"\"\n    Create an igraph Graph from dataset and terminology.\n\n    Parameters\n    ----------\n    dataset : list\n        List of dictionaries containing prerequisite and target relationships.\n    terminology : list\n        List of terms/vertices to be included in the graph.\n\n    Returns\n    -------\n    igraph.Graph\n        An igraph Graph object representing the concept relationships.\n    \"\"\"\n    #print(igraph.__version__)\n    terminology#_vertex = set(dataset[\"prerequisite\"].append(dataset[\"target\"]))\n    # I_graph = igraph.Graph(n=0, edges=None, directed=True, graph_attrs=None, vertex_attrs=None, edge_attrs=None)\n    i_graph = igraph.Graph()\n    # print(dataset[\"prerequisite\"])\n    for v in terminology:#_vertex:\n        i_graph.add_vertices(v)\n    for r in dataset:\n        i_graph.add_edge(r[\"prerequisite\"], r[\"target\"])\n\n    return i_graph\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.metrics.edge_overlap","title":"<code>edge_overlap(concept_map1, concept_map2)</code>","text":"<p>Calculate edge overlap similarity between two concept maps.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map1</code> <code>list</code> <p>First concept map as list of relationship dictionaries.</p> required <code>concept_map2</code> <code>list</code> <p>Second concept map as list of relationship dictionaries.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Vertex and Edge Overlap score between the two concept maps.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/metrics.py</code> <pre><code>def edge_overlap(concept_map1, concept_map2):\n    \"\"\"\n    Calculate edge overlap similarity between two concept maps.\n\n    Parameters\n    ----------\n    concept_map1 : list\n        First concept map as list of relationship dictionaries.\n    concept_map2 : list\n        Second concept map as list of relationship dictionaries.\n\n    Returns\n    -------\n    float\n        Vertex and Edge Overlap score between the two concept maps.\n    \"\"\"\n    count_edge_comuni = 0\n    count_vertex_comuni = 0\n\n    vertexes1 = []\n    vertexes2 = []\n\n    for rel in concept_map2:\n        if rel[\"prerequisite\"] not in vertexes2:\n            vertexes2.append(rel[\"prerequisite\"])\n\n        if rel[\"target\"] not in vertexes2:\n            vertexes2.append(rel[\"target\"])\n\n\n    for rel in concept_map1:\n\n        if rel[\"prerequisite\"] not in vertexes1:\n            vertexes1.append(rel[\"prerequisite\"])\n            if rel[\"prerequisite\"] in vertexes2:\n                count_vertex_comuni += 1\n\n        if rel[\"target\"] not in vertexes1:\n            vertexes1.append(rel[\"target\"])\n            if rel[\"target\"] in vertexes2:\n                count_vertex_comuni += 1\n\n        for r in concept_map2:\n            if r[\"target\"] == rel[\"target\"] and r[\"prerequisite\"] == rel[\"prerequisite\"]:\n                count_edge_comuni += 1\n    if len(vertexes1)+len(vertexes2)+len(concept_map1)+len(concept_map2) != 0:\n        VEO = 2 * ((count_vertex_comuni + count_edge_comuni)/(len(vertexes1)+len(vertexes2)+len(concept_map1)+len(concept_map2)))\n    else:\n        return 0\n\n    return round(VEO, 3)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/metrics/metrics/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.metrics.metrics.pageRank_similarity","title":"<code>pageRank_similarity(graphs, terminology)</code>","text":"<p>Calculate PageRank similarity between graphs.</p> <p>Parameters:</p> Name Type Description Default <code>graphs</code> <code>list</code> <p>List of dictionaries containing graph and author information.</p> required <code>terminology</code> <code>list</code> <p>List of terms to be considered.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Pearson correlation coefficient of PageRank scores between graphs.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/metrics/metrics.py</code> <pre><code>def pageRank_similarity(graphs, terminology):\n    \"\"\"\n    Calculate PageRank similarity between graphs.\n\n    Parameters\n    ----------\n    graphs : list\n        List of dictionaries containing graph and author information.\n    terminology : list\n        List of terms to be considered.\n\n    Returns\n    -------\n    float\n        Pearson correlation coefficient of PageRank scores between graphs.\n    \"\"\"\n\n    pagerank_df = pd.DataFrame(columns=['concept', 'Burst', 'Annotator'])\n    pagerank_df['concept'] = list(terminology)\n    rank = pagerank_df.set_index('concept', verify_integrity=True)\n\n    for graph in graphs:\n        pagerank = nx.pagerank(graph[\"graph\"])\n        for k, v in pagerank.items():\n            rank.loc[k, graph[\"author\"]] = v\n\n    rank.astype(float)\n    p = rank.fillna(0)\n\n    return round(p.corr(method='pearson').loc[\"Burst\", \"Annotator\"], 3)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/","title":"Method_01","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#method-01","title":"Method 01","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_01.Method_1","title":"<code>Method_1</code>","text":"<p>Class for evaluating prerequisite relationship using hyponyms, hypernyms and meronyms Attributes:     - words: list of strings     - pre_req: dict with all prerequisites</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_01.py</code> <pre><code>class Method_1():\n    \"\"\"\n    Class for evaluating prerequisite relationship using hyponyms, hypernyms and meronyms\n    Attributes:\n        - words: list of strings\n        - pre_req: dict with all prerequisites\n    \"\"\"\n\n    def __init__(self, words):\n        \"\"\"\n            :param words: array of concepts\n        \"\"\"\n        self.words = words\n        self.pre_req = dict.fromkeys(words)\n        for word in self.pre_req:\n            self.pre_req[word] = []\n\n\n    def hyponyms(self, concept):\n        \"\"\" get a concept and takes all meanings from wordnet. Then gets all the hyponyms of\n            that word and check if it's inside the list words\n             :param concept: string\n             :return\n        \"\"\"\n        meanings = wn.synsets(concept)\n        for word in meanings:\n            for types in word.hyponyms():\n                for lemma in types.lemmas():\n                    self.search_inv(concept, lemma.name().lower())\n\n    def hypernyms(self, concept):\n        \"\"\" get a concept and takes all meanings from wordnet. Then gets all the hypernyms of\n            that word and check if it's inside the list words\n            :param concept: string\n        \"\"\"\n\n        # iperonimi fino alla root\n        meanings = wn.synsets(concept)\n        for word in meanings:\n            for paths in (word.hypernym_paths()):\n                for level in paths:\n                    for lemma in level.lemmas():\n                        self.search(concept, lemma.name().lower())\n\n    def meronyms(self, concept):\n        \"\"\" get a concept and takes all meanings from wordnet. Then gets all the different meronyms of\n            that word and check if it's inside the list words\n            :param concept:string\n        \"\"\"\n        meanings = wn.synsets(concept)\n        for word in meanings:\n            for meronym in word.part_meronyms():\n                for lemma in meronym.lemmas():\n                    self.search_inv(concept, lemma.name().lower())\n            for meronym in word.substance_meronyms():\n                for lemma in meronym.lemmas():\n                    self.search_inv(concept, lemma.name().lower())\n            for meronym in word.member_holonyms():\n                for lemma in meronym.lemmas():\n                    self.search_inv(concept, lemma.name().lower())\n\n    def search(self, concept, lemma):\n        \"\"\" add lemma as prerequisite of concept\n             :param concept\n             :param lemma\n        \"\"\"\n        if (lemma in self.words):\n            self.pre_req[concept].append(lemma)\n\n    def search_inv(self, concept, lemma):\n        \"\"\" add concept as prerequisite of lemma\n             :param concept\n             :param lemma\n        \"\"\"\n        if (lemma in self.words):\n            self.pre_req[lemma].append(concept)\n\n\n\n\n    def launch(self):\n        \"\"\"Launch Method 1 and update database\"\"\"\n        concept_map = []\n        try:\n            self.words = [word.lower() for word in self.words]\n\n            for i, word in enumerate(self.words):\n                self.words.remove(word)\n                self.hyponyms(word)\n                self.hypernyms(word)\n                self.meronyms(word)\n                self.words.insert(i, word)\n\n\n            for concept in self.words:\n                for lemma in [lemma for lemma in self.words if concept != lemma]:\n                    if lemma in self.pre_req[concept]:\n                        concept_map.append({\"prerequisite\": lemma, \"target\": concept})\n\n\n\n        except:\n            print(\"error:\", sys.exc_info())\n            raise\n\n\n        return concept_map\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_01.Method_1.__init__","title":"<code>__init__(words)</code>","text":"<p>:param words: array of concepts</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_01.py</code> <pre><code>def __init__(self, words):\n    \"\"\"\n        :param words: array of concepts\n    \"\"\"\n    self.words = words\n    self.pre_req = dict.fromkeys(words)\n    for word in self.pre_req:\n        self.pre_req[word] = []\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_01.Method_1.hypernyms","title":"<code>hypernyms(concept)</code>","text":"<p>get a concept and takes all meanings from wordnet. Then gets all the hypernyms of that word and check if it's inside the list words :param concept: string</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_01.py</code> <pre><code>def hypernyms(self, concept):\n    \"\"\" get a concept and takes all meanings from wordnet. Then gets all the hypernyms of\n        that word and check if it's inside the list words\n        :param concept: string\n    \"\"\"\n\n    # iperonimi fino alla root\n    meanings = wn.synsets(concept)\n    for word in meanings:\n        for paths in (word.hypernym_paths()):\n            for level in paths:\n                for lemma in level.lemmas():\n                    self.search(concept, lemma.name().lower())\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_01.Method_1.hyponyms","title":"<code>hyponyms(concept)</code>","text":"<p>get a concept and takes all meanings from wordnet. Then gets all the hyponyms of that word and check if it's inside the list words  :param concept: string  :return</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_01.py</code> <pre><code>def hyponyms(self, concept):\n    \"\"\" get a concept and takes all meanings from wordnet. Then gets all the hyponyms of\n        that word and check if it's inside the list words\n         :param concept: string\n         :return\n    \"\"\"\n    meanings = wn.synsets(concept)\n    for word in meanings:\n        for types in word.hyponyms():\n            for lemma in types.lemmas():\n                self.search_inv(concept, lemma.name().lower())\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_01.Method_1.launch","title":"<code>launch()</code>","text":"<p>Launch Method 1 and update database</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_01.py</code> <pre><code>def launch(self):\n    \"\"\"Launch Method 1 and update database\"\"\"\n    concept_map = []\n    try:\n        self.words = [word.lower() for word in self.words]\n\n        for i, word in enumerate(self.words):\n            self.words.remove(word)\n            self.hyponyms(word)\n            self.hypernyms(word)\n            self.meronyms(word)\n            self.words.insert(i, word)\n\n\n        for concept in self.words:\n            for lemma in [lemma for lemma in self.words if concept != lemma]:\n                if lemma in self.pre_req[concept]:\n                    concept_map.append({\"prerequisite\": lemma, \"target\": concept})\n\n\n\n    except:\n        print(\"error:\", sys.exc_info())\n        raise\n\n\n    return concept_map\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_01.Method_1.meronyms","title":"<code>meronyms(concept)</code>","text":"<p>get a concept and takes all meanings from wordnet. Then gets all the different meronyms of that word and check if it's inside the list words :param concept:string</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_01.py</code> <pre><code>def meronyms(self, concept):\n    \"\"\" get a concept and takes all meanings from wordnet. Then gets all the different meronyms of\n        that word and check if it's inside the list words\n        :param concept:string\n    \"\"\"\n    meanings = wn.synsets(concept)\n    for word in meanings:\n        for meronym in word.part_meronyms():\n            for lemma in meronym.lemmas():\n                self.search_inv(concept, lemma.name().lower())\n        for meronym in word.substance_meronyms():\n            for lemma in meronym.lemmas():\n                self.search_inv(concept, lemma.name().lower())\n        for meronym in word.member_holonyms():\n            for lemma in meronym.lemmas():\n                self.search_inv(concept, lemma.name().lower())\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_01.Method_1.search","title":"<code>search(concept, lemma)</code>","text":"<p>add lemma as prerequisite of concept :param concept :param lemma</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_01.py</code> <pre><code>def search(self, concept, lemma):\n    \"\"\" add lemma as prerequisite of concept\n         :param concept\n         :param lemma\n    \"\"\"\n    if (lemma in self.words):\n        self.pre_req[concept].append(lemma)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_01/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_01.Method_1.search_inv","title":"<code>search_inv(concept, lemma)</code>","text":"<p>add concept as prerequisite of lemma :param concept :param lemma</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_01.py</code> <pre><code>def search_inv(self, concept, lemma):\n    \"\"\" add concept as prerequisite of lemma\n         :param concept\n         :param lemma\n    \"\"\"\n    if (lemma in self.words):\n        self.pre_req[lemma].append(concept)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_02/","title":"Method_02","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_02/#method-02","title":"Method 02","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_02/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_02.Method_2","title":"<code>Method_2</code>","text":"<p>Class for evaluating prerequisite relationship using lexical patterns</p> <p>Attributes:     - sentence: list with all the sentences of the book     - words: list of concepts     - pre_req: dict with all prerequisites     - text: text of the book     - phrase: dict with keys [concept1_concept2] containing the phrase of the relation     - phrase_id: dict with keys [concept1_concept2] containing the phrase id of the relation</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_02.py</code> <pre><code>class Method_2():\n    \"\"\"\n    Class for evaluating prerequisite relationship using lexical patterns\n\n    Attributes:\n        - sentence: list with all the sentences of the book\n        - words: list of concepts\n        - pre_req: dict with all prerequisites\n        - text: text of the book\n        - phrase: dict with keys [concept1_concept2] containing the phrase of the relation\n        - phrase_id: dict with keys [concept1_concept2] containing the phrase id of the relation\n    \"\"\"\n\n    lex_pattern = [\"/y/ such as /x/ \", \"/y/ such as a /x/ \", \"such /y/ as an /x/\", \"/x/ is a /y/\", \"/x/ is an /y/\", \"/y/ includes /x/\", \"/y/ includs /x/\", \"/y/ including /x/\", \"/x/ is called /y/\", \"/x/ are called /y/\", \"/x/, one of /y/\", \"/x/ and other /y/\", \"/x/ or other /y/\", \"/y/ consist of /x/\", \"/y/ consists of /x/\", \"/y/ like /x/\", \"/y/, specially /x/\", \"/x/ in /y/\", \"/x/ belong to /y/\"]\n\n    def __init__(self, words, conll, text):\n        \"\"\" Initialization of attributes \"\"\"\n        self.sentence = parse(conll)\n        self.words = words\n        self.text = text\n        self.pre_req = dict.fromkeys(words)\n        self.phrase_id = {}\n        self.phrase = {}\n        for word in self.pre_req:\n            self.pre_req[word] = []\n\n\n\n\n    def pattern(self, word, prereq):\n        \"\"\" If the relation prereq -&gt; word is found, update the attributes \"\"\"\n        for phrase in self.lex_pattern:\n            phrase = phrase.replace(\"/x/\", word)\n            phrase = phrase.replace(\"/y/\", prereq)\n\n            result = self.text.find(phrase)\n\n            if result != -1:\n                sentence_id = self.id_to_sentence(result)\n\n                # Se in questa frase non ci sono ancora relazioni\n                if sentence_id not in list(self.phrase_id.values()):\n                    if prereq not in self.pre_req[word]:\n                        self.pre_req[word].append(prereq)\n                    self.phrase_id[word + \"_\" + prereq] = sentence_id\n                    self.phrase[word + \"_\" + prereq] = phrase\n\n                else:\n                    '''nel caso di pi\u00f9 relazioni trovate nella stessa frase devo prenderle tutte\n                    ma nel caso di una relazione multiword devo prendere solo quella con pi\u00f9 parole'''\n\n                    new_rel = phrase\n                    old_rels = []\n\n                    # prendo tutte le relazioni gi\u00e0 messe nella frase\n                    for r in self.phrase_id:\n                        if self.phrase_id[r] == sentence_id:\n                            old_rels.append(r)\n\n                        #  per ogni relazione nella frase\n                    for old_rel in old_rels:\n                        lemmas = old_rel.split(\"_\")\n\n                        # se una nuova relazione contiene una relazione vecchia (nella stessa frase),\n                        # elimino la vecchia e metto la nuova\n                        # ad esempio:  \"phone network is an internet\" contiene \"network is an internet\" -&gt; la levo\n                        if self.phrase[old_rel] in new_rel:\n\n                            self.pre_req[lemmas[0]].remove(lemmas[1])\n                            del self.phrase_id[old_rel]\n                            del self.phrase[old_rel]\n                            if prereq not in self.pre_req[word]:\n                                self.pre_req[word].append(prereq)\n                            self.phrase_id[word + \"_\" + prereq] = sentence_id\n                            self.phrase[word + \"_\" + prereq] = new_rel\n\n                        # se la nuova relazione non \u00e8 contenuta in una vecchia, aggiungo la relazione\n                        elif new_rel not in self.phrase[old_rel]:\n                            if prereq not in self.pre_req[word]:\n                                self.pre_req[word].append(prereq)\n                            self.phrase_id[word + \"_\" + prereq] = sentence_id\n                            self.phrase[word + \"_\" + prereq] = new_rel\n\n\n    def id_to_sentence(self, key):\n        stop = 0\n        for ids, phrase in enumerate(self.sentence):\n            for words in phrase:\n                stop += len(words[\"form\"])\n                stop += 1 # aggiungo lunghzza dello spazio tra una parola e l'altra\n            if stop &gt;= key:\n                return ids + 1\n\n\n\n\n    def launch(self):\n        \"\"\"Launch Method 2 and update database\"\"\"\n        concept_map = []\n        try:\n\n            for word in self.words:\n                for prereq in [x for x in self.words if x != word]:\n                    self.pattern(word, prereq)\n\n            for concept in self.words:\n                for lemma in [lemma for lemma in self.words if concept != lemma]:\n                    if lemma in self.pre_req[concept]:\n                        concept_map.append({\"prerequisite\": lemma, \"target\": concept})\n\n\n        except:\n\n            print(\"error:\", sys.exc_info())\n            raise\n\n        return concept_map\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_02/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_02.Method_2.__init__","title":"<code>__init__(words, conll, text)</code>","text":"<p>Initialization of attributes</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_02.py</code> <pre><code>def __init__(self, words, conll, text):\n    \"\"\" Initialization of attributes \"\"\"\n    self.sentence = parse(conll)\n    self.words = words\n    self.text = text\n    self.pre_req = dict.fromkeys(words)\n    self.phrase_id = {}\n    self.phrase = {}\n    for word in self.pre_req:\n        self.pre_req[word] = []\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_02/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_02.Method_2.launch","title":"<code>launch()</code>","text":"<p>Launch Method 2 and update database</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_02.py</code> <pre><code>def launch(self):\n    \"\"\"Launch Method 2 and update database\"\"\"\n    concept_map = []\n    try:\n\n        for word in self.words:\n            for prereq in [x for x in self.words if x != word]:\n                self.pattern(word, prereq)\n\n        for concept in self.words:\n            for lemma in [lemma for lemma in self.words if concept != lemma]:\n                if lemma in self.pre_req[concept]:\n                    concept_map.append({\"prerequisite\": lemma, \"target\": concept})\n\n\n    except:\n\n        print(\"error:\", sys.exc_info())\n        raise\n\n    return concept_map\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_02/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_02.Method_2.pattern","title":"<code>pattern(word, prereq)</code>","text":"<p>If the relation prereq -&gt; word is found, update the attributes</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_02.py</code> <pre><code>def pattern(self, word, prereq):\n    \"\"\" If the relation prereq -&gt; word is found, update the attributes \"\"\"\n    for phrase in self.lex_pattern:\n        phrase = phrase.replace(\"/x/\", word)\n        phrase = phrase.replace(\"/y/\", prereq)\n\n        result = self.text.find(phrase)\n\n        if result != -1:\n            sentence_id = self.id_to_sentence(result)\n\n            # Se in questa frase non ci sono ancora relazioni\n            if sentence_id not in list(self.phrase_id.values()):\n                if prereq not in self.pre_req[word]:\n                    self.pre_req[word].append(prereq)\n                self.phrase_id[word + \"_\" + prereq] = sentence_id\n                self.phrase[word + \"_\" + prereq] = phrase\n\n            else:\n                '''nel caso di pi\u00f9 relazioni trovate nella stessa frase devo prenderle tutte\n                ma nel caso di una relazione multiword devo prendere solo quella con pi\u00f9 parole'''\n\n                new_rel = phrase\n                old_rels = []\n\n                # prendo tutte le relazioni gi\u00e0 messe nella frase\n                for r in self.phrase_id:\n                    if self.phrase_id[r] == sentence_id:\n                        old_rels.append(r)\n\n                    #  per ogni relazione nella frase\n                for old_rel in old_rels:\n                    lemmas = old_rel.split(\"_\")\n\n                    # se una nuova relazione contiene una relazione vecchia (nella stessa frase),\n                    # elimino la vecchia e metto la nuova\n                    # ad esempio:  \"phone network is an internet\" contiene \"network is an internet\" -&gt; la levo\n                    if self.phrase[old_rel] in new_rel:\n\n                        self.pre_req[lemmas[0]].remove(lemmas[1])\n                        del self.phrase_id[old_rel]\n                        del self.phrase[old_rel]\n                        if prereq not in self.pre_req[word]:\n                            self.pre_req[word].append(prereq)\n                        self.phrase_id[word + \"_\" + prereq] = sentence_id\n                        self.phrase[word + \"_\" + prereq] = new_rel\n\n                    # se la nuova relazione non \u00e8 contenuta in una vecchia, aggiungo la relazione\n                    elif new_rel not in self.phrase[old_rel]:\n                        if prereq not in self.pre_req[word]:\n                            self.pre_req[word].append(prereq)\n                        self.phrase_id[word + \"_\" + prereq] = sentence_id\n                        self.phrase[word + \"_\" + prereq] = new_rel\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_03/","title":"Method_03","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_03/#method-03","title":"Method 03","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_03/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_03.count_concept","title":"<code>count_concept(words, links, page_words)</code>","text":"<p>Append every link to links array</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_03.py</code> <pre><code>def count_concept(words, links, page_words):\n    \"\"\" Append every link to links array\"\"\"\n\n    for concept in words:\n        for list_link in words:\n            if (page_words[str(list_link) + \"_links\"]):\n                if (concept.capitalize() in page_words[list_link + \"_links\"]):\n                    links.append(1)\n                else:\n                    links.append(0)\n            else:\n                links.append(0)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_03/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_03.counter_df","title":"<code>counter_df(links, count_df, length, words)</code>","text":"<p>Append in count df the links presented in every concept page. so every cell of count_df rapresent the numbers of links in a page</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_03.py</code> <pre><code>def counter_df(links, count_df, length, words):\n    \"\"\" Append in count df the links presented in every concept page. so every cell of count_df\n    rapresent the numbers of links in a page \"\"\"\n    for n in range(length):\n        count_df.append(sum(links[int(n)*length:length*(int(n)+1)]))    \n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_03/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_03.page_finder","title":"<code>page_finder(words, page_words)</code>","text":"<p>For every concept takes the out links and save them in a dictionary</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_03.py</code> <pre><code>def page_finder(words, page_words):\n    \"\"\" For every concept takes the out links and save them in a dictionary\"\"\"\n\n    for concept, title in words.items():\n        if title:\n            try:\n                poss = wikipedia.page(title=title, auto_suggest=False)\n                page_words[concept] = poss\n                page_words[concept + \"_links\"] = poss.links\n            except wikipedia.exceptions.DisambiguationError as e:\n                page_words[concept] = None\n                page_words[concept + \"_links\"] = None\n            except wikipedia.exceptions.PageError as e:\n                page_words[concept] = None\n                page_words[concept + \"_links\"] = None\n            except requests.exceptions.RequestException as e:\n                # troppe richieste insieme a wikipedia, mi fermo e riprovo\n                print(e)\n                sleep(5)\n                continue\n        else:\n            page_words[concept] = None\n            page_words[concept + \"_links\"] = None\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_04/","title":"Method_04","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_04/#method-04","title":"Method 04","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/Method_04/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.misc_unused.Method_04.out_links","title":"<code>out_links(b, a, page_words)</code>","text":"<p>Method to check outlinks in a page</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/misc_unused/Method_04.py</code> <pre><code>def out_links(b, a, page_words):\n    \"\"\"Method to check outlinks in a page\"\"\"\n    return (len(page_words[a].links) - len(page_words[b].links))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/misc_unused/skos_synonyms_query/","title":"skos_synonyms_query","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/skos_synonyms_query/#skos-synonyms-query","title":"Skos Synonyms Query","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/summary/","title":"summary","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/summary/#summary","title":"Summary","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/wikipe/","title":"wikipe","text":""},{"location":"codebase/EKEELVideoAnnotation/misc_unused/wikipe/#wikipe","title":"Wikipe","text":""},{"location":"codebase/EKEELVideoAnnotation/models/xgboost_adapter/","title":"xgboost_adapter","text":""},{"location":"codebase/EKEELVideoAnnotation/models/xgboost_adapter/#xgboost-adapter","title":"Xgboost Adapter","text":"<p>This module provides an adapter for the XGBoost pretrained model used for lecture type detection and text recognition.</p> <p>Classes:</p> Name Description <code>XGBoostModelAdapter</code> <p>Model adapter for the XGBoost pretrained model <code>xgboost500.sav</code> that classifies the lesson kind from images.</p>"},{"location":"codebase/EKEELVideoAnnotation/models/xgboost_adapter/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.models.xgboost_adapter.XGBoostModelAdapter","title":"<code>XGBoostModelAdapter</code>","text":"<p>Model adapter for the XGBoost pretrained model from the misc/lecture type detection and text recognition folder.</p> <p>Attributes:</p> Name Type Description <code>_labels</code> <code>dict</code> <p>A dictionary mapping label indices to their corresponding names.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the model adapter with the given model path.</p> <code>_extract_faces_info</code> <p>Extracts face information from the current image.</p> <code>_extract_features_from_image</code> <p>Extracts features from the given image.</p> <code>predict_probability</code> <p>Predicts the probability distribution over classes for the given image.</p> <code>predict_max_confidence</code> <p>Predicts the class with the highest confidence for the given image.</p> <code>get_label</code> <p>Gets the label corresponding to the given prediction.</p> <code>is_enough_slidish_like</code> <p>Predicts if the image is likely to be a slide with a small margin of confidence.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/models/xgboost_adapter.py</code> <pre><code>class XGBoostModelAdapter:\n    '''\n    Model adapter for the XGBoost pretrained model from the\n    misc/lecture type detection and text recognition folder.\n\n    Attributes\n    ----------\n    _labels : dict\n        A dictionary mapping label indices to their corresponding names.\n\n    Methods\n    -------\n    __init__(model_path: str) -&gt; None\n        Initializes the model adapter with the given model path.\n    _extract_faces_info(detections: 'list[Detection] | None')\n        Extracts face information from the current image.\n    _extract_features_from_image(image: ImageClassifier, norm_minmax: bool = False)\n        Extracts features from the given image.\n    predict_probability(image: ImageClassifier)\n        Predicts the probability distribution over classes for the given image.\n    predict_max_confidence(image: ImageClassifier)\n        Predicts the class with the highest confidence for the given image.\n    get_label(prediction)\n        Gets the label corresponding to the given prediction.\n    is_enough_slidish_like(image: ImageClassifier)\n        Predicts if the image is likely to be a slide with a small margin of confidence.\n    '''\n\n    _labels = {0: \"Blackboard\", 1: \"Slide\", 2: \"Slide-and-Talk\", 3: \"Talk\"}\n\n    def __init__(self, model_path: str) -&gt; None:\n        '''\n        Initializes the model adapter with the given model path.\n\n        Parameters\n        ----------\n        model_path : str\n            The path to the pretrained XGBoost model.\n        '''\n        try:\n            self._model: XGBClassifier = load_model(open(model_path, 'rb'))\n        except:\n            raise FileExistsError(\"cannot find XGBoost model\")\n\n    def _extract_faces_info(self, detections: 'list[Detection] | None'):\n        '''\n        Extracts face information from the current image.\n\n        Parameters\n        ----------\n        detections : list[Detection] or None\n            A list of face detections or None if no faces are detected.\n\n        Returns\n        -------\n        out_arr : ndarray\n            An array with face information: [x_center, face_size, n_faces].\n        '''\n        out_arr = zeros((1, 3), dtype=float)\n        if detections is None:\n            return out_arr\n        for detection in detections:\n            bounding_box = detection.bounding_box\n            xmin, width, height = bounding_box.origin_x, bounding_box.width, bounding_box.height\n            face_size = width * height\n            out_arr[0, 1] = max(out_arr[0, 1], face_size)\n            if out_arr[0, 1] == face_size:\n                out_arr[0, 0] = xmin + width / 2\n            if out_arr[0, 2] &lt; 2:\n                out_arr[0, 2] += 1\n        return out_arr\n\n    def _extract_features_from_image(self, image: ImageClassifier, norm_minmax: bool = False):\n        '''\n        Extracts features from the given image.\n\n        Parameters\n        ----------\n        image : ImageClassifier\n            The image classifier object.\n        norm_minmax : bool, optional\n            Whether to normalize the features using min-max normalization (default is False).\n\n        Returns\n        -------\n        out_arr : ndarray\n            An array with extracted features.\n        '''\n        assert isinstance(image, ImageClassifier) and image.get_img().shape[2] == 3\n        out_arr = empty((1, 19), dtype=float)\n        out_arr[0, :16] = image.get_hists(normalize=norm_minmax, bins=16, grayscaled=True)\n        if not norm_minmax:\n            out_arr[0, :16] /= prod(image.get_img_shape()[:2])\n        out_arr[0, 16:] = self._extract_faces_info(image.detect_faces())\n        return out_arr\n\n    def predict_probability(self, image: ImageClassifier):\n        '''\n        Predicts the probability distribution over classes for the given image.\n\n        Parameters\n        ----------\n        image : ImageClassifier\n            The image classifier object.\n\n        Returns\n        -------\n        probs : ndarray\n            The probability distribution over classes.\n        '''\n        return self._model.predict_proba(self._extract_features_from_image(image))\n\n    def predict_max_confidence(self, image: ImageClassifier):\n        '''\n        Predicts the class with the highest confidence for the given image.\n\n        Parameters\n        ----------\n        image : ImageClassifier\n            The image classifier object.\n\n        Returns\n        -------\n        prediction : int\n            The class index with the highest confidence.\n        '''\n        return int(argmax(self.predict_probability(self._extract_features_from_image(image))))\n\n    def get_label(self, prediction):\n        '''\n        Gets the label corresponding to the given prediction.\n\n        Parameters\n        ----------\n        prediction : int or ndarray\n            The prediction index or probability distribution.\n\n        Returns\n        -------\n        label : str or dict\n            The label corresponding to the prediction index or a dictionary of labels with their probabilities.\n        '''\n        if isinstance(prediction, int):\n            assert 0 &lt;= prediction &lt;= 3\n            return self._labels[prediction]\n        elif isinstance(prediction, ndarray):\n            assert prediction.shape == (1, 4)\n            return {self._labels[i]: prediction[0, i] for i in range(4)}\n        else:\n            return None\n\n    def is_enough_slidish_like(self, image: ImageClassifier):\n        '''\n        Predicts if the image is likely to be a slide with a small margin of confidence.\n\n        Parameters\n        ----------\n        image : ImageClassifier\n            The image classifier object.\n\n        Returns\n        -------\n        is_slidish : bool\n            True if the image is likely to be a slide, False otherwise.\n        '''\n        probs = self.predict_probability(image)\n        best_slidish_prob = max(probs[0, (1, 2)])\n        best_not_slidish_prob = max(probs[0, (0, 3)])\n        return 1.15 * best_slidish_prob &gt;= best_not_slidish_prob\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/models/xgboost_adapter/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.models.xgboost_adapter.XGBoostModelAdapter.__init__","title":"<code>__init__(model_path)</code>","text":"<p>Initializes the model adapter with the given model path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>The path to the pretrained XGBoost model.</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/models/xgboost_adapter.py</code> <pre><code>def __init__(self, model_path: str) -&gt; None:\n    '''\n    Initializes the model adapter with the given model path.\n\n    Parameters\n    ----------\n    model_path : str\n        The path to the pretrained XGBoost model.\n    '''\n    try:\n        self._model: XGBClassifier = load_model(open(model_path, 'rb'))\n    except:\n        raise FileExistsError(\"cannot find XGBoost model\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/models/xgboost_adapter/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.models.xgboost_adapter.XGBoostModelAdapter.get_label","title":"<code>get_label(prediction)</code>","text":"<p>Gets the label corresponding to the given prediction.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>int or ndarray</code> <p>The prediction index or probability distribution.</p> required <p>Returns:</p> Name Type Description <code>label</code> <code>str or dict</code> <p>The label corresponding to the prediction index or a dictionary of labels with their probabilities.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/models/xgboost_adapter.py</code> <pre><code>def get_label(self, prediction):\n    '''\n    Gets the label corresponding to the given prediction.\n\n    Parameters\n    ----------\n    prediction : int or ndarray\n        The prediction index or probability distribution.\n\n    Returns\n    -------\n    label : str or dict\n        The label corresponding to the prediction index or a dictionary of labels with their probabilities.\n    '''\n    if isinstance(prediction, int):\n        assert 0 &lt;= prediction &lt;= 3\n        return self._labels[prediction]\n    elif isinstance(prediction, ndarray):\n        assert prediction.shape == (1, 4)\n        return {self._labels[i]: prediction[0, i] for i in range(4)}\n    else:\n        return None\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/models/xgboost_adapter/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.models.xgboost_adapter.XGBoostModelAdapter.is_enough_slidish_like","title":"<code>is_enough_slidish_like(image)</code>","text":"<p>Predicts if the image is likely to be a slide with a small margin of confidence.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageClassifier</code> <p>The image classifier object.</p> required <p>Returns:</p> Name Type Description <code>is_slidish</code> <code>bool</code> <p>True if the image is likely to be a slide, False otherwise.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/models/xgboost_adapter.py</code> <pre><code>def is_enough_slidish_like(self, image: ImageClassifier):\n    '''\n    Predicts if the image is likely to be a slide with a small margin of confidence.\n\n    Parameters\n    ----------\n    image : ImageClassifier\n        The image classifier object.\n\n    Returns\n    -------\n    is_slidish : bool\n        True if the image is likely to be a slide, False otherwise.\n    '''\n    probs = self.predict_probability(image)\n    best_slidish_prob = max(probs[0, (1, 2)])\n    best_not_slidish_prob = max(probs[0, (0, 3)])\n    return 1.15 * best_slidish_prob &gt;= best_not_slidish_prob\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/models/xgboost_adapter/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.models.xgboost_adapter.XGBoostModelAdapter.predict_max_confidence","title":"<code>predict_max_confidence(image)</code>","text":"<p>Predicts the class with the highest confidence for the given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageClassifier</code> <p>The image classifier object.</p> required <p>Returns:</p> Name Type Description <code>prediction</code> <code>int</code> <p>The class index with the highest confidence.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/models/xgboost_adapter.py</code> <pre><code>def predict_max_confidence(self, image: ImageClassifier):\n    '''\n    Predicts the class with the highest confidence for the given image.\n\n    Parameters\n    ----------\n    image : ImageClassifier\n        The image classifier object.\n\n    Returns\n    -------\n    prediction : int\n        The class index with the highest confidence.\n    '''\n    return int(argmax(self.predict_probability(self._extract_features_from_image(image))))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/models/xgboost_adapter/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.models.xgboost_adapter.XGBoostModelAdapter.predict_probability","title":"<code>predict_probability(image)</code>","text":"<p>Predicts the probability distribution over classes for the given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageClassifier</code> <p>The image classifier object.</p> required <p>Returns:</p> Name Type Description <code>probs</code> <code>ndarray</code> <p>The probability distribution over classes.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/models/xgboost_adapter.py</code> <pre><code>def predict_probability(self, image: ImageClassifier):\n    '''\n    Predicts the probability distribution over classes for the given image.\n\n    Parameters\n    ----------\n    image : ImageClassifier\n        The image classifier object.\n\n    Returns\n    -------\n    probs : ndarray\n        The probability distribution over classes.\n    '''\n    return self._model.predict_proba(self._extract_features_from_image(image))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/ontology/rdf_graph/","title":"rdf_graph","text":""},{"location":"codebase/EKEELVideoAnnotation/ontology/rdf_graph/#rdf-graph","title":"Rdf Graph","text":"<p>This module provides functionality to convert video annotations into JSON-LD format. It includes the following main functions:</p> <ol> <li> <p>annotations_to_jsonLD(annotations, isAutomatic: bool):</p> <ul> <li>Converts video annotations into a JSON-LD graph.</li> <li>Binds various namespaces to the RDF graph.</li> <li>Adds nodes and triples for video, concepts, and prerequisites.</li> <li>Serializes the RDF graph into JSON-LD format.</li> <li>Compacts the JSON-LD using a provided context.</li> <li>Returns the RDF graph and the compacted JSON-LD data.</li> </ul> </li> <li> <p>graph_to_rdf(jsonld):</p> <ul> <li>Converts JSON-LD data into an RDF graph.</li> </ul> </li> </ol>"},{"location":"codebase/EKEELVideoAnnotation/ontology/rdf_graph/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.ontology.rdf_graph.annotations_to_jsonLD","title":"<code>annotations_to_jsonLD(annotations, isAutomatic)</code>","text":"<p>Converts video annotations into a JSON-LD graph.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>dict</code> <p>A dictionary containing video annotations.</p> required <code>isAutomatic</code> <code>bool</code> <p>A flag indicating whether the annotations are automatic.</p> required <p>Returns:</p> Name Type Description <code>g</code> <code>Graph</code> <p>The RDF graph containing the annotations.</p> <code>data</code> <code>dict</code> <p>The compacted JSON-LD data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; annotations = {\n...     \"id\": \"video1\",\n...     \"definitions\": [{\"concept\": \"example\", \"description_type\": \"type\", \"start\": \"2023-01-01T00:00:00\", \"start_sent_id\": \"1\", \"end\": \"2023-01-01T00:01:00\", \"end_sent_id\": \"2\"}],\n...     \"relations\": [{\"target\": \"example\", \"prerequisite\": \"example2\", \"time\": \"2023-01-01T00:00:00\", \"xywh\": \"None\", \"sent_id\": \"1\", \"word_id\": \"None\"}],\n...     \"annotator\": \"http://example.org/annotator\"\n... }\n&gt;&gt;&gt; g, data = annotations_to_jsonLD(annotations, isAutomatic=False)\n</code></pre> Source code in <code>EVA_apps/EKEELVideoAnnotation/ontology/rdf_graph.py</code> <pre><code>def annotations_to_jsonLD(annotations, isAutomatic:bool):\n    \"\"\"\n    Converts video annotations into a JSON-LD graph.\n\n    Parameters\n    ----------\n    annotations : dict\n        A dictionary containing video annotations.\n    isAutomatic : bool\n        A flag indicating whether the annotations are automatic.\n\n    Returns\n    -------\n    g : rdflib.Graph\n        The RDF graph containing the annotations.\n    data : dict\n        The compacted JSON-LD data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; annotations = {\n    ...     \"id\": \"video1\",\n    ...     \"definitions\": [{\"concept\": \"example\", \"description_type\": \"type\", \"start\": \"2023-01-01T00:00:00\", \"start_sent_id\": \"1\", \"end\": \"2023-01-01T00:01:00\", \"end_sent_id\": \"2\"}],\n    ...     \"relations\": [{\"target\": \"example\", \"prerequisite\": \"example2\", \"time\": \"2023-01-01T00:00:00\", \"xywh\": \"None\", \"sent_id\": \"1\", \"word_id\": \"None\"}],\n    ...     \"annotator\": \"http://example.org/annotator\"\n    ... }\n    &gt;&gt;&gt; g, data = annotations_to_jsonLD(annotations, isAutomatic=False)\n    \"\"\"\n\n    print(\"***** EKEEL - Video Annotation: ontology.py::to_jsonLD(): Inizio ******\")\n\n    concepts_anno = []\n    prereq_anno = []\n\n    if \"definitions\" in annotations:\n        concepts_anno = annotations[\"definitions\"]\n    if \"relations\" in annotations:    \n        prereq_anno = annotations[\"relations\"]\n    video_id = annotations[\"id\"]\n\n    if not isAutomatic:\n        creator = annotations[\"annotator\"]\n        creator = URIRef(creator)\n\n    g = Graph()\n\n    g.bind(\"oa\", oa)\n    g.bind(\"dctypes\", dctypes)\n    g.bind(\"edu\", edu)\n    g.bind(\"SKOS\", SKOS)\n    g.bind(\"dcterms\", dcterms)\n\n\n    # aggiunge un nodo\n    video = URIRef(\"video_\" + str(video_id))\n    #video = URIRef(edurell + \"video_\" + str(video_id))\n    g.add((video, RDF.type, dctypes.MovingImage))\n\n    conll = URIRef(\"conll_\" + str(video_id))\n    #conll = URIRef(edurell + \"conll_\" + str(video_id))\n    g.add((conll, RDF.type, dctypes.Text))\n\n\n    # collegamento video conll\n    ann_linking_conll = URIRef(\"ann0\")\n    #ann_linking_conll = URIRef(edurell + \"ann0\")\n    g.add((ann_linking_conll, RDF.type, oa.Annotation))\n    g.add((ann_linking_conll, oa.motivatedBy, edu.linkingConll))\n    g.add((ann_linking_conll, oa.hasBody, conll))\n    g.add((ann_linking_conll, oa.hasTarget, video))\n\n    date = Literal(datetime.now())\n\n    #creo il nuovo nodo dei concetti\n    #localVocabulary = URIRef(\"localVocabulary\")\n    #g.add((localVocabulary, RDF.type, SKOS.Collection))\n\n\n    # per ogni annotazione di concetto spiegato aggiungo le triple\n    for i, annotation in enumerate(concepts_anno):\n        ann = URIRef(\"ann\" + str(i + 1))\n\n        g.add((ann, RDF.type, oa.Annotation))\n\n        if isAutomatic:\n            g.add((ann, dcterms.creator, URIRef(annotation[\"creator\"])))\n        else:\n            g.add((ann, dcterms.creator, creator))\n\n        g.add((ann, dcterms.created, date))\n        g.add((ann, oa.motivatedBy, oa.describing))\n        g.add((ann, SKOS.note, Literal(\"concept\"+annotation[\"description_type\"])))\n\n\n        concept = URIRef(\"concept_\" + annotation[\"concept\"].replace(\" \", \"_\"))\n\n        #crea un nodo concetto\n\n        #g.add((localVocabulary, SKOS.member, concept))\n        #g.add((concept, RDF.type, SKOS.Concept))\n\n\n        g.add((ann, oa.hasBody, concept))\n\n        blank_target = BNode()\n\n\n        blank_selector = BNode()\n\n        g.add((ann, oa.hasTarget, blank_target))\n        g.add((blank_target, RDF.type, oa.SpecificResource))\n\n        g.add((blank_target, oa.hasSelector, blank_selector))\n        g.add((blank_selector, RDF.type, oa.RangeSelector))\n\n        g.add((blank_target, oa.hasSource, video))\n\n        blank_startSelector = BNode()\n        blank_endSelector = BNode()\n\n        g.add((blank_startSelector, RDF.type, edu.InstantSelector))\n        g.add((blank_endSelector, RDF.type, edu.InstantSelector))\n\n        g.add((blank_selector, oa.hasStartSelector, blank_startSelector))\n        g.add((blank_selector, oa.hasEndSelector, blank_endSelector))\n\n        g.add((blank_startSelector, RDF.value, Literal(annotation[\"start\"] + \"^^xsd:dateTime\")))\n        g.add((blank_startSelector, edu.conllSentId, Literal(annotation[\"start_sent_id\"])))\n        #g.add((blank_startSelector, edu.conllWordId, Literal(annotation[\"word_id\"])))\n\n        g.add((blank_endSelector, RDF.value, Literal(annotation[\"end\"] + \"^^xsd:dateTime\")))\n        g.add((blank_endSelector, edu.conllSentId, Literal(annotation[\"end_sent_id\"])))\n\n\n    num_definitions = len(concepts_anno) + 1\n\n    # per ogni annotazione di prerequisito aggiungo le triple\n    for i, annotation in enumerate(prereq_anno):\n        ann = URIRef(\"ann\" + str(num_definitions + i))\n\n        target_concept = URIRef(\"concept_\" +  annotation[\"target\"].replace(\" \", \"_\"))\n        prereq_concept = URIRef(\"concept_\" +  annotation[\"prerequisite\"].replace(\" \", \"_\"))\n\n        #g.add((target_concept, RDF.type, SKOS.Concept))\n        #g.add((prereq_concept, RDF.type, SKOS.Concept))\n\n        g.add((ann, RDF.type, oa.Annotation))\n\n        if isAutomatic:\n            g.add((ann, dcterms.creator, URIRef(annotation[\"creator\"])))\n        else:\n            g.add((ann, dcterms.creator, creator))\n\n        g.add((ann, dcterms.created, date))\n        g.add((ann, oa.motivatedBy, edu.linkingPrerequisite))\n\n        g.add((ann, oa.hasBody, prereq_concept))\n        g.add((ann, SKOS.note, Literal(annotation[\"weight\"] + \"Prerequisite\")))\n\n        blank_target = BNode()\n\n        g.add((ann, oa.hasTarget, blank_target))\n        g.add((blank_target, RDF.type, oa.SpecificResource))\n        g.add((blank_target, dcterms.subject, target_concept))\n\n        g.add((blank_target, oa.hasSource, video))\n\n        blank_selector_video = BNode()\n\n        g.add((blank_target, oa.hasSelector, blank_selector_video))\n        g.add((blank_selector_video, RDF.type, edu.InstantSelector))\n        g.add((blank_selector_video, RDF.value, Literal(annotation[\"time\"] + \"^^xsd:dateTime\")))\n\n        if annotation[\"xywh\"] != \"None\":\n            g.add((blank_selector_video, edu.hasMediaFrag, Literal(annotation[\"xywh\"])))\n\n\n        g.add((blank_selector_video, edu.conllSentId, Literal(annotation[\"sent_id\"])))\n\n        if annotation[\"word_id\"] != \"None\":\n            g.add((blank_selector_video, edu.conllWordId, Literal(annotation[\"word_id\"])))\n\n    # stampo il grafo in formato turtle\n    # turtle = g.serialize(format='turtle').decode(\"utf-8\")\n    # print(turtle)\n\n    # creo file json-ld\n\n    #jsonld = json.loads(g.serialize(format='json-ld', context=context))\n\n    jsonld = json.loads(g.serialize(format='json-ld'))\n    jsonld = pyld.jsonld.compact(jsonld, context)\n\n    '''\n    Nested nodes creation\n\n    FROM:                            |  TO:\n    {                                |  {\n        ...                          |    ...\n        \"target\": 123                |     \"target\":{\n    },                               |                 \"id\":123,\n    {                                |                  \"type\":example\n        \"id\": 123,                   |               }\n        \"type\": example              |   }\n    }\n\n    '''\n\n    for o in jsonld[\"@graph\"]:\n        if \"target\" in o:\n            for i, t in enumerate(jsonld[\"@graph\"]):\n                if o[\"motivation\"] != \"edu:linkingConll\" and o[\"target\"] == t[\"id\"]:\n                    o[\"target\"] = t\n                    del jsonld[\"@graph\"][i]\n                    for j, s in enumerate(jsonld[\"@graph\"]):\n                        if o[\"target\"][\"selector\"] == s[\"id\"]:\n                            o[\"target\"][\"selector\"] = s\n                            del jsonld[\"@graph\"][j]\n\n                            if o[\"motivation\"] == \"describing\":\n                                for k, p in enumerate(jsonld[\"@graph\"]):\n                                    if o[\"target\"][\"selector\"][\"startSelector\"] == p[\"id\"]:\n                                        o[\"target\"][\"selector\"][\"startSelector\"] = p\n                                        del jsonld[\"@graph\"][k]\n                                        break\n\n                                for k, p in enumerate(jsonld[\"@graph\"]):\n                                    if o[\"target\"][\"selector\"][\"endSelector\"] == p[\"id\"]:\n                                        o[\"target\"][\"selector\"][\"endSelector\"] = p\n                                        del jsonld[\"@graph\"][k]\n                                        break\n\n    #print(jsonld)\n    data = {\n        \"graph\":jsonld\n    }\n\n    #print(data)\n    print(\"***** EKEEL - Video Annotation: ontology.py::to_jsonLD(): Fine ******\")\n\n    return g, data\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/ontology/rdf_graph/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.ontology.rdf_graph.graph_to_rdf","title":"<code>graph_to_rdf(jsonld)</code>","text":"<p>Converts JSON-LD data into an RDF graph.</p> <p>Parameters:</p> Name Type Description Default <code>jsonld</code> <code>dict</code> <p>A dictionary containing JSON-LD data.</p> required <p>Returns:</p> Name Type Description <code>g</code> <code>Graph</code> <p>The RDF graph parsed from the JSON-LD data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; jsonld = {\n...     \"@context\": \"http://schema.org\",\n...     \"@type\": \"Person\",\n...     \"name\": \"Jane Doe\"\n... }\n&gt;&gt;&gt; g = graph_to_rdf(jsonld)\n</code></pre> Source code in <code>EVA_apps/EKEELVideoAnnotation/ontology/rdf_graph.py</code> <pre><code>def graph_to_rdf(jsonld):\n    \"\"\"\n    Converts JSON-LD data into an RDF graph.\n\n    Parameters\n    ----------\n    jsonld : dict\n        A dictionary containing JSON-LD data.\n\n    Returns\n    -------\n    g : rdflib.Graph\n        The RDF graph parsed from the JSON-LD data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; jsonld = {\n    ...     \"@context\": \"http://schema.org\",\n    ...     \"@type\": \"Person\",\n    ...     \"name\": \"Jane Doe\"\n    ... }\n    &gt;&gt;&gt; g = graph_to_rdf(jsonld)\n    \"\"\"\n    json_expanded = pyld.jsonld.expand(jsonld)\n\n    return Graph().parse(data=json.dumps(json_expanded), format='json-ld')\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/services/NLP_API/","title":"NLP_API","text":""},{"location":"codebase/EKEELVideoAnnotation/services/NLP_API/#nlp-api","title":"Nlp Api","text":"<p>This module provides interfaces for interacting with various NLP APIs, including the Italian NLP API and the CoNLL API.</p> <p>Classes:</p> Name Description <code>ItaliaNLAPI</code> <p>Interface for interacting with the Italian NLP API.</p> <code>ConllAPISingleton</code> <p>Singleton class for interacting with the CoNLL API.</p> <p>Attributes:</p> Name Type Description <code>None</code> <p>Functions:</p> Name Description <code>None</code>"},{"location":"codebase/EKEELVideoAnnotation/services/NLP_API/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.services.NLP_API.ConllAPISingleton","title":"<code>ConllAPISingleton</code>","text":"<p>Singleton class for interacting with the CoNLL API.</p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>ConllAPISingleton</code> <p>Singleton instance of the class.</p> <code>_models</code> <code>dict[str, str]</code> <p>Dictionary mapping languages to their best performing models.</p> <p>Methods:</p> Name Description <code>__new__</code> <p>Creates a new instance of the class if one does not already exist.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/services/NLP_API.py</code> <pre><code>class ConllAPISingleton:\n    \"\"\"\n    Singleton class for interacting with the CoNLL API.\n\n    Attributes\n    ----------\n    _instance : ConllAPISingleton\n        Singleton instance of the class.\n    _models : dict[str, str]\n        Dictionary mapping languages to their best performing models.\n\n    Methods\n    -------\n    __new__(cls, *args, **kwargs)\n        Creates a new instance of the class if one does not already exist.\n    \"\"\"\n    _instance = None\n    _models:'dict[str,str]'\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super(ConllAPISingleton, cls).__new__(cls)\n\n            # Get all the models\n            conll_models = sorted(list(requests.post('http://lindat.mff.cuni.cz/services/udpipe/api/models').json()['models'].keys()))\n\n            langs = sorted(list(Locale().get_supported_languages(FORMAT_FULL)))\n            target_langs_models = {lang:[] for lang in langs}\n\n            # Selects only those that are based on supported languages\n            for model_name in conll_models:\n                model_name_lang = model_name.split(\"-\")[0]\n                if model_name_lang &lt; langs[0]:\n                    pass\n                elif any(model_name_lang == lang for lang in langs):\n                    for lang in langs:\n                        if lang == model_name_lang:\n                            target_langs_models[lang].append(model_name)\n                elif model_name_lang &gt; langs[-1]:\n                    break\n\n            # Filters by best performing model and most recent version\n            for lang, models_names in target_langs_models.items():\n                for model_name in models_names:\n                    _,train_kind,_,version,_ = model_name.split('-')\n                    major_version, minor_version = map(int, version.split('.'))\n                    if train_kind in [\"ewt\", \"partut\"] and \\\n                            (major_version &gt; 2 or (major_version == 2 and minor_version &gt;= 12)):\n                        target_langs_models[lang] = model_name\n                        break\n\n            # Maps to pt1 for compliance\n            for lang in langs:\n                target_langs_models[Locale().get_pt1_from_full(lang)] = target_langs_models[lang]\n                target_langs_models.pop(lang)\n            cls._models = target_langs_models\n        return cls._instance\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/services/NLP_API/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.services.NLP_API.ItaliaNLAPI","title":"<code>ItaliaNLAPI</code>","text":"<p>Interface for interacting with the Italian NLP API.</p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>ItaliaNLAPI</code> <p>Singleton instance of the class.</p> <code>_server_address</code> <code>str</code> <p>Address of the Italian NLP API server.</p> <code>_max_term_len</code> <code>int</code> <p>Maximum length of terms.</p> <code>_term_extraction_configs</code> <code>dict</code> <p>Configuration for term extraction.</p> <p>Methods:</p> Name Description <code>upload_document</code> <p>Uploads a document to the server.</p> <code>wait_for_named_entity_tag</code> <p>Waits for named entity tagging to complete.</p> <code>wait_for_pos_tagging</code> <p>Waits for POS tagging to complete.</p> <code>execute_term_extraction</code> <p>Executes term extraction on the document.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/services/NLP_API.py</code> <pre><code>class ItaliaNLAPI:\n    \"\"\"\n    Interface for interacting with the Italian NLP API.\n\n    Attributes\n    ----------\n    _instance : ItaliaNLAPI\n        Singleton instance of the class.\n    _server_address : str\n        Address of the Italian NLP API server.\n    _max_term_len : int\n        Maximum length of terms.\n    _term_extraction_configs : dict\n        Configuration for term extraction.\n\n    Methods\n    -------\n    upload_document(text, language, async_call=True)\n        Uploads a document to the server.\n    wait_for_named_entity_tag(doc_id)\n        Waits for named entity tagging to complete.\n    wait_for_pos_tagging(doc_id)\n        Waits for POS tagging to complete.\n    execute_term_extraction(doc_id, configuration=None, apply_contrast=True, n_try=60) -&gt; DataFrame\n        Executes term extraction on the document.\n    \"\"\"\n\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if cls._instance is None:\n            cls._instance = super(ItaliaNLAPI, cls).__new__(cls)\n            cls._server_address = \"http://api.italianlp.it\"\n            cls._max_term_len = 5\n            cls._term_extraction_configs = {\n                \"name-misc-name\": {\n                    'pos_start_term': ['c:S', 'c:A'],\n                    'pos_internal_term': ['c:E', 'c:A', 'f:S', 'c:B'],\n                    'pos_end_term': ['c:S', 'c:A'],\n                    'max_length_term': 3\n                },\n                \"alzetta-conf\": {\n                    'pos_start_term': ['c:S'],\n                    'pos_internal_term': ['c:A', 'c:E', 'c:S', 'c:EA', 'c:SP'],\n                    'pos_end_term': ['c:A', 'c:S', 'c:SP'],\n                    'statistical_threshold_single': 30,\n                    'statistical_threshold_multi': 10000,\n                    'statistical_frequency_threshold': 1,\n                    'max_length_term': 5,\n                    'apply_contrast': True\n                },\n                \"alzetta-conf-no-contrast\": {\n                    'pos_start_term': ['c:S'],\n                    'pos_internal_term': ['c:A', 'c:E', 'c:S', 'c:EA', 'c:SP'],\n                    'pos_end_term': ['c:A', 'c:S', 'c:SP'],\n                    'statistical_threshold_single': 30,\n                    'statistical_threshold_multi': 10000,\n                    'statistical_frequency_threshold': 1,\n                    'max_length_term': 5,\n                    'apply_contrast': False\n                }\n            }\n        return cls._instance\n\n    def upload_document(self, text: str, language: str, async_call: bool = True):\n        \"\"\"\n        Uploads a document to the server.\n\n        Parameters\n        ----------\n        text : str\n            Text of the document to upload.\n        language : str\n            Language of the document.\n        async_call : bool, optional\n            Whether to make the API call asynchronously, by default True.\n\n        Returns\n        -------\n        str\n            ID of the uploaded document.\n        \"\"\"\n        r = requests.post(self._server_address + '/documents/',\n                          data={'text': text,\n                                'lang': language.upper(),\n                                'async': async_call})\n\n        doc_id = r.json()['id']\n        return doc_id\n\n    def wait_for_named_entity_tag(self, doc_id):\n        \"\"\"\n        Waits for named entity tagging to complete.\n\n        Parameters\n        ----------\n        doc_id : str\n            ID of the document.\n        \"\"\"\n        api_res = {'postagging_executed': False, 'sentences': {'next': False, 'data': []}}\n        while not api_res['postagging_executed'] or api_res['sentences']['next']:\n            r = requests.get(self._server_address + '/documents/action/named_entity/%s' % (doc_id))\n            api_res = r.json()\n\n\n    def wait_for_pos_tagging(self, doc_id):\n        \"\"\"\n        Waits for POS tagging to complete.\n\n        Parameters\n        ----------\n        doc_id : str\n            ID of the document.\n        \"\"\"\n        page = 1\n        api_res = {'postagging_executed': False}\n        while not api_res['postagging_executed']:\n            r = requests.get(self._server_address + '/documents/details/%s?page=%s' % (doc_id, page))\n            api_res = r.json()\n\n            if api_res['postagging_executed']:\n                sentences = api_res[\"sentences\"][\"data\"]\n\n    def execute_term_extraction(self, doc_id, configuration=None, apply_contrast=True, n_try=60) -&gt; DataFrame:\n        \"\"\"\n        Executes term extraction on the document.\n\n        Parameters\n        ----------\n        doc_id : str\n            ID of the document.\n        configuration : dict, optional\n            Configuration for term extraction, by default None.\n        apply_contrast : bool, optional\n            Whether to apply contrast in term extraction, by default True.\n        n_try : int, optional\n            Number of attempts to check for term extraction completion, by default 60.\n\n        Returns\n        -------\n        DataFrame\n            DataFrame containing extracted terms.\n        \"\"\"\n        if configuration is None:\n            configuration = self._term_extraction_configs['alzetta-conf'+\"-no-contrast\"*(not apply_contrast)]\n\n        url = self._server_address + '/documents/term_extraction'\n        term_extraction_id = requests.post(url=url,\n                                 json={'doc_ids': [doc_id],\n                                       'configuration': configuration}).json()['id']\n        for _ in range(n_try):\n            res = requests.get(url=url,params={'id': term_extraction_id}).json()\n            if res['status'] == \"OK\":\n                if len(res[\"terms\"]) == 0:\n                    print(\"With this config ItaliaNLP.term_extraction() has not found anything\")\n                break\n            elif res[\"status\"] == \"IN_PROGRESS\":\n                print(f\"Been waiting term extraction for {(_+1)*10} seconds...\")\n            time.sleep(10)\n        else:\n            raise Exception(f\"ItalianNLP API hasn't sent the requested data in {n_try*5} seconds\")\n\n        terms = DataFrame(res['terms'])\n        if terms.empty:\n            terms = DataFrame(columns=[\"term\", \"domain_relevance\", \"frequency\"])\n        terms['word_count'] = terms['term'].apply(lambda x: len(x.split()))\n        return terms.sort_values(by='word_count').drop(columns=['word_count'])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/services/NLP_API/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.services.NLP_API.ItaliaNLAPI.execute_term_extraction","title":"<code>execute_term_extraction(doc_id, configuration=None, apply_contrast=True, n_try=60)</code>","text":"<p>Executes term extraction on the document.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>ID of the document.</p> required <code>configuration</code> <code>dict</code> <p>Configuration for term extraction, by default None.</p> <code>None</code> <code>apply_contrast</code> <code>bool</code> <p>Whether to apply contrast in term extraction, by default True.</p> <code>True</code> <code>n_try</code> <code>int</code> <p>Number of attempts to check for term extraction completion, by default 60.</p> <code>60</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing extracted terms.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/services/NLP_API.py</code> <pre><code>def execute_term_extraction(self, doc_id, configuration=None, apply_contrast=True, n_try=60) -&gt; DataFrame:\n    \"\"\"\n    Executes term extraction on the document.\n\n    Parameters\n    ----------\n    doc_id : str\n        ID of the document.\n    configuration : dict, optional\n        Configuration for term extraction, by default None.\n    apply_contrast : bool, optional\n        Whether to apply contrast in term extraction, by default True.\n    n_try : int, optional\n        Number of attempts to check for term extraction completion, by default 60.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame containing extracted terms.\n    \"\"\"\n    if configuration is None:\n        configuration = self._term_extraction_configs['alzetta-conf'+\"-no-contrast\"*(not apply_contrast)]\n\n    url = self._server_address + '/documents/term_extraction'\n    term_extraction_id = requests.post(url=url,\n                             json={'doc_ids': [doc_id],\n                                   'configuration': configuration}).json()['id']\n    for _ in range(n_try):\n        res = requests.get(url=url,params={'id': term_extraction_id}).json()\n        if res['status'] == \"OK\":\n            if len(res[\"terms\"]) == 0:\n                print(\"With this config ItaliaNLP.term_extraction() has not found anything\")\n            break\n        elif res[\"status\"] == \"IN_PROGRESS\":\n            print(f\"Been waiting term extraction for {(_+1)*10} seconds...\")\n        time.sleep(10)\n    else:\n        raise Exception(f\"ItalianNLP API hasn't sent the requested data in {n_try*5} seconds\")\n\n    terms = DataFrame(res['terms'])\n    if terms.empty:\n        terms = DataFrame(columns=[\"term\", \"domain_relevance\", \"frequency\"])\n    terms['word_count'] = terms['term'].apply(lambda x: len(x.split()))\n    return terms.sort_values(by='word_count').drop(columns=['word_count'])\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/services/NLP_API/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.services.NLP_API.ItaliaNLAPI.upload_document","title":"<code>upload_document(text, language, async_call=True)</code>","text":"<p>Uploads a document to the server.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text of the document to upload.</p> required <code>language</code> <code>str</code> <p>Language of the document.</p> required <code>async_call</code> <code>bool</code> <p>Whether to make the API call asynchronously, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>ID of the uploaded document.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/services/NLP_API.py</code> <pre><code>def upload_document(self, text: str, language: str, async_call: bool = True):\n    \"\"\"\n    Uploads a document to the server.\n\n    Parameters\n    ----------\n    text : str\n        Text of the document to upload.\n    language : str\n        Language of the document.\n    async_call : bool, optional\n        Whether to make the API call asynchronously, by default True.\n\n    Returns\n    -------\n    str\n        ID of the uploaded document.\n    \"\"\"\n    r = requests.post(self._server_address + '/documents/',\n                      data={'text': text,\n                            'lang': language.upper(),\n                            'async': async_call})\n\n    doc_id = r.json()['id']\n    return doc_id\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/services/NLP_API/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.services.NLP_API.ItaliaNLAPI.wait_for_named_entity_tag","title":"<code>wait_for_named_entity_tag(doc_id)</code>","text":"<p>Waits for named entity tagging to complete.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>ID of the document.</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/services/NLP_API.py</code> <pre><code>def wait_for_named_entity_tag(self, doc_id):\n    \"\"\"\n    Waits for named entity tagging to complete.\n\n    Parameters\n    ----------\n    doc_id : str\n        ID of the document.\n    \"\"\"\n    api_res = {'postagging_executed': False, 'sentences': {'next': False, 'data': []}}\n    while not api_res['postagging_executed'] or api_res['sentences']['next']:\n        r = requests.get(self._server_address + '/documents/action/named_entity/%s' % (doc_id))\n        api_res = r.json()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/services/NLP_API/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.services.NLP_API.ItaliaNLAPI.wait_for_pos_tagging","title":"<code>wait_for_pos_tagging(doc_id)</code>","text":"<p>Waits for POS tagging to complete.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>ID of the document.</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/services/NLP_API.py</code> <pre><code>def wait_for_pos_tagging(self, doc_id):\n    \"\"\"\n    Waits for POS tagging to complete.\n\n    Parameters\n    ----------\n    doc_id : str\n        ID of the document.\n    \"\"\"\n    page = 1\n    api_res = {'postagging_executed': False}\n    while not api_res['postagging_executed']:\n        r = requests.get(self._server_address + '/documents/details/%s?page=%s' % (doc_id, page))\n        api_res = r.json()\n\n        if api_res['postagging_executed']:\n            sentences = api_res[\"sentences\"][\"data\"]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/conll/","title":"conll","text":""},{"location":"codebase/EKEELVideoAnnotation/text_processor/conll/#conll","title":"Conll","text":""},{"location":"codebase/EKEELVideoAnnotation/text_processor/conll/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.conll.conll_gen","title":"<code>conll_gen(video_id, string_text, language)</code>","text":"<p>Generate CoNLL-U format parsing for input text.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <code>string_text</code> <code>str</code> <p>Input text to be parsed</p> required <code>language</code> <code>str</code> <p>Language code for the text</p> required <p>Returns:</p> Type Description <code>list</code> <p>Parsed CoNLL-U sentences</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/conll.py</code> <pre><code>def conll_gen(video_id:str,string_text:str, language:str):\n    \"\"\"\n    Generate CoNLL-U format parsing for input text.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video\n    string_text : str\n        Input text to be parsed\n    language : str\n        Language code for the text\n\n    Returns\n    -------\n    list\n        Parsed CoNLL-U sentences\n    \"\"\"\n    # checks whether the conll is already on server\n    conll = get_conll(video_id)\n\n    if conll is not None:\n        return parse(conll)\n\n    # requests the conll from an api\n    # aggiunto ita\n    #files = {\n    #    'data': text,\n    #    'model': (None, 'english-ewt-ud-2.4-190531'),\n    #    'tokenizer': (None, ''),\n    #    'tagger': (None, ''),\n    #    'parser': (None, ''),\n    #}\n    files = {\n        'data': string_text,\n        'model': (None, CONLL._models[language]),\n        'tokenizer': (None, ''),\n        'tagger': (None, ''),\n        'parser': (None, ''),\n    }\n    r = requests.post('http://lindat.mff.cuni.cz/services/udpipe/api/process', files=files)\n    re = r.json()\n\n    # json da salvare\n    conll = re['result']\n    insert_conll_MongoDB({'video_id':video_id, 'conll':conll})\n    return parse(conll)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/conll/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.conll.get_text","title":"<code>get_text(video_id, return_conll=False)</code>","text":"<p>Retrieve text from stored CoNLL-U format.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <code>return_conll</code> <code>bool</code> <p>If True, returns both text and CoNLL-U format</p> <code>False</code> <p>Returns:</p> Type Description <code>str or tuple</code> <p>Text string if return_conll=False Tuple of (text, conll) if return_conll=True</p> <code>None</code> <p>If no CoNLL data found for video_id</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/conll.py</code> <pre><code>def get_text(video_id:str, return_conll:bool=False):\n    \"\"\"\n    Retrieve text from stored CoNLL-U format.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video\n    return_conll : bool, default=False\n        If True, returns both text and CoNLL-U format\n\n    Returns\n    -------\n    str or tuple\n        Text string if return_conll=False\n        Tuple of (text, conll) if return_conll=True\n    None\n        If no CoNLL data found for video_id\n    \"\"\"\n    conll = get_conll(video_id)\n\n    if conll is None:\n        return None\n\n    parsed = parse(conll)\n    text = \" \".join([sentence.metadata['text'] for sentence in parsed])\n    if return_conll:\n        return text, conll\n    return text\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/conll/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.conll.html_interactable_transcript_legacy","title":"<code>html_interactable_transcript_legacy(subtitles, conll_sentences, language)</code>","text":"<p>Create an interactive HTML transcript with word-level annotations.</p> <p>Parameters:</p> Name Type Description Default <code>subtitles</code> <code>list</code> <p>List of subtitle dictionaries with text and timing</p> required <code>conll_sentences</code> <code>list</code> <p>Parsed CoNLL-U sentences</p> required <code>language</code> <code>str</code> <p>Language code for the text</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>(lemmatized_subtitles, all_lemmas) where: - lemmatized_subtitles: list of dicts with HTML-formatted text - all_lemmas: list of unique lemmas found in text</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/conll.py</code> <pre><code>def html_interactable_transcript_legacy(subtitles:list, conll_sentences:list, language:str):\n    \"\"\"\n    Create an interactive HTML transcript with word-level annotations.\n\n    Parameters\n    ----------\n    subtitles : list\n        List of subtitle dictionaries with text and timing\n    conll_sentences : list\n        Parsed CoNLL-U sentences\n    language : str\n        Language code for the text\n\n    Returns\n    -------\n    tuple\n        (lemmatized_subtitles, all_lemmas) where:\n        - lemmatized_subtitles: list of dicts with HTML-formatted text\n        - all_lemmas: list of unique lemmas found in text\n    \"\"\"\n    from text_processor.words import SemanticText\n    #lemmatizer = WordNetLemmatizer()\n    sem_text = SemanticText(\"\",language)\n\n    sent_id = 0\n    word_id = 0\n    word_counter = 0\n\n    all_lemmas = []\n    lemmatized_subtitles = []\n\n    for sub in subtitles:\n        sent = {\"text\": \"\"}\n        text:str = sub[\"text\"]\n        text = text.replace(\"\\n\", \" \").replace(\"\u2019\", \" \u2019\").replace(\"'\", \" '\")\n        in_phrase_word_indx = 0\n\n\n        # # aggiungo uno spazio vuoto prima e dopo la punteggiatura\n        # text = re.sub('([.,!?():])', r' \\1 ', text)\n        # text = re.sub('\\s{2,}', ' ', text)\n        text = text.replace(\"/\", \" / \")#.replace(\"-\", \" - \")\n        text = text.replace(\"'\", \" '\").replace(\"\u2019\", \" \u2019\").replace(\"\u201d\", \" \u201d \").replace(\"\u201c\", \" \u201c \")\n        if language == \"it\":\n            text = text.replace(\"l '\",\"l' \")\n        #text_words = text.split(\" \")\n        text_words = re.split(' |-', text)\n        #print(text_words)\n        for w in text_words:\n            sentence_finished = False\n            if w != '':\n\n                if w not in [\".\",\":\",\"?\",\"!\",\",\",\";\",\"/\",\"\u201c\",\"'\",'\"',\"\u201d\"]:\n\n                    text_word = w.lower().translate(str.maketrans('', '', string.punctuation))\\\n                        .replace('\u201d', '').replace(\"\u201c\", \"\").replace('\u2026', '')\n\n                    # there is a bug where sent_id value is over the len of conll_senteces\n                    # i add this if to solve it\n                    if sent_id &lt; len(conll_sentences):\n                        conll_words = conll_sentences[sent_id].filter(upos=lambda x: x != \"PUNCT\")\n\n                    for i, c in enumerate(conll_words):\n                        if \"-\" in c[\"form\"]:\n                            conll_words.insert(i, c.copy())\n                            conll_words[i][\"form\"] = conll_words[i][\"form\"].split(\"-\")[0]\n                            conll_words[i+1][\"form\"] = conll_words[i+1][\"form\"].split(\"-\")[1]\n\n                    for i in range(word_counter, len(conll_words)):\n\n                        conll_word = str(conll_words[i][\"form\"]).lower().translate(str.maketrans('', '', string.punctuation))\n                        #print( conll_word, text_word)\n\n                        if text_word == conll_word:\n                            word_id = conll_words[i][\"id\"]\n                            word_counter += 1\n\n                            if conll_words[i][\"id\"] == conll_words[-1][\"id\"]:\n                                sentence_finished = True\n                            break\n\n                if w in [\"!\", \"?\", \".\"]:\n                    s = sent_id\n                else:\n                    s = sent_id + 1\n\n                #print(s)\n                toLemmatize = w.lower()\n                if toLemmatize[-1] in [\"?\", \".\", \"!\", \";\", \",\"]:\n                    toLemmatize = toLemmatize[:-1]\n                sem_text.set_text(toLemmatize)\n                lemma = sem_text.lemmatize()[0]\n                #lemma = lemmatizer.lemmatize(toLemmatize)\n                #print(toLemmatize, lemma)\n                #print(sub)\n\n                sent[\"text\"] += f'&lt;span lemma=\"{lemma}\" sent_id=\"{str(s)}\" word_id=\"{str(word_id)}\" start_time=\"{sub[\"start\"]}\" end_time=\"{sub[\"end\"]}\" &gt;' + w + '&lt;/span&gt; '\n\n\n                if lemma not in all_lemmas:\n                    all_lemmas.append(lemma)\n\n                if sentence_finished:\n                    sent_id += 1\n                    word_id = 0\n                    word_counter = 0\n                in_phrase_word_indx += 1\n\n        lemmatized_subtitles.append(sent)\n\n    return lemmatized_subtitles, all_lemmas\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/conll/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.conll.html_interactable_transcript_word_level","title":"<code>html_interactable_transcript_word_level(sentences)</code>","text":"<p>Create word-level interactive HTML transcript with detailed linguistic annotations.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <code>list</code> <p>List of sentence dictionaries containing word information</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of dictionaries containing HTML-formatted sentences with linguistic annotations (lemma, POS, gender, number) and timing information for each word</p> Notes <p>Special handling implemented for:</p> <ul> <li> <p>Articulated prepositions with apostrophe</p> </li> <li> <p>Verbs with cyclic pronouns</p> </li> <li> <p>Words with punctuation</p> </li> <li> <p>Website URLs</p> </li> </ul> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/conll.py</code> <pre><code>def html_interactable_transcript_word_level(sentences:list):\n    \"\"\"\n    Create word-level interactive HTML transcript with detailed linguistic annotations.\n\n    Parameters\n    ----------\n    sentences : list\n        List of sentence dictionaries containing word information\n\n    Returns\n    -------\n    list\n        List of dictionaries containing HTML-formatted sentences with\n        linguistic annotations (lemma, POS, gender, number) and timing\n        information for each word\n\n    Notes\n    -----\n    Special handling implemented for:\\n\n    - Articulated prepositions with apostrophe\\n\n    - Verbs with cyclic pronouns\\n\n    - Words with punctuation\\n\n    - Website URLs\n    \"\"\"\n    html_lemmatized_sents = []\n    for sent_id, sentence in enumerate(sentences):\n        html_sent = []\n        for word_id, word in enumerate(sentence[\"words\"]):\n            word_text = word[\"word\"]\n            html_sent += [f'&lt;span lemma=\"{word[\"lemma\"]}\"' +\n                                f' sent_id=\"{str(sent_id)}\"' +\n                                f' word_id=\"{str(word_id)}\"' +\n                                f' start_time=\"{word[\"start\"]}\"' +\n                                f' end_time=\"{word[\"end\"]}\"' +\n                                f' cpos=\"{word[\"cpos\"]}\"' +\n                                f' pos=\"{word[\"pos\"]}\"' +\n                                f' gen=\"{word[\"gen\"]}\"' +\n                                f' num=\"{word[\"num\"]}\" &gt;' +\n                                f'{word_text}' +\n                            '&lt;/span&gt;', \" \"]\n            # if articulated preposition with apostrophe eg. \"dell'\" \n            # or verb with after a cyclic pronoun eg. \"specchiar-si\" \n            # or word that has a punctuation mark after\n            # or websites (saved as \"www.google.com\" and words [\"www\",\".google\",\".com\"])\n            # don't add space between\n            if  word[\"word\"].endswith(\"'\") or \\\n                (word[\"cpos\"] == \"PUNCT\" and word[\"word\"] != \",\") or \\\n                word[\"cpos\"] == \"NUM\" or \\\n                (word_id + 1 &lt; len(sentence[\"words\"]) and ( sentence[\"words\"][word_id+1][\"cpos\"] == \"X\" or \\\n                                                            sentence[\"words\"][word_id+1][\"pos\"] in [\"FC\",\"FF\",\"FS\"] or \\\n                                                            sentence[\"words\"][word_id+1][\"word\"].startswith(\".\") or \\\n                                                            sentence[\"words\"][word_id+1][\"word\"].startswith(\"'\") or \\\n                                                            sentence[\"words\"][word_id+1][\"cpos\"] == \"PUNCT\" or \\\n                                                            (word[\"cpos\"] == \"V\"  and sentence[\"words\"][word_id+1][\"pos\"] == \"PC\"))):\n               html_sent.pop()\n        html_lemmatized_sents.append({\"text\": \"\".join(html_sent)})\n    return html_lemmatized_sents\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/locales/","title":"locales","text":""},{"location":"codebase/EKEELVideoAnnotation/text_processor/locales/#locales","title":"Locales","text":""},{"location":"codebase/EKEELVideoAnnotation/text_processor/locales/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.locales.Locale","title":"<code>Locale</code>","text":"<p>Singleton class for managing supported languages and ISO language codes.</p> <p>Provides functionality to handle language codes in both ISO 639-1 (two-letter) and full name formats.</p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>Locale or None</code> <p>Singleton instance of the class</p> <code>_supported_languages</code> <code>set</code> <p>Set of supported ISO 639-1 language codes</p> <p>Methods:</p> Name Description <code>get_supported_languages</code> <p>Returns set of supported languages in specified format</p> <code>is_language_supported</code> <p>Checks if a language is supported</p> <code>get_pt1_from_full</code> <p>Converts full language name to ISO 639-1 code</p> <code>get_full_from_pt1</code> <p>Converts ISO 639-1 code to full language name</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/locales.py</code> <pre><code>class Locale:\n    \"\"\"\n    Singleton class for managing supported languages and ISO language codes.\n\n    Provides functionality to handle language codes in both ISO 639-1 (two-letter)\n    and full name formats.\n\n    Attributes\n    ----------\n    _instance : Locale or None\n        Singleton instance of the class\n    _supported_languages : set\n        Set of supported ISO 639-1 language codes\n\n    Methods\n    -------\n    get_supported_languages(kind=FORMAT_PT1)\n        Returns set of supported languages in specified format\n    is_language_supported(language)\n        Checks if a language is supported\n    get_pt1_from_full(language)\n        Converts full language name to ISO 639-1 code\n    get_full_from_pt1(language, lower=True)\n        Converts ISO 639-1 code to full language name\n    \"\"\"\n\n    _instance = None\n\n    def __new__(cls):\n        \"\"\"\n        Creates or returns the singleton instance of Locale.\n\n        Returns\n        -------\n        Locale\n            The singleton instance\n        \"\"\"\n        if cls._instance is None:\n            cls._instance = super(Locale, cls).__new__(cls)\n            cls._supported_languages = {'it','en'}\n        return cls._instance\n\n    def get_supported_languages(self, kind=FORMAT_PT1):\n        \"\"\"\n        Returns set of supported languages in specified format.\n\n        Parameters\n        ----------\n        kind : int, default=FORMAT_PT1\n            Format type for returned language codes:\n            - FORMAT_PT1: ISO 639-1 two-letter codes\n            - FORMAT_FULL: Full language names\n\n        Returns\n        -------\n        set\n            Set of language codes or names\n\n        Raises\n        ------\n        Exception\n            If kind parameter is not implemented\n        \"\"\"\n        if kind == FORMAT_PT1:\n            return self._supported_languages.copy()\n        elif kind == FORMAT_FULL:\n            return {self.get_full_from_pt1(lang) for lang in self._supported_languages.copy()}\n        raise Exception(\"Locale.get_supported_language() -&gt; kind not implemented\")\n\n    def is_language_supported(self, language:str):\n        \"\"\"\n        Checks if a language is in the supported languages set.\n\n        Parameters\n        ----------\n        language : str\n            Language code or name to check\n\n        Returns\n        -------\n        bool\n            True if language is supported, False otherwise\n        \"\"\"\n        if len(language) &gt; 2:\n            language = Lang(language).pt1\n        return language in self._supported_languages\n\n    @staticmethod\n    def get_pt1_from_full(language:str):\n        \"\"\"\n        Converts full language name to ISO 639-1 code.\n\n        Parameters\n        ----------\n        language : str\n            Full language name\n\n        Returns\n        -------\n        str\n            ISO 639-1 two-letter language code\n        \"\"\"\n        return Lang(name=language.capitalize()).pt1\n\n    @staticmethod\n    def get_full_from_pt1(language:str, lower:bool=True):\n        \"\"\"\n        Converts ISO 639-1 code to full language name.\n\n        Parameters\n        ----------\n        language : str\n            ISO 639-1 two-letter language code\n        lower : bool, default=True\n            If True, returns lowercase name\n\n        Returns\n        -------\n        str\n            Full language name\n        \"\"\"\n        return Lang(pt1=language).name if not lower else Lang(pt1=language).name.lower()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/locales/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.locales.Locale.__new__","title":"<code>__new__()</code>","text":"<p>Creates or returns the singleton instance of Locale.</p> <p>Returns:</p> Type Description <code>Locale</code> <p>The singleton instance</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/locales.py</code> <pre><code>def __new__(cls):\n    \"\"\"\n    Creates or returns the singleton instance of Locale.\n\n    Returns\n    -------\n    Locale\n        The singleton instance\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = super(Locale, cls).__new__(cls)\n        cls._supported_languages = {'it','en'}\n    return cls._instance\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/locales/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.locales.Locale.get_full_from_pt1","title":"<code>get_full_from_pt1(language, lower=True)</code>  <code>staticmethod</code>","text":"<p>Converts ISO 639-1 code to full language name.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>ISO 639-1 two-letter language code</p> required <code>lower</code> <code>bool</code> <p>If True, returns lowercase name</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Full language name</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/locales.py</code> <pre><code>@staticmethod\ndef get_full_from_pt1(language:str, lower:bool=True):\n    \"\"\"\n    Converts ISO 639-1 code to full language name.\n\n    Parameters\n    ----------\n    language : str\n        ISO 639-1 two-letter language code\n    lower : bool, default=True\n        If True, returns lowercase name\n\n    Returns\n    -------\n    str\n        Full language name\n    \"\"\"\n    return Lang(pt1=language).name if not lower else Lang(pt1=language).name.lower()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/locales/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.locales.Locale.get_pt1_from_full","title":"<code>get_pt1_from_full(language)</code>  <code>staticmethod</code>","text":"<p>Converts full language name to ISO 639-1 code.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Full language name</p> required <p>Returns:</p> Type Description <code>str</code> <p>ISO 639-1 two-letter language code</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/locales.py</code> <pre><code>@staticmethod\ndef get_pt1_from_full(language:str):\n    \"\"\"\n    Converts full language name to ISO 639-1 code.\n\n    Parameters\n    ----------\n    language : str\n        Full language name\n\n    Returns\n    -------\n    str\n        ISO 639-1 two-letter language code\n    \"\"\"\n    return Lang(name=language.capitalize()).pt1\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/locales/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.locales.Locale.get_supported_languages","title":"<code>get_supported_languages(kind=FORMAT_PT1)</code>","text":"<p>Returns set of supported languages in specified format.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>int</code> <p>Format type for returned language codes: - FORMAT_PT1: ISO 639-1 two-letter codes - FORMAT_FULL: Full language names</p> <code>FORMAT_PT1</code> <p>Returns:</p> Type Description <code>set</code> <p>Set of language codes or names</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If kind parameter is not implemented</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/locales.py</code> <pre><code>def get_supported_languages(self, kind=FORMAT_PT1):\n    \"\"\"\n    Returns set of supported languages in specified format.\n\n    Parameters\n    ----------\n    kind : int, default=FORMAT_PT1\n        Format type for returned language codes:\n        - FORMAT_PT1: ISO 639-1 two-letter codes\n        - FORMAT_FULL: Full language names\n\n    Returns\n    -------\n    set\n        Set of language codes or names\n\n    Raises\n    ------\n    Exception\n        If kind parameter is not implemented\n    \"\"\"\n    if kind == FORMAT_PT1:\n        return self._supported_languages.copy()\n    elif kind == FORMAT_FULL:\n        return {self.get_full_from_pt1(lang) for lang in self._supported_languages.copy()}\n    raise Exception(\"Locale.get_supported_language() -&gt; kind not implemented\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/locales/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.locales.Locale.is_language_supported","title":"<code>is_language_supported(language)</code>","text":"<p>Checks if a language is in the supported languages set.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Language code or name to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if language is supported, False otherwise</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/locales.py</code> <pre><code>def is_language_supported(self, language:str):\n    \"\"\"\n    Checks if a language is in the supported languages set.\n\n    Parameters\n    ----------\n    language : str\n        Language code or name to check\n\n    Returns\n    -------\n    bool\n        True if language is supported, False otherwise\n    \"\"\"\n    if len(language) &gt; 2:\n        language = Lang(language).pt1\n    return language in self._supported_languages\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/synonyms/","title":"synonyms","text":""},{"location":"codebase/EKEELVideoAnnotation/text_processor/synonyms/#synonyms","title":"Synonyms","text":""},{"location":"codebase/EKEELVideoAnnotation/text_processor/synonyms/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.synonyms.create_skos_dictionary","title":"<code>create_skos_dictionary(synonyms, video_id, mode, language)</code>","text":"<p>Create SKOS dictionary from a dict with synonyms.</p> <p>Creates an RDF graph with SKOS vocabulary structure from a dictionary of words and their synonyms.</p> <p>Parameters:</p> Name Type Description Default <code>synonyms</code> <code>dict</code> <p>Dictionary with words as keys and lists of synonyms as values</p> required <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <code>mode</code> <code>str</code> <p>Mode of operation for the annotator</p> required <code>language</code> <code>str</code> <p>Language code for the labels</p> required <p>Returns:</p> Type Description <code>dict</code> <p>JSON-LD representation of the SKOS dictionary</p> Notes <p>The output follows the W3C Web Annotation Data Model context and includes custom Edurell namespace references</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/synonyms.py</code> <pre><code>def create_skos_dictionary(synonyms, video_id:str, mode:str, language:str):\n    \"\"\"\n    Create SKOS dictionary from a dict with synonyms.\n\n    Creates an RDF graph with SKOS vocabulary structure from a dictionary\n    of words and their synonyms.\n\n    Parameters\n    ----------\n    synonyms : dict\n        Dictionary with words as keys and lists of synonyms as values\n    video_id : str\n        Identifier for the video\n    mode : str\n        Mode of operation for the annotator\n    language : str\n        Language code for the labels\n\n    Returns\n    -------\n    dict\n        JSON-LD representation of the SKOS dictionary\n\n    Notes\n    -----\n    The output follows the W3C Web Annotation Data Model context\n    and includes custom Edurell namespace references\n    \"\"\"\n    print(\"***** EKEEL - Video Annotation: synonyms.py::create_skos_dictionary(): Inizio ******\")\n\n    graph = Graph()\n    skos = Namespace('http://www.w3.org/2004/02/skos/core#')\n    graph.bind('skos', skos)\n    uri_edurell = 'https://teldh.github.io/edurell#'\n\n    for concept in synonyms.keys():\n\n        uri_concept = URIRef(\"concept_\" + concept.replace(\" \", \"_\"))\n        graph.add((uri_concept, RDF['type'], skos['Concept']))\n        graph.add((uri_concept, skos['prefLabel'], Literal(concept, lang=language)))\n        for synonym in synonyms[concept]:\n            graph.add((uri_concept, skos['altLabel'], Literal(synonym, lang=language)))\n\n    # Save graph in file\n    #graph.serialize(destination='output.txt', format='json-ld')\n    #print graph.serialize(format='json-ld').decode('utf-8')\n\n    context = [\"http://www.w3.org/ns/anno.jsonld\", \n        {\"edu\": uri_edurell, \n        \"@base\": \"https://edurell.dibris.unige.it/annotator/\"+mode+\"/\"+video_id+\"/\", \"@version\": 1.1}]        \n\n    jsonld = json.loads(graph.serialize(format='json-ld'))\n    jsonld = pyld.jsonld.compact(jsonld, context)\n\n    if '@graph' not in jsonld:\n        node = {}\n        for key in list(jsonld.keys()):\n            if key != \"@context\":\n                node[key] = jsonld.pop(key)\n        jsonld[\"@graph\"] = [node] if len(node.keys()) &gt; 1 else []\n\n    print(\"***** EKEEL - Video Annotation: synonyms.py::create_skos_dictionary(): Fine ******\")\n\n    return jsonld\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/synonyms/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.synonyms.get_synonyms_from_list","title":"<code>get_synonyms_from_list(concepts)</code>","text":"<p>Find synonyms from a list of words using WordNet NLTK.</p> <p>Parameters:</p> Name Type Description Default <code>concepts</code> <code>list</code> <p>List of words to find synonyms for</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with words as keys and lists of found synonyms as values</p> Notes <p>Words with spaces are converted to underscore format for WordNet lookup and converted back to spaces in the output</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/synonyms.py</code> <pre><code>def get_synonyms_from_list(concepts:list):\n    \"\"\"\n    Find synonyms from a list of words using WordNet NLTK.\n\n    Parameters\n    ----------\n    concepts : list\n        List of words to find synonyms for\n\n    Returns\n    -------\n    dict\n        Dictionary with words as keys and lists of found synonyms as values\n\n    Notes\n    -----\n    Words with spaces are converted to underscore format for WordNet lookup\n    and converted back to spaces in the output\n    \"\"\"\n    synonyms=dict()\n\n    keywords = copy.copy(concepts)\n    converter = lambda x: x.replace(' ', '_')\n    keywords = list(map(converter, keywords))\n\n    synonymsFound={}\n    for starting_keyword in keywords:\n        wordnetSynset1 = wn.synsets(starting_keyword)\n        tempList1=[]\n        synonymsFoundTemp=[]\n        for synset1 in wordnetSynset1:\n            for synWords1 in synset1.lemma_names():\n                if(synWords1.lower() != starting_keyword.lower()):\n                    tempList1.append(synWords1.lower())\n\n        tempList1=list(set(tempList1))        \n\n        for synonym in tempList1:\n            for word in keywords:\n                if (synonym==word):\n                    synonymsFoundTemp.append(word.replace('_',' '))\n        synonymsFoundTemp=list(set(synonymsFoundTemp))\n\n        synonyms[starting_keyword.replace('_',' ')]=synonymsFoundTemp\n\n    return synonyms \n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/","title":"words","text":""},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#words","title":"Words","text":""},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words--natural-language-processing-module","title":"Natural Language Processing Module","text":"<p>Provides text processing capabilities for multiple languages using SpaCy, NLTK, and other NLP libraries.</p> <p>Initialization</p> <ul> <li>Sets tokenizer parallelism to true.</li> <li>Downloads necessary NLTK resources if not already available:<ul> <li>Stopwords</li> <li>Word corpus</li> <li>Punkt tokenizer</li> </ul> </li> <li>Downloads SpaCy models for supported languages (English and Italian).</li> </ul>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.NLPSingleton","title":"<code>NLPSingleton</code>","text":"<p>Multilanguage NLP Singleton for text processing.</p> <p>Maintains Spacy models in memory for efficient processing while allowing SentenceTransformer to be garbage collected after use.</p> <p>Attributes:</p> Name Type Description <code>_instance</code> <code>NLPSingleton</code> <p>Single instance of the class</p> <code>_spacy</code> <code>dict</code> <p>Dictionary of loaded spacy models by language</p> <p>Methods:</p> Name Description <code>lemmatize</code> <p>Lemmatizes input text using specified language model</p> <code>encode_text</code> <p>Encodes text using SentenceTransformer</p> <code>destroy</code> <p>Destroys the singleton instance</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>class NLPSingleton:\n    \"\"\"\n    Multilanguage NLP Singleton for text processing.\n\n    Maintains Spacy models in memory for efficient processing while allowing\n    SentenceTransformer to be garbage collected after use.\n\n    Attributes\n    ----------\n    _instance : NLPSingleton\n        Single instance of the class\n    _spacy : dict\n        Dictionary of loaded spacy models by language\n\n    Methods\n    -------\n    lemmatize(text, lang)\n        Lemmatizes input text using specified language model\n    encode_text(text)\n        Encodes text using SentenceTransformer\n    destroy()\n        Destroys the singleton instance\n    \"\"\"\n\n    _instance = None\n    _spacy: dict = None\n\n    def __new__(cls):\n        \"\"\"\n        Create or return singleton instance.\n\n        Returns\n        -------\n        NLPSingleton\n            The singleton instance\n        \"\"\"\n        if cls._instance is None:\n            cls._instance = super(NLPSingleton, cls).__new__(cls)\n            cls._spacy = {'en': spacy.load('en_core_web_lg'),\n                         'it': spacy.load('it_core_news_lg')}\n        return cls._instance\n\n    def lemmatize(self, text: str, lang: str):\n        \"\"\"\n        Lemmatize input text using specified language model.\n\n        Parameters\n        ----------\n        text : str\n            Text to lemmatize\n        lang : str\n            Language code ('en' or 'it')\n\n        Returns\n        -------\n        spacy.tokens.doc.Doc\n            Processed document with lemmas\n        \"\"\"\n        return NLPSingleton()._spacy[lang](text)\n\n    @staticmethod \n    def encode_text(text: str):\n        \"\"\"\n        Encode text using sentence transformer model.\n\n        Parameters\n        ----------\n        text : str\n            Text to encode\n\n        Returns\n        -------\n        torch.Tensor\n            Encoded text tensor\n        \"\"\"\n        return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2').encode(text, convert_to_tensor=True)\n\n    def destroy(self):\n        \"\"\"\n        Reset the singleton instance.\n        \"\"\"\n        NLPSingleton._instance = None\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.NLPSingleton.__new__","title":"<code>__new__()</code>","text":"<p>Create or return singleton instance.</p> <p>Returns:</p> Type Description <code>NLPSingleton</code> <p>The singleton instance</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def __new__(cls):\n    \"\"\"\n    Create or return singleton instance.\n\n    Returns\n    -------\n    NLPSingleton\n        The singleton instance\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = super(NLPSingleton, cls).__new__(cls)\n        cls._spacy = {'en': spacy.load('en_core_web_lg'),\n                     'it': spacy.load('it_core_news_lg')}\n    return cls._instance\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.NLPSingleton.destroy","title":"<code>destroy()</code>","text":"<p>Reset the singleton instance.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def destroy(self):\n    \"\"\"\n    Reset the singleton instance.\n    \"\"\"\n    NLPSingleton._instance = None\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.NLPSingleton.encode_text","title":"<code>encode_text(text)</code>  <code>staticmethod</code>","text":"<p>Encode text using sentence transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to encode</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded text tensor</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>@staticmethod \ndef encode_text(text: str):\n    \"\"\"\n    Encode text using sentence transformer model.\n\n    Parameters\n    ----------\n    text : str\n        Text to encode\n\n    Returns\n    -------\n    torch.Tensor\n        Encoded text tensor\n    \"\"\"\n    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2').encode(text, convert_to_tensor=True)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.NLPSingleton.lemmatize","title":"<code>lemmatize(text, lang)</code>","text":"<p>Lemmatize input text using specified language model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to lemmatize</p> required <code>lang</code> <code>str</code> <p>Language code ('en' or 'it')</p> required <p>Returns:</p> Type Description <code>Doc</code> <p>Processed document with lemmas</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def lemmatize(self, text: str, lang: str):\n    \"\"\"\n    Lemmatize input text using specified language model.\n\n    Parameters\n    ----------\n    text : str\n        Text to lemmatize\n    lang : str\n        Language code ('en' or 'it')\n\n    Returns\n    -------\n    spacy.tokens.doc.Doc\n        Processed document with lemmas\n    \"\"\"\n    return NLPSingleton()._spacy[lang](text)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText","title":"<code>SemanticText</code>","text":"<p>Text processor with semantic analysis capabilities.</p> <p>Provides methods for text analysis including lemmatization, tokenization, and semantic structure extraction.</p> <p>Attributes:</p> Name Type Description <code>_text</code> <code>str</code> <p>The text to analyze</p> <code>_tokenized_text</code> <code>str or None</code> <p>Cached tokenized version of the text</p> <code>_language</code> <code>str</code> <p>Language code for the text</p> <code>_nlp</code> <code>NLPSingleton</code> <p>NLP processor instance</p> <p>Methods:</p> Name Description <code>set_text</code> <p>Sets or updates the text and optionally language</p> <code>get_text</code> <p>Returns the stored text</p> <code>get_language</code> <p>Returns the language code</p> <code>lemmatize</code> <p>Returns lemmatized tokens from the text</p> <code>tokenize</code> <p>Splits text into tokens</p> <code>get_embeddings</code> <p>Returns text embeddings using transformer model</p> <code>get_semantic_structure_info</code> <p>Extracts detailed linguistic information from text</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>class SemanticText:\n    \"\"\"\n    Text processor with semantic analysis capabilities.\n\n    Provides methods for text analysis including lemmatization,\n    tokenization, and semantic structure extraction.\n\n    Attributes\n    ----------\n    _text : str\n        The text to analyze\n    _tokenized_text : str or None\n        Cached tokenized version of the text\n    _language : str\n        Language code for the text\n    _nlp : NLPSingleton\n        NLP processor instance\n\n    Methods\n    -------\n    set_text(text, language)\n        Sets or updates the text and optionally language\n    get_text()\n        Returns the stored text\n    get_language()\n        Returns the language code\n    lemmatize()\n        Returns lemmatized tokens from the text\n    tokenize()\n        Splits text into tokens\n    get_embeddings()\n        Returns text embeddings using transformer model\n    get_semantic_structure_info()\n        Extracts detailed linguistic information from text\n    \"\"\"\n\n    def __init__(self, text: str, language: str) -&gt; None:\n        \"\"\"\n        Initialize the SemanticText object with text and language.\n\n        Parameters\n        ----------\n        text : str\n            The text to process.\n        language : str\n            The language of the text.\n        \"\"\"\n        assert text is not None and language is not None, 'must set both text and language'\n        self._text = text\n        self._tokenized_text = None\n        self._language = language\n        self._nlp = NLPSingleton()\n\n    def set_text(self, text: str, language: str = None):\n        \"\"\"\n        Set the text and optionally the language.\n\n        Parameters\n        ----------\n        text : str\n            The text to set.\n        language : str, optional\n            The language of the text.\n        \"\"\"\n        self._text = text\n        if language is not None:\n            self._language = language\n        self._tokenized_text = None\n        return self\n\n    def get_text(self):\n        \"\"\"\n        Get the original text.\n\n        Returns\n        -------\n        str\n            The original text.\n        \"\"\"\n        return self._text\n\n    def get_language(self):\n        \"\"\"\n        Get the language of the text.\n\n        Returns\n        -------\n        str\n            The language of the text.\n        \"\"\"\n        return self._language\n\n    def lemmatize_abbreviations(self):\n        \"\"\"\n        Expand English abbreviations in the text.\n        \"\"\"\n        if self._language == Locale.get_pt1_from_full('English'):\n            self._text = self._text.replace(\"'ve\", \" have\").replace(\"'re\", \" are\").replace(\"'s\", \" is\").replace(\"'ll\", \" will\")\n\n    def extract_keywords_from_title(self):\n        \"\"\"\n        Extract keywords from the title.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        return None\n\n    def lemmatize(self):\n        \"\"\"\n        Lemmatize the text.\n\n        Returns\n        -------\n        list\n            List of lemmatized tokens.\n        \"\"\"\n        assert self._text is not None\n        tokens = self._nlp.lemmatize(self._text, self._language)\n        return [token.lemma_ for token in tokens]\n\n    def tokenize(self):\n        \"\"\"\n        Tokenize the text into sentences.\n\n        Returns\n        -------\n        str\n            Tokenized text.\n        \"\"\"\n        if self._tokenized_text is None:\n            self._tokenized_text = sent_tokenize(self._text, Locale.get_full_from_pt1(self._language))\n        return self._tokenized_text\n\n    def get_embeddings(self):\n        \"\"\"\n        Get embeddings for the tokenized text.\n\n        Returns\n        -------\n        torch.Tensor\n            Encoded text tensor.\n        \"\"\"\n        self.tokenize()\n        return NLPSingleton().encode_text(self._tokenized_text)\n\n    def get_semantic_structure_info(self):\n        \"\"\"\n        Get semantic structure information of the text.\n\n        Returns\n        -------\n        dict\n            Dictionary containing semantic structure information.\n        \"\"\"\n        doc = self._nlp.lemmatize(self._text, self._language)\n        term_infos = {\"text\": self._text, \"lemmatization_data\": {\"tokens\": []}}\n        for i, token in enumerate(doc):\n            term_word = {\"word\": token.text,\n                         \"gen\": token.morph.get(\"Gender\")[0] if len(token.morph.get(\"Gender\")) else \"\",\n                         \"num\": token.morph.get(\"Number\")[0] if len(token.morph.get(\"Number\")) else \"\",\n                         \"lemma\": token.lemma_\n                        }\n            term_infos[\"lemmatization_data\"][\"tokens\"].append(term_word)\n            if token.dep_ == \"ROOT\":\n                term_infos[\"lemmatization_data\"][\"head_indx\"] = i\n        return term_infos\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.__init__","title":"<code>__init__(text, language)</code>","text":"<p>Initialize the SemanticText object with text and language.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to process.</p> required <code>language</code> <code>str</code> <p>The language of the text.</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def __init__(self, text: str, language: str) -&gt; None:\n    \"\"\"\n    Initialize the SemanticText object with text and language.\n\n    Parameters\n    ----------\n    text : str\n        The text to process.\n    language : str\n        The language of the text.\n    \"\"\"\n    assert text is not None and language is not None, 'must set both text and language'\n    self._text = text\n    self._tokenized_text = None\n    self._language = language\n    self._nlp = NLPSingleton()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.extract_keywords_from_title","title":"<code>extract_keywords_from_title()</code>","text":"<p>Extract keywords from the title.</p> <p>Returns:</p> Type Description <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def extract_keywords_from_title(self):\n    \"\"\"\n    Extract keywords from the title.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    return None\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.get_embeddings","title":"<code>get_embeddings()</code>","text":"<p>Get embeddings for the tokenized text.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded text tensor.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_embeddings(self):\n    \"\"\"\n    Get embeddings for the tokenized text.\n\n    Returns\n    -------\n    torch.Tensor\n        Encoded text tensor.\n    \"\"\"\n    self.tokenize()\n    return NLPSingleton().encode_text(self._tokenized_text)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.get_language","title":"<code>get_language()</code>","text":"<p>Get the language of the text.</p> <p>Returns:</p> Type Description <code>str</code> <p>The language of the text.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_language(self):\n    \"\"\"\n    Get the language of the text.\n\n    Returns\n    -------\n    str\n        The language of the text.\n    \"\"\"\n    return self._language\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.get_semantic_structure_info","title":"<code>get_semantic_structure_info()</code>","text":"<p>Get semantic structure information of the text.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing semantic structure information.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_semantic_structure_info(self):\n    \"\"\"\n    Get semantic structure information of the text.\n\n    Returns\n    -------\n    dict\n        Dictionary containing semantic structure information.\n    \"\"\"\n    doc = self._nlp.lemmatize(self._text, self._language)\n    term_infos = {\"text\": self._text, \"lemmatization_data\": {\"tokens\": []}}\n    for i, token in enumerate(doc):\n        term_word = {\"word\": token.text,\n                     \"gen\": token.morph.get(\"Gender\")[0] if len(token.morph.get(\"Gender\")) else \"\",\n                     \"num\": token.morph.get(\"Number\")[0] if len(token.morph.get(\"Number\")) else \"\",\n                     \"lemma\": token.lemma_\n                    }\n        term_infos[\"lemmatization_data\"][\"tokens\"].append(term_word)\n        if token.dep_ == \"ROOT\":\n            term_infos[\"lemmatization_data\"][\"head_indx\"] = i\n    return term_infos\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.get_text","title":"<code>get_text()</code>","text":"<p>Get the original text.</p> <p>Returns:</p> Type Description <code>str</code> <p>The original text.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_text(self):\n    \"\"\"\n    Get the original text.\n\n    Returns\n    -------\n    str\n        The original text.\n    \"\"\"\n    return self._text\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.lemmatize","title":"<code>lemmatize()</code>","text":"<p>Lemmatize the text.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of lemmatized tokens.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def lemmatize(self):\n    \"\"\"\n    Lemmatize the text.\n\n    Returns\n    -------\n    list\n        List of lemmatized tokens.\n    \"\"\"\n    assert self._text is not None\n    tokens = self._nlp.lemmatize(self._text, self._language)\n    return [token.lemma_ for token in tokens]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.lemmatize_abbreviations","title":"<code>lemmatize_abbreviations()</code>","text":"<p>Expand English abbreviations in the text.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def lemmatize_abbreviations(self):\n    \"\"\"\n    Expand English abbreviations in the text.\n    \"\"\"\n    if self._language == Locale.get_pt1_from_full('English'):\n        self._text = self._text.replace(\"'ve\", \" have\").replace(\"'re\", \" are\").replace(\"'s\", \" is\").replace(\"'ll\", \" will\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.set_text","title":"<code>set_text(text, language=None)</code>","text":"<p>Set the text and optionally the language.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to set.</p> required <code>language</code> <code>str</code> <p>The language of the text.</p> <code>None</code> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def set_text(self, text: str, language: str = None):\n    \"\"\"\n    Set the text and optionally the language.\n\n    Parameters\n    ----------\n    text : str\n        The text to set.\n    language : str, optional\n        The language of the text.\n    \"\"\"\n    self._text = text\n    if language is not None:\n        self._language = language\n    self._tokenized_text = None\n    return self\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.SemanticText.tokenize","title":"<code>tokenize()</code>","text":"<p>Tokenize the text into sentences.</p> <p>Returns:</p> Type Description <code>str</code> <p>Tokenized text.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def tokenize(self):\n    \"\"\"\n    Tokenize the text into sentences.\n\n    Returns\n    -------\n    str\n        Tokenized text.\n    \"\"\"\n    if self._tokenized_text is None:\n        self._tokenized_text = sent_tokenize(self._text, Locale.get_full_from_pt1(self._language))\n    return self._tokenized_text\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.TextCleaner","title":"<code>TextCleaner</code>","text":"<p>Singleton class for text cleaning operations.</p> <p>Removes special characters and normalizes text format.</p> <p>Attributes:</p> Name Type Description <code>pattern</code> <code>Pattern</code> <p>Compiled regex pattern for text cleaning</p> <code>_instance</code> <code>TextCleaner</code> <p>Single instance of the class</p> <p>Methods:</p> Name Description <code>clean_text</code> <p>Cleans and normalizes input text</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>class TextCleaner:\n    \"\"\"\n    Singleton class for text cleaning operations.\n\n    Removes special characters and normalizes text format.\n\n    Attributes\n    ----------\n    pattern : re.Pattern\n        Compiled regex pattern for text cleaning\n    _instance : TextCleaner\n        Single instance of the class\n\n    Methods\n    -------\n    clean_text(text)\n        Cleans and normalizes input text\n    \"\"\"\n\n    _instance = None\n\n    def __new__(cls):\n        \"\"\"\n        Create a new instance of TextCleaner if it doesn't exist.\n\n        Returns\n        -------\n        TextCleaner\n            Singleton instance of the TextCleaner class.\n        \"\"\"\n        if cls._instance is None:\n            cls.pattern = re.compile('[^a-zA-Z\\d &amp;-]')\n            cls._instance = super(TextCleaner, cls).__new__(cls)\n        return cls._instance\n\n    def clean_text(self, text: str) -&gt; str:\n        \"\"\"\n        Clean the input text by removing unwanted characters and converting to lowercase.\n\n        Parameters\n        ----------\n        text : str\n            Input text to be cleaned.\n\n        Returns\n        -------\n        str\n            Cleaned text.\n        \"\"\"\n        return ' '.join(self.pattern.sub(' ', text).split()).lower()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.TextCleaner.__new__","title":"<code>__new__()</code>","text":"<p>Create a new instance of TextCleaner if it doesn't exist.</p> <p>Returns:</p> Type Description <code>TextCleaner</code> <p>Singleton instance of the TextCleaner class.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def __new__(cls):\n    \"\"\"\n    Create a new instance of TextCleaner if it doesn't exist.\n\n    Returns\n    -------\n    TextCleaner\n        Singleton instance of the TextCleaner class.\n    \"\"\"\n    if cls._instance is None:\n        cls.pattern = re.compile('[^a-zA-Z\\d &amp;-]')\n        cls._instance = super(TextCleaner, cls).__new__(cls)\n    return cls._instance\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.TextCleaner.clean_text","title":"<code>clean_text(text)</code>","text":"<p>Clean the input text by removing unwanted characters and converting to lowercase.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to be cleaned.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Cleaned text.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def clean_text(self, text: str) -&gt; str:\n    \"\"\"\n    Clean the input text by removing unwanted characters and converting to lowercase.\n\n    Parameters\n    ----------\n    text : str\n        Input text to be cleaned.\n\n    Returns\n    -------\n    str\n        Cleaned text.\n    \"\"\"\n    return ' '.join(self.pattern.sub(' ', text).split()).lower()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.TextSimilarityClassifier","title":"<code>TextSimilarityClassifier</code>","text":"<p>Text similarity and difference classifier.</p> <p>Provides multiple methods to compare texts and determine their similarity or if one text is contained within another.</p> <p>Attributes:</p> Name Type Description <code>_CV</code> <code>CountVectorizer</code> <p>Vectorizer for text processing</p> <code>_txt_cleaner</code> <code>TextCleaner</code> <p>Text cleaning utility instance</p> <code>_comp_methods</code> <code>set</code> <p>Set of enabled comparison methods</p> <code>language</code> <code>str</code> <p>Language code for text processing</p> <code>removed_chars_diff_ratio_thresh</code> <code>float</code> <p>Threshold for character removal difference ratio</p> <code>added_chars_diff_ratio_thresh</code> <code>float</code> <p>Threshold for character addition difference ratio</p> <code>common_chars_txt_ratio_thresh</code> <code>float</code> <p>Threshold for common characters ratio</p> <code>time_tol</code> <code>int</code> <p>Time tolerance for comparisons</p> <code>cosine_sim_thresh</code> <code>float</code> <p>Threshold for cosine similarity</p> <code>fuzz_ratio_thresh</code> <code>float</code> <p>Threshold for fuzzy matching ratio</p> <p>Methods:</p> Name Description <code>is_partially_in</code> <p>Determines if one text is part of another</p> <code>are_cosine_similar</code> <p>Checks if texts are similar using cosine similarity</p> <code>is_exactly_in_txt_version</code> <p>Checks if text1 appears exactly in text2</p> <code>subtract_common_text</code> <p>Removes common text between two strings</p> <code>set_comparison_methods</code> <p>Updates the enabled comparison methods</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>class TextSimilarityClassifier:\n    \"\"\"\n    Text similarity and difference classifier.\n\n    Provides multiple methods to compare texts and determine their similarity\n    or if one text is contained within another.\n\n    Attributes\n    ----------\n    _CV : CountVectorizer\n        Vectorizer for text processing\n    _txt_cleaner : TextCleaner\n        Text cleaning utility instance\n    _comp_methods : set\n        Set of enabled comparison methods\n    language : str\n        Language code for text processing\n    removed_chars_diff_ratio_thresh : float\n        Threshold for character removal difference ratio\n    added_chars_diff_ratio_thresh : float\n        Threshold for character addition difference ratio\n    common_chars_txt_ratio_thresh : float\n        Threshold for common characters ratio\n    time_tol : int\n        Time tolerance for comparisons\n    cosine_sim_thresh : float\n        Threshold for cosine similarity\n    fuzz_ratio_thresh : float\n        Threshold for fuzzy matching ratio\n\n    Methods\n    -------\n    is_partially_in(TFT1, TFT2)\n        Determines if one text is part of another\n    are_cosine_similar(text1, text2, confidence)\n        Checks if texts are similar using cosine similarity\n    is_exactly_in_txt_version(text1, text2, chars_tol_percentage)\n        Checks if text1 appears exactly in text2\n    subtract_common_text(text1, text2)\n        Removes common text between two strings\n    set_comparison_methods(methods)\n        Updates the enabled comparison methods\n    \"\"\"\n\n    def __init__(self, comp_methods: List[int] | str | None = None,\n                 max_removed_chars_over_total_diff: float = 0.1,\n                 min_common_chars_ratio: float = 0.8,\n                 max_removed_chars_over_txt: float = 0.3,\n                 max_added_chars_over_total: float = 0.2,\n                 fuzzy_ratio_thresh: float = 0.9,\n                 cosine_sim_chars_distrib_thresh: float = 0.95,\n                 extra_lemmas_ratio_thresh: float = 0.1,\n                 time_tol=5,\n                 language: str = \"en\") -&gt; None:\n        self._CV = CountVectorizer()\n        self._txt_cleaner: TextCleaner = TextCleaner()\n        if comp_methods is None:\n            self._comp_methods = {method for method in list(ComparisonMethods)[:2]}\n        elif comp_methods == \"all\":\n            self._comp_methods = {method for method in list(ComparisonMethods)}\n        else:\n            self.set_comparison_methods(comp_methods)\n        self.removed_chars_diff_ratio_thresh = max_removed_chars_over_total_diff\n        self.added_chars_diff_ratio_thresh = max_added_chars_over_total\n        self.common_chars_txt_ratio_thresh = min_common_chars_ratio\n        self.removed_chars_txt_ratio_thresh = max_removed_chars_over_txt\n        self.time_tol = time_tol\n        self.cosine_sim_thresh = cosine_sim_chars_distrib_thresh\n        self.fuzz_ratio_thresh = fuzzy_ratio_thresh\n        self.extra_lemmas_ratio_thresh = extra_lemmas_ratio_thresh\n        self.language = language\n\n    def is_partially_in(self, TFT1: \"VideoSlide\", TFT2: \"VideoSlide\") -&gt; bool:\n        \"\"\"\n        Check if text1 is partially in text2.\n\n        Parameters\n        ----------\n        TFT1 : VideoSlide\n            First text to compare.\n        TFT2 : VideoSlide\n            Second text to compare.\n\n        Returns\n        -------\n        bool\n            True if text1 is part of text2, False otherwise.\n        \"\"\"\n        #comp_methods = self._comp_methods\n        checks:list[bool] = [bool(TFT1) and bool(TFT2)]\n        text1 = TFT1.get_full_text()\n        text2 = TFT2.get_full_text()\n        checks.append(bool(text1) and bool(text2))\n\n        comp_methods = self._comp_methods\n        removed_chars_count = None\n        cleaner = self._txt_cleaner\n        text1_cleaned = cleaner.clean_text(text1)\n        text2_cleaned = cleaner.clean_text(text2)\n        checks.append(len(text1_cleaned) &lt; len(text2_cleaned))\n\n        if not all(checks):\n            return False\n\n        if ComparisonMethods.FUZZY_PARTIAL_RATIO in comp_methods:\n            fuzz_ratio = partial_ratio(text1_cleaned, text2_cleaned)/100\n            checks.append(fuzz_ratio &gt; self.fuzz_ratio_thresh)\n\n        if not all(checks):\n            return False\n\n        if ComparisonMethods.TXT_SIM_RATIO in comp_methods:\n            counter = Counter([change[0] for change in ndiff(text1_cleaned,text2_cleaned)])\n            removed_chars_count = 0 if not '-' in counter.keys() else counter['-']\n            common_chars_count = 0 if not ' ' in counter.keys() else counter[' ']\n            added_chars_count = 0 if not '+' in counter.keys() else counter['+']\n            diffs_len = removed_chars_count+common_chars_count+added_chars_count\n            text1_len = len(text1_cleaned)\n            checks.append(  diffs_len &gt; 0 and \n                            text1_len &gt; 0 and \n                            removed_chars_count/diffs_len &lt; self.removed_chars_diff_ratio_thresh and \n                            common_chars_count/text1_len &gt; self.common_chars_txt_ratio_thresh  and\n                            added_chars_count/diffs_len &lt; self.added_chars_diff_ratio_thresh)\n\n        if not all(checks):\n            return False\n\n        if ComparisonMethods.MEANINGFUL_WORDS_COUNT in comp_methods:\n            all_words = self._words\n            txt1_split = text1_cleaned.split(); txt2_split = text2_cleaned.split()\n            len_txt1_split = len(txt1_split); len_txt2_split = len(txt2_split) \n            checks.append(( 0 &lt; len_txt1_split &lt;= len_txt2_split \n                                and ( len([word for word in txt1_split if word in all_words]) / len_txt1_split \n                                        &lt;= \n                                      len([word for word in txt2_split if word in all_words]) / len_txt2_split) ) \n                            or len_txt1_split &lt;= len_txt2_split )\n\n        if not all(checks):\n            return False\n\n        if ComparisonMethods.TXT_MISS_RATIO in comp_methods:\n            if removed_chars_count is None:\n                text1_cleaned = cleaner.clean_text(text1); text2_cleaned = cleaner.clean_text(text2)\n                counter = Counter([change[0] for change in ndiff(text1_cleaned,text2_cleaned)])\n                removed_chars_count = 0 if not '-' in counter.keys() else counter['-']\n                added_chars_count = 0 if not '+' in counter.keys() else counter['+']\n                text1_len = len(text1_cleaned)\n            checks.append(  text1_len &gt; 0 and \n                            removed_chars_count/len(text1_cleaned) &lt; self.removed_chars_txt_ratio_thresh)\n\n        if not all(checks):\n            return False\n\n        if ComparisonMethods.CHARS_COMMON_DISTRIB in comp_methods:\n            counts1 = Counter(text1_cleaned); counts2 = Counter(text2_cleaned)\n            counts1.pop(\" \",0); counts2.pop(\" \",0)\n            for key in set(counts1.keys()).union(set(counts2.keys())):\n                if key not in counts1:\n                    counts1[key] = 0\n                if key not in counts2:\n                    counts2[key] = 0\n\n            cosine_sim = cosine_similarity([np.array(list(counts1[key] for key in sorted(counts1)))], \n                                           [np.array(list(counts2[key] for key in sorted(counts2)))])[0][0]\n\n            checks.append(cosine_sim &gt; self.cosine_sim_thresh or (ComparisonMethods.FUZZY_PARTIAL_RATIO in comp_methods and fuzz_ratio &gt; 0.95))\n\n        if not all(checks):\n            return False\n\n        if ComparisonMethods.LEMMAS_CONTAINED_RATIO in comp_methods:\n            lang = self.language\n            lemmas1:list = NLPSingleton().lemmatize(text1_cleaned,lang)\n            lemmas2:list = NLPSingleton().lemmatize(text2_cleaned,lang)\n            lemmas_diff = [lemma for lemma in lemmas2 if not lemma in lemmas1 or lemmas1.remove(lemma)]\n            checks.append(len(lemmas_diff)/len(lemmas1) &lt;= self.extra_lemmas_ratio_thresh)\n\n        return all(checks)\n\n\n    def are_cosine_similar(self,text1:str,text2:str,confidence:float=0.9) -&gt; bool:\n        '''\n        Determine if two texts are cosine similar.\n\n        This is evaluated in terms of words mapped to a unique number\\n\n\n        Warning\n        ----------\n\n        May collapse when performed on texts with num words = 1 vs 2 or 2 vs 1\n\n        Parameters\n        -----------\n            text1 (str) : The first text to compare.\\n\n            text2 (str) : The second text to compare.\\n\n            confidence (float, optional) : The minimum confidence level required to consider\n                the texts similar. Defaults to 0.9\\n\n\n        Returns\n        ----------\n            bool \n                True if the texts are cosine similar with a confidence level above `confidence`, False otherwise.  \n\n        '''\n        cleaner = self._txt_cleaner\n        text1_clean_split, text2_clean_split = cleaner.clean_text(text1).split(), cleaner.clean_text(text2).split()\n        len_split1, len_split2 = len(text1_clean_split), len(text2_clean_split)\n        max_len = max(len_split1,len_split2)\n        if max_len &gt; 0 and min(len_split1, len_split2) == 0:\n            return False\n        words_set = set(text1_clean_split+text2_clean_split)\n        values = list(range(1,len(words_set)+1))\n        words_dict = dict(zip(words_set,values))\n        text1_vectorized = list(map(lambda key: words_dict[key], text1_clean_split))\n        text2_vectorized = list(map(lambda key: words_dict[key], text2_clean_split))\n        texts_vectorized = empty((2,max_len),dtype=int)\n        texts_vectorized[0,:len_split1] = text1_vectorized; texts_vectorized[0,len_split1:] = 0\n        texts_vectorized[1,:len_split2] = text2_vectorized; texts_vectorized[1,len_split2:] = 0\n        return sum(prod(texts_vectorized,axis=0))/prod(norm(texts_vectorized,axis=1)) &gt; confidence\n\n\n    def is_exactly_in_txt_version(self,text1:str,text2:str,chars_tol_percentage:float=0.9):\n        '''\n        Checks whether there's the same string text1 in text2, within a reasonable margin of error\\n\n        Based on perfect match from left to right so if text1 has char in second position different\\n\n        from what is in text2, the match is insignificant\\n\\n\n\n        TODO can be improved with match in reversed string but might get expensive\n        '''\n        cleaner = self._txt_cleaner\n        clean_text1 = cleaner.clean_text(text1)\n        res = re.search(clean_text1,cleaner.clean_text(text2))\n        return res is not None and len(res.group(0))/len(clean_text1) &gt; chars_tol_percentage\n\n    def subtract_common_text(self,text1:str,text2:str):\n        '''\n        Performs text1 - text2 where they are in common\n\n        Can be improved by using ndiff alg but for now keep it simple\n        '''\n        return self._txt_cleaner.clean_text(text1.replace(text2,'').lstrip())\n\n    def set_comparison_methods(self,methods:List[int]):\n        self._comp_methods = set(methods)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.TextSimilarityClassifier.are_cosine_similar","title":"<code>are_cosine_similar(text1, text2, confidence=0.9)</code>","text":"<p>Determine if two texts are cosine similar.</p> <p>This is evaluated in terms of words mapped to a unique number</p> Warning <p>May collapse when performed on texts with num words = 1 vs 2 or 2 vs 1</p> <p>Returns:</p> Type Description <code>    bool </code> <p>True if the texts are cosine similar with a confidence level above <code>confidence</code>, False otherwise.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def are_cosine_similar(self,text1:str,text2:str,confidence:float=0.9) -&gt; bool:\n    '''\n    Determine if two texts are cosine similar.\n\n    This is evaluated in terms of words mapped to a unique number\\n\n\n    Warning\n    ----------\n\n    May collapse when performed on texts with num words = 1 vs 2 or 2 vs 1\n\n    Parameters\n    -----------\n        text1 (str) : The first text to compare.\\n\n        text2 (str) : The second text to compare.\\n\n        confidence (float, optional) : The minimum confidence level required to consider\n            the texts similar. Defaults to 0.9\\n\n\n    Returns\n    ----------\n        bool \n            True if the texts are cosine similar with a confidence level above `confidence`, False otherwise.  \n\n    '''\n    cleaner = self._txt_cleaner\n    text1_clean_split, text2_clean_split = cleaner.clean_text(text1).split(), cleaner.clean_text(text2).split()\n    len_split1, len_split2 = len(text1_clean_split), len(text2_clean_split)\n    max_len = max(len_split1,len_split2)\n    if max_len &gt; 0 and min(len_split1, len_split2) == 0:\n        return False\n    words_set = set(text1_clean_split+text2_clean_split)\n    values = list(range(1,len(words_set)+1))\n    words_dict = dict(zip(words_set,values))\n    text1_vectorized = list(map(lambda key: words_dict[key], text1_clean_split))\n    text2_vectorized = list(map(lambda key: words_dict[key], text2_clean_split))\n    texts_vectorized = empty((2,max_len),dtype=int)\n    texts_vectorized[0,:len_split1] = text1_vectorized; texts_vectorized[0,len_split1:] = 0\n    texts_vectorized[1,:len_split2] = text2_vectorized; texts_vectorized[1,len_split2:] = 0\n    return sum(prod(texts_vectorized,axis=0))/prod(norm(texts_vectorized,axis=1)) &gt; confidence\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.TextSimilarityClassifier.is_exactly_in_txt_version","title":"<code>is_exactly_in_txt_version(text1, text2, chars_tol_percentage=0.9)</code>","text":"<p>Checks whether there's the same string text1 in text2, within a reasonable margin of error</p> <p>Based on perfect match from left to right so if text1 has char in second position different</p> <p>from what is in text2, the match is insignificant</p> <p>TODO can be improved with match in reversed string but might get expensive</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def is_exactly_in_txt_version(self,text1:str,text2:str,chars_tol_percentage:float=0.9):\n    '''\n    Checks whether there's the same string text1 in text2, within a reasonable margin of error\\n\n    Based on perfect match from left to right so if text1 has char in second position different\\n\n    from what is in text2, the match is insignificant\\n\\n\n\n    TODO can be improved with match in reversed string but might get expensive\n    '''\n    cleaner = self._txt_cleaner\n    clean_text1 = cleaner.clean_text(text1)\n    res = re.search(clean_text1,cleaner.clean_text(text2))\n    return res is not None and len(res.group(0))/len(clean_text1) &gt; chars_tol_percentage\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.TextSimilarityClassifier.is_partially_in","title":"<code>is_partially_in(TFT1, TFT2)</code>","text":"<p>Check if text1 is partially in text2.</p> <p>Parameters:</p> Name Type Description Default <code>TFT1</code> <code>VideoSlide</code> <p>First text to compare.</p> required <code>TFT2</code> <code>VideoSlide</code> <p>Second text to compare.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if text1 is part of text2, False otherwise.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def is_partially_in(self, TFT1: \"VideoSlide\", TFT2: \"VideoSlide\") -&gt; bool:\n    \"\"\"\n    Check if text1 is partially in text2.\n\n    Parameters\n    ----------\n    TFT1 : VideoSlide\n        First text to compare.\n    TFT2 : VideoSlide\n        Second text to compare.\n\n    Returns\n    -------\n    bool\n        True if text1 is part of text2, False otherwise.\n    \"\"\"\n    #comp_methods = self._comp_methods\n    checks:list[bool] = [bool(TFT1) and bool(TFT2)]\n    text1 = TFT1.get_full_text()\n    text2 = TFT2.get_full_text()\n    checks.append(bool(text1) and bool(text2))\n\n    comp_methods = self._comp_methods\n    removed_chars_count = None\n    cleaner = self._txt_cleaner\n    text1_cleaned = cleaner.clean_text(text1)\n    text2_cleaned = cleaner.clean_text(text2)\n    checks.append(len(text1_cleaned) &lt; len(text2_cleaned))\n\n    if not all(checks):\n        return False\n\n    if ComparisonMethods.FUZZY_PARTIAL_RATIO in comp_methods:\n        fuzz_ratio = partial_ratio(text1_cleaned, text2_cleaned)/100\n        checks.append(fuzz_ratio &gt; self.fuzz_ratio_thresh)\n\n    if not all(checks):\n        return False\n\n    if ComparisonMethods.TXT_SIM_RATIO in comp_methods:\n        counter = Counter([change[0] for change in ndiff(text1_cleaned,text2_cleaned)])\n        removed_chars_count = 0 if not '-' in counter.keys() else counter['-']\n        common_chars_count = 0 if not ' ' in counter.keys() else counter[' ']\n        added_chars_count = 0 if not '+' in counter.keys() else counter['+']\n        diffs_len = removed_chars_count+common_chars_count+added_chars_count\n        text1_len = len(text1_cleaned)\n        checks.append(  diffs_len &gt; 0 and \n                        text1_len &gt; 0 and \n                        removed_chars_count/diffs_len &lt; self.removed_chars_diff_ratio_thresh and \n                        common_chars_count/text1_len &gt; self.common_chars_txt_ratio_thresh  and\n                        added_chars_count/diffs_len &lt; self.added_chars_diff_ratio_thresh)\n\n    if not all(checks):\n        return False\n\n    if ComparisonMethods.MEANINGFUL_WORDS_COUNT in comp_methods:\n        all_words = self._words\n        txt1_split = text1_cleaned.split(); txt2_split = text2_cleaned.split()\n        len_txt1_split = len(txt1_split); len_txt2_split = len(txt2_split) \n        checks.append(( 0 &lt; len_txt1_split &lt;= len_txt2_split \n                            and ( len([word for word in txt1_split if word in all_words]) / len_txt1_split \n                                    &lt;= \n                                  len([word for word in txt2_split if word in all_words]) / len_txt2_split) ) \n                        or len_txt1_split &lt;= len_txt2_split )\n\n    if not all(checks):\n        return False\n\n    if ComparisonMethods.TXT_MISS_RATIO in comp_methods:\n        if removed_chars_count is None:\n            text1_cleaned = cleaner.clean_text(text1); text2_cleaned = cleaner.clean_text(text2)\n            counter = Counter([change[0] for change in ndiff(text1_cleaned,text2_cleaned)])\n            removed_chars_count = 0 if not '-' in counter.keys() else counter['-']\n            added_chars_count = 0 if not '+' in counter.keys() else counter['+']\n            text1_len = len(text1_cleaned)\n        checks.append(  text1_len &gt; 0 and \n                        removed_chars_count/len(text1_cleaned) &lt; self.removed_chars_txt_ratio_thresh)\n\n    if not all(checks):\n        return False\n\n    if ComparisonMethods.CHARS_COMMON_DISTRIB in comp_methods:\n        counts1 = Counter(text1_cleaned); counts2 = Counter(text2_cleaned)\n        counts1.pop(\" \",0); counts2.pop(\" \",0)\n        for key in set(counts1.keys()).union(set(counts2.keys())):\n            if key not in counts1:\n                counts1[key] = 0\n            if key not in counts2:\n                counts2[key] = 0\n\n        cosine_sim = cosine_similarity([np.array(list(counts1[key] for key in sorted(counts1)))], \n                                       [np.array(list(counts2[key] for key in sorted(counts2)))])[0][0]\n\n        checks.append(cosine_sim &gt; self.cosine_sim_thresh or (ComparisonMethods.FUZZY_PARTIAL_RATIO in comp_methods and fuzz_ratio &gt; 0.95))\n\n    if not all(checks):\n        return False\n\n    if ComparisonMethods.LEMMAS_CONTAINED_RATIO in comp_methods:\n        lang = self.language\n        lemmas1:list = NLPSingleton().lemmatize(text1_cleaned,lang)\n        lemmas2:list = NLPSingleton().lemmatize(text2_cleaned,lang)\n        lemmas_diff = [lemma for lemma in lemmas2 if not lemma in lemmas1 or lemmas1.remove(lemma)]\n        checks.append(len(lemmas_diff)/len(lemmas1) &lt;= self.extra_lemmas_ratio_thresh)\n\n    return all(checks)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.TextSimilarityClassifier.subtract_common_text","title":"<code>subtract_common_text(text1, text2)</code>","text":"<p>Performs text1 - text2 where they are in common</p> <p>Can be improved by using ndiff alg but for now keep it simple</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def subtract_common_text(self,text1:str,text2:str):\n    '''\n    Performs text1 - text2 where they are in common\n\n    Can be improved by using ndiff alg but for now keep it simple\n    '''\n    return self._txt_cleaner.clean_text(text1.replace(text2,'').lstrip())\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide","title":"<code>VideoSlide</code>","text":"<p>Representation of a text slide in a video.</p> <p>Manages text content with frame timing and screen location information.</p> <p>Attributes:</p> Name Type Description <code>_full_text</code> <code>str</code> <p>Complete text content of the slide.</p> <code>_framed_sentences</code> <code>List[Tuple[Tuple[int, int], Tuple[int, int, int, int]]]</code> <p>List of text segments with their screen locations.</p> <code>_bounding_box</code> <code>Tuple[int, int, int, int]</code> <p>Overall bounding box coordinates.</p> <code>start_end_frames</code> <code>List[Tuple[int, int]]</code> <p>List of frame number ranges where slide appears.</p> <code>txt_sim_class</code> <code>TextSimilarityClassifier</code> <p>Text similarity analyzer instance.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes the VideoSlide with framed sentences and start-end frames.</p> <code>copy</code> <p>Creates a copy of the slide.</p> <code>merge_frames</code> <p>Combines frame ranges with another slide.</p> <code>get_full_text</code> <p>Returns complete slide text.</p> <code>get_split_text</code> <p>Returns text split by newlines.</p> <code>get_framed_sentences</code> <p>Returns text segments with screen locations.</p> <code>merge_adjacent_startend_frames</code> <p>Merges nearby frame ranges.</p> <code>__iter__</code> <p>Iterates over the slide attributes.</p> <code>__eq__</code> <p>Checks if two slides are equal based on text similarity and bounding boxes.</p> <code>__lt__</code> <p>Compares two slides based on their start frame.</p> <code>__repr__</code> <p>Returns a string representation of the slide.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>class VideoSlide:\n    \"\"\"\n    Representation of a text slide in a video.\n\n    Manages text content with frame timing and screen location information.\n\n    Attributes\n    ----------\n    _full_text : str\n        Complete text content of the slide.\n    _framed_sentences : List[Tuple[Tuple[int, int], Tuple[int, int, int, int]]]\n        List of text segments with their screen locations.\n    _bounding_box : Tuple[int, int, int, int]\n        Overall bounding box coordinates.\n    start_end_frames : List[Tuple[int, int]]\n        List of frame number ranges where slide appears.\n    txt_sim_class : TextSimilarityClassifier\n        Text similarity analyzer instance.\n\n    Methods\n    -------\n    __init__(framed_sentences, startend_frames)\n        Initializes the VideoSlide with framed sentences and start-end frames.\n    copy()\n        Creates a copy of the slide.\n    merge_frames(other_slide)\n        Combines frame ranges with another slide.\n    get_full_text()\n        Returns complete slide text.\n    get_split_text()\n        Returns text split by newlines.\n    get_framed_sentences()\n        Returns text segments with screen locations.\n    merge_adjacent_startend_frames(max_dist)\n        Merges nearby frame ranges.\n    __iter__()\n        Iterates over the slide attributes.\n    __eq__(other)\n        Checks if two slides are equal based on text similarity and bounding boxes.\n    __lt__(other)\n        Compares two slides based on their start frame.\n    __repr__()\n        Returns a string representation of the slide.\n    \"\"\"\n    _full_text: str\n    _framed_sentences: List[Tuple[Tuple[int, int], Tuple[int, int, int, int]]]\n    _bounding_box: Tuple[int, int, int, int]\n    start_end_frames: List[Tuple[int, int]]\n    txt_sim_class = TextSimilarityClassifier()\n\n    def __init__(self, framed_sentences: List[Tuple[str, Tuple[int, int, int, int]]], startend_frames: List[Tuple[int, int]]) -&gt; None:\n        self.start_end_frames = [startend_frames]\n        full_text = ''\n        converted_framed_sentence: List[Tuple[Tuple[int, int], Tuple[int, int, int, int]]] = []\n        curr_start = 0\n        min_bb = [0, 0, 0, 0]\n        for sentence, bb in framed_sentences:\n            full_text += sentence\n            len_sent = len(sentence)\n            converted_framed_sentence.append(((curr_start, curr_start + len_sent), bb))\n            curr_start += len_sent\n            min_bb[0] = np.minimum(min_bb[0], bb[0])\n            min_bb[1] = np.minimum(min_bb[1], bb[1])\n            min_bb[2] = np.maximum(min_bb[2], bb[2])\n            min_bb[3] = np.maximum(min_bb[3], bb[3])\n\n        self._bounding_box = np.array(min_bb)\n        self._framed_sentences = converted_framed_sentence\n        self._full_text = full_text\n\n    def copy(self):\n        \"\"\"\n        Creates a copy of the slide.\n\n        Returns\n        -------\n        VideoSlide\n            A copy of the current slide.\n        \"\"\"\n        tft_copy = VideoSlide(framed_sentences=None, start_end_frames=self.start_end_frames)\n        tft_copy._framed_sentences = self._framed_sentences\n        tft_copy._full_text = self._full_text\n        return tft_copy\n\n    def merge_frames(self, other_slide: \"VideoSlide\") -&gt; None:\n        \"\"\"\n        Combines frame ranges with another slide.\n\n        Parameters\n        ----------\n        other_slide : VideoSlide\n            Another slide to merge frames with.\n        \"\"\"\n        this_startend_frames = self.start_end_frames\n        for other_start_end_elem in other_slide.start_end_frames:\n            if not other_start_end_elem in this_startend_frames:\n                insort_left(self.start_end_frames, other_start_end_elem)\n\n    def get_full_text(self):\n        \"\"\"\n        Returns complete slide text.\n\n        Returns\n        -------\n        str\n            Complete text content of the slide.\n        \"\"\"\n        return self._full_text\n\n    def get_split_text(self):\n        \"\"\"\n        Returns text split by newlines.\n\n        Returns\n        -------\n        List[str]\n            Text content split by newlines.\n        \"\"\"\n        return self._full_text.split(\"(?&lt;=\\n)\")\n\n    def get_framed_sentences(self):\n        \"\"\"\n        Returns text segments with screen locations.\n\n        Returns\n        -------\n        List[Tuple[str, Tuple[int, int, int, int]]]\n            List of text segments with their screen locations.\n        \"\"\"\n        full_text = self._full_text\n        return [((full_text[start_char_pos:end_char_pos]), bb) for (start_char_pos, end_char_pos), bb in self._framed_sentences]\n\n    def merge_adjacent_startend_frames(self, max_dist: int = 15) -&gt; 'VideoSlide':\n        \"\"\"\n        Merges nearby frame ranges.\n\n        Parameters\n        ----------\n        max_dist : int, optional\n            Maximum distance between frame ranges to merge, by default 15.\n\n        Returns\n        -------\n        VideoSlide\n            The slide with merged frame ranges.\n        \"\"\"\n        start_end_frames = self.start_end_frames\n        merged_start_end_frames = []\n        curr_start, curr_end = start_end_frames[0]\n        for new_start, new_end in start_end_frames:\n            if new_start - curr_end &lt;= max_dist:\n                curr_end = max(curr_end, new_end)\n            else:\n                merged_start_end_frames.append((curr_start, curr_end))\n                curr_start = new_start\n                curr_end = new_end\n        merged_start_end_frames.append((curr_start, curr_end))\n        self.start_end_frames = merged_start_end_frames\n        return self\n\n    def __iter__(self):\n        \"\"\"\n        Iterates over the slide attributes.\n\n        Yields\n        ------\n        Tuple[str, Any]\n            Attribute name and value.\n        \"\"\"\n        yield \"text\", self._full_text\n        yield \"full_bounding_box\", self._bounding_box.tolist()\n        yield \"sent_indxs_and_bb\", self._framed_sentences\n\n    def __eq__(self, other: \"VideoSlide\") -&gt; bool:\n        \"\"\"\n        Checks if two slides are equal based on text similarity and bounding boxes.\n\n        Parameters\n        ----------\n        other : VideoSlide\n            Another slide to compare with.\n\n        Returns\n        -------\n        bool\n            True if slides are equal, False otherwise.\n        \"\"\"\n        if not self.txt_sim_class.are_cosine_similar(self._full_text, other._full_text, confidence=0.8):\n            return False\n\n        this_bbs = self._bounding_box\n        other_bbs = other._bounding_box\n        return (this_bbs - 10 &lt;= other_bbs).all() and (other_bbs &lt;= this_bbs + 10).all()\n\n    def __lt__(self, other: 'VideoSlide'):\n        \"\"\"\n        Compares two slides based on their start frame.\n\n        Parameters\n        ----------\n        other : VideoSlide\n            Another slide to compare with.\n\n        Returns\n        -------\n        bool\n            True if this slide starts before the other slide, False otherwise.\n        \"\"\"\n        return self.start_end_frames[0][0] &lt; other.start_end_frames[0][0]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Returns a string representation of the slide.\n\n        Returns\n        -------\n        str\n            String representation of the slide.\n        \"\"\"\n        return 'TFT(txt={0}, window_time={1}, bbs={2})'.format(\n            repr(self._full_text), repr(self.start_end_frames), repr(self._bounding_box))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Checks if two slides are equal based on text similarity and bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>VideoSlide</code> <p>Another slide to compare with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if slides are equal, False otherwise.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def __eq__(self, other: \"VideoSlide\") -&gt; bool:\n    \"\"\"\n    Checks if two slides are equal based on text similarity and bounding boxes.\n\n    Parameters\n    ----------\n    other : VideoSlide\n        Another slide to compare with.\n\n    Returns\n    -------\n    bool\n        True if slides are equal, False otherwise.\n    \"\"\"\n    if not self.txt_sim_class.are_cosine_similar(self._full_text, other._full_text, confidence=0.8):\n        return False\n\n    this_bbs = self._bounding_box\n    other_bbs = other._bounding_box\n    return (this_bbs - 10 &lt;= other_bbs).all() and (other_bbs &lt;= this_bbs + 10).all()\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the slide attributes.</p> <p>Yields:</p> Type Description <code>Tuple[str, Any]</code> <p>Attribute name and value.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterates over the slide attributes.\n\n    Yields\n    ------\n    Tuple[str, Any]\n        Attribute name and value.\n    \"\"\"\n    yield \"text\", self._full_text\n    yield \"full_bounding_box\", self._bounding_box.tolist()\n    yield \"sent_indxs_and_bb\", self._framed_sentences\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Compares two slides based on their start frame.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>VideoSlide</code> <p>Another slide to compare with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this slide starts before the other slide, False otherwise.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def __lt__(self, other: 'VideoSlide'):\n    \"\"\"\n    Compares two slides based on their start frame.\n\n    Parameters\n    ----------\n    other : VideoSlide\n        Another slide to compare with.\n\n    Returns\n    -------\n    bool\n        True if this slide starts before the other slide, False otherwise.\n    \"\"\"\n    return self.start_end_frames[0][0] &lt; other.start_end_frames[0][0]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the slide.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the slide.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the slide.\n\n    Returns\n    -------\n    str\n        String representation of the slide.\n    \"\"\"\n    return 'TFT(txt={0}, window_time={1}, bbs={2})'.format(\n        repr(self._full_text), repr(self.start_end_frames), repr(self._bounding_box))\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.copy","title":"<code>copy()</code>","text":"<p>Creates a copy of the slide.</p> <p>Returns:</p> Type Description <code>VideoSlide</code> <p>A copy of the current slide.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Creates a copy of the slide.\n\n    Returns\n    -------\n    VideoSlide\n        A copy of the current slide.\n    \"\"\"\n    tft_copy = VideoSlide(framed_sentences=None, start_end_frames=self.start_end_frames)\n    tft_copy._framed_sentences = self._framed_sentences\n    tft_copy._full_text = self._full_text\n    return tft_copy\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.get_framed_sentences","title":"<code>get_framed_sentences()</code>","text":"<p>Returns text segments with screen locations.</p> <p>Returns:</p> Type Description <code>List[Tuple[str, Tuple[int, int, int, int]]]</code> <p>List of text segments with their screen locations.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_framed_sentences(self):\n    \"\"\"\n    Returns text segments with screen locations.\n\n    Returns\n    -------\n    List[Tuple[str, Tuple[int, int, int, int]]]\n        List of text segments with their screen locations.\n    \"\"\"\n    full_text = self._full_text\n    return [((full_text[start_char_pos:end_char_pos]), bb) for (start_char_pos, end_char_pos), bb in self._framed_sentences]\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.get_full_text","title":"<code>get_full_text()</code>","text":"<p>Returns complete slide text.</p> <p>Returns:</p> Type Description <code>str</code> <p>Complete text content of the slide.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_full_text(self):\n    \"\"\"\n    Returns complete slide text.\n\n    Returns\n    -------\n    str\n        Complete text content of the slide.\n    \"\"\"\n    return self._full_text\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.get_split_text","title":"<code>get_split_text()</code>","text":"<p>Returns text split by newlines.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>Text content split by newlines.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_split_text(self):\n    \"\"\"\n    Returns text split by newlines.\n\n    Returns\n    -------\n    List[str]\n        Text content split by newlines.\n    \"\"\"\n    return self._full_text.split(\"(?&lt;=\\n)\")\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.merge_adjacent_startend_frames","title":"<code>merge_adjacent_startend_frames(max_dist=15)</code>","text":"<p>Merges nearby frame ranges.</p> <p>Parameters:</p> Name Type Description Default <code>max_dist</code> <code>int</code> <p>Maximum distance between frame ranges to merge, by default 15.</p> <code>15</code> <p>Returns:</p> Type Description <code>VideoSlide</code> <p>The slide with merged frame ranges.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def merge_adjacent_startend_frames(self, max_dist: int = 15) -&gt; 'VideoSlide':\n    \"\"\"\n    Merges nearby frame ranges.\n\n    Parameters\n    ----------\n    max_dist : int, optional\n        Maximum distance between frame ranges to merge, by default 15.\n\n    Returns\n    -------\n    VideoSlide\n        The slide with merged frame ranges.\n    \"\"\"\n    start_end_frames = self.start_end_frames\n    merged_start_end_frames = []\n    curr_start, curr_end = start_end_frames[0]\n    for new_start, new_end in start_end_frames:\n        if new_start - curr_end &lt;= max_dist:\n            curr_end = max(curr_end, new_end)\n        else:\n            merged_start_end_frames.append((curr_start, curr_end))\n            curr_start = new_start\n            curr_end = new_end\n    merged_start_end_frames.append((curr_start, curr_end))\n    self.start_end_frames = merged_start_end_frames\n    return self\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.VideoSlide.merge_frames","title":"<code>merge_frames(other_slide)</code>","text":"<p>Combines frame ranges with another slide.</p> <p>Parameters:</p> Name Type Description Default <code>other_slide</code> <code>VideoSlide</code> <p>Another slide to merge frames with.</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def merge_frames(self, other_slide: \"VideoSlide\") -&gt; None:\n    \"\"\"\n    Combines frame ranges with another slide.\n\n    Parameters\n    ----------\n    other_slide : VideoSlide\n        Another slide to merge frames with.\n    \"\"\"\n    this_startend_frames = self.start_end_frames\n    for other_start_end_elem in other_slide.start_end_frames:\n        if not other_start_end_elem in this_startend_frames:\n            insort_left(self.start_end_frames, other_start_end_elem)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.WhisperToPosTagged","title":"<code>WhisperToPosTagged</code>","text":"<p>Class for converting Whisper transcriptions to POS-tagged text.</p> <p>Attributes:</p> Name Type Description <code>_lang</code> <code>str</code> <p>Language of the transcription.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize the WhisperToPosTagged object with language.</p> <code>_group_short_sentences</code> <p>Groups short sentences in the transcription.</p> <code>_apply_italian_fixes</code> <p>Applies Italian-specific fixes to the transcription.</p> <code>_apply_english_fixes</code> <p>Applies English-specific fixes to the transcription.</p> <code>_restore_italian_fixes</code> <p>Restores Italian-specific fixes in the transcription.</p> <code>request_tagged_transcript</code> <p>Requests POS-tagged transcript for the given video.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>class WhisperToPosTagged:\n    \"\"\"\n    Class for converting Whisper transcriptions to POS-tagged text.\n\n    Attributes\n    ----------\n    _lang : str\n        Language of the transcription.\n\n    Methods\n    -------\n    __init__(language)\n        Initialize the WhisperToPosTagged object with language.\n    _group_short_sentences(timed_sentences, min_segment_len=4, min_segment_duration=3)\n        Groups short sentences in the transcription.\n    _apply_italian_fixes(timed_sentences)\n        Applies Italian-specific fixes to the transcription.\n    _apply_english_fixes(timed_sentences)\n        Applies English-specific fixes to the transcription.\n    _restore_italian_fixes(timed_transcript)\n        Restores Italian-specific fixes in the transcription.\n    request_tagged_transcript(video_id, timed_transcript)\n        Requests POS-tagged transcript for the given video.\n    \"\"\"\n\n    _lang: str\n\n    def __init__(self, language: str) -&gt; None:\n        \"\"\"\n        Initialize the WhisperToPosTagged object with language.\n\n        Parameters\n        ----------\n        language : str\n            Language of the transcription.\n        \"\"\"\n        self._lang = language\n\n    def _group_short_sentences(self, timed_sentences: list, min_segment_len: int = 4, min_segment_duration: float = 3):\n        \"\"\"\n        Group short sentences in the transcription.\n\n        Parameters\n        ----------\n        timed_sentences : list\n            List of timed sentences.\n        min_segment_len : int, optional\n            Minimum segment length, by default 4.\n        min_segment_duration : float, optional\n            Minimum segment duration, by default 3.\n\n        Returns\n        -------\n        list\n            List of grouped timed sentences.\n        \"\"\"\n        for i, sentence in reversed(list(enumerate(timed_sentences))): \n            # if i=4 and sent3 = \"Ebbene,\" sent4 = \"nulla si puo' dire perche' tutto e' stato detto.\" =&gt; merge into \"Ebbene, nulla si puo' dire perche' tutto e' stato detto.\"\n            # if sent3 = \"quindi otteniamo cio' che ci aspettiamo,\" and sent4 = \"cioe' niente.\" =&gt; merge into sent3 = \"quindi otteniamo cio' che ci aspettiamo, cioe' niente.\" and pop sent4\n            if i &gt; 0 and not timed_sentences[i-1][\"text\"].endswith(\".\") and \\\n              (len(sentence[\"text\"].strip().split()) &lt; min_segment_len or \n               len(timed_sentences[i-1][\"text\"].strip().split()) &lt; min_segment_len or \n               sentence[\"end\"] - sentence[\"start\"] &lt; min_segment_duration or \n               timed_sentences[i-1][\"text\"].endswith(\"'\") or \n               sentence[\"text\"].startswith(\"'\")):\n                prev_segment = timed_sentences[i-1]\n                prev_segment[\"text\"] += \" \" + sentence[\"text\"] \n                for word in sentence[\"words\"]:\n                    prev_segment[\"words\"].append(word)\n                prev_segment[\"end\"] = sentence[\"end\"]\n                timed_sentences.pop(i)\n        return timed_sentences \n\n    def _apply_italian_fixes(self,timed_sentences:list):\n        '''\n        Applies italian specific fixes to the text in order to be correctly analized by ItaliaNLP and matched back\n        Furthemore groups short sentences.\n        '''\n\n        out_sentences = []    \n        accent_replacements = {\n                                  \"e'\": \"\u00e8\", \"E'\": \"\u00c8\",\n                                  \"o'\": \"\u00f2\", \"O'\": \"\u00d2\",\n                                  \"a'\": \"\u00e0\", \"A'\": \"\u00c0\",\n                                  \"i'\": \"\u00ec\", \"I'\": \"\u00cc\",\n                                  \"u'\": \"\u00f9\", \"U'\": \"\u00d9\", \"po'\": \"p\u00f2\"\n                              }\n        number_regex = r'(-?\\d+(?:\\.\\d*)?)'\n        big_numer_regex = r'(-?)(\\d+)(.\\d\\d\\d)+'\n        degrees_regex = number_regex[:-1] + r'\u00b0)'\n        temperature_regex = degrees_regex[:-1] + r'[C|c|F|f|K|k])'\n\n        for i, segment in enumerate(timed_sentences):\n            segment = {\"text\": segment[\"text\"].lstrip(),\n                       \"words\": segment[\"words\"],\n                       \"start\": segment[\"start\"], \n                       \"end\": segment[\"end\"]}\n\n            to_remove_words = []\n\n            for j, word in enumerate(segment[\"words\"]):\n                word[\"word\"] = word[\"word\"].lstrip()\n\n                word.pop(\"tokens\",None)\n\n                # Match \"fatto,\" that must be split into tokens \"fatto\" and \",\" \n                if len(word[\"word\"]) &gt; 1 and word[\"word\"][-1] in [\",\",\".\",\"?\",\"!\"]:\n                    new_word = word.copy()\n                    new_word[\"word\"] = word[\"word\"][-1]\n                    segment[\"words\"].insert(j+1, new_word)\n                    word[\"word\"] = word[\"word\"][:-1]\n\n                if word[\"word\"][0] == \",\" and any(re.findall(r',\\d+',word[\"word\"])):\n                    new_word = word.copy()\n                    new_word[\"word\"] = word[\"word\"][1:]\n                    word[\"word\"] = word[\"word\"][0]\n                    segment[\"words\"].insert(j+1, new_word)\n                    segment[\"text\"] = segment[\"text\"].replace(\",\",\", \")\n\n                # Realign apostrophe and replace accented words\n                if word[\"word\"].startswith(\"'\"):\n                    segment[\"words\"][j-1][\"word\"] += \"'\"\n                    if len(segment[\"words\"][j-1][\"word\"]) == 2:\n                        for pattern, replacement in accent_replacements.items():\n                            segment[\"words\"][j-1][\"word\"] = re.sub(pattern, replacement, segment[\"words\"][j-1][\"word\"])\n                            segment[\"text\"] = re.sub(pattern, replacement, segment[\"text\"])\n                    word[\"word\"] = word[\"word\"][1:]\n\n                # Match math symbol terminology \"x'\" that should be \"x primo\" and \"E'\" that should be \"\u00c8\"\n                elif any(re.findall(r\"(?&lt;![a-zA-Z])[a-zA-Z]'\", word[\"word\"])):\n                    match_ = re.findall(r\"(?&lt;![a-zA-Z])[a-zA-Z]'\", word[\"word\"])[0]\n                    if match_ in accent_replacements.keys():\n                        replacement = accent_replacements[word[\"word\"]]\n                        segment[\"text\"] = segment[\"text\"].replace(word[\"word\"],replacement)\n                        word[\"word\"] = replacement\n                    # TODO there can be a case like \"l'altezza di l'(primo) nel...\" in text that can break it\n                    # but make it work would require to rework the structures and map words as indices of the text string\n                    elif any(re.findall(f\"{match_}[ .,:?!]\",segment[\"text\"])):\n                        word[\"word\"] = word[\"word\"].split(\"'\")[0]\n                        new_word = word.copy()\n                        new_word[\"word\"] = \"primo\"\n                        segment[\"words\"].insert(j+1,new_word)\n                        segment[\"text\"] = segment[\"text\"].replace(match_,match_[:-1]+\" primo\")\n                    # fixes some random \" l 'apostrofo mal messo\" -&gt; \" l'apostrofo mal messo\"\n                    elif any(re.findall(r\"[a-zA-Z]+ '\", segment[\"text\"])):\n                        match_ = re.findall(r\"[a-zA-Z]+ '\", segment[\"text\"])[0]\n                        segment[\"text\"] = segment[\"text\"].replace(match_, match_[:-2]+\"'\")\n\n                # \"di raggio R.\" separated symbol R -&gt; \"di raggio R .\" for T2K correct pos tag\n                # TODO check with another video that Whisper AI correctly separates the two words in words list \n                elif any(re.findall(r\"\\s[a-zA-Z][?,.!;:]\", segment[\"text\"])):\n                    segment[\"text\"] = re.sub(r\"\\s([a-zA-Z])([?,.!;:])\",r\" \\1 \\2\", segment[\"text\"])\n\n                # Match \"po'\" but ignores \"anch'\" or \"dell'\" \n                elif any(re.findall(r\"[a-zA-Z]+'\", word[\"word\"])) and word[\"word\"].endswith(\"'\") and len(word[\"word\"]) &gt;= 3:\n                    match_ = re.findall(r\"[a-zA-Z]+'\", word[\"word\"])[0]\n                    if match_ in accent_replacements.keys():\n                        replacement = accent_replacements[word[\"word\"]]\n                        word[\"word\"] = replacement\n\n                # Case \"termo\" \"-idrometrico\" -&gt; merged into \"termo-idrometrico\" for T2K compatibility\n                # can also be \"pay-as-you-go\" found split into \"pay\", \"-as\", \"-you\", \"-go\"\n                elif word[\"word\"].startswith(\"-\"):\n                    for head_word in segment[\"words\"][j-1::-1]:\n                        if not head_word[\"word\"].startswith(\"-\"):\n                            break\n                    head_word[\"word\"] = head_word[\"word\"] + word[\"word\"]\n                    head_word[\"end\"] = word[\"end\"]\n                    to_remove_words.append(j)\n\n                # Sometime happened the shift of the apostrophe [\"accetta l\", \"'ipotesi forte\"] \n                # Should not happen due to prior merge\n                #if segment[\"text\"].startswith(\"'\") and not \"'\" in segment[\"words\"][0] and i &gt; 0:\n                #    segment[\"text\"] = segment[\"text\"][1:]\n                #    timed_sentences[i-1][\"text\"] += \"'\"\n                #    timed_sentences[i-1][\"words\"][-1][\"word\"] += \"'\"\n\n                # Case \"25\u00b0C\" -&gt; \"25\u00b0\" \"celsius\"\n                if any(re.findall(temperature_regex,word[\"word\"])):\n                    new_word = word.copy()\n                    new_word[\"word\"] = word[\"word\"][-1]\n                    #segment[\"text\"] = segment[\"text\"].replace(word[\"word\"][-1],\" \"+scale)\n                    segment[\"words\"].insert(j+1, new_word)\n                    word[\"word\"] = word[\"word\"][:-1]\n\n                # Case \"22\u00b0\" -&gt; \"22\" \"\u00b0\"\n                if any(re.findall(degrees_regex, word[\"word\"])):\n                    new_word = word.copy()\n                    new_word[\"word\"] = \"\u00b0\" \n                    #segment[\"text\"] = segment[\"text\"].replace(\"\u00b0\",\" \u00b0 \")\n                    segment[\"words\"].insert(j+1, new_word)\n                    word[\"word\"] = word[\"word\"][:-1]\n\n                elif any(re.findall(r\"\\s\\.[0-9]+\", word[\"word\"])):\n                    new_word = word.copy()\n                    new_word[\"word\"] = word[\"word\"][1:]\n                    segment[\"words\"].insert(j+1, new_word)\n                    word[\"word\"] = \".\"\n\n                if any(re.findall(big_numer_regex, segment[\"text\"])):\n                    match_ = \"\".join(re.findall(big_numer_regex, segment[\"text\"])[0])\n                    segment[\"text\"] = segment[\"text\"].replace(match_, match_.replace(\"-\",\" - \").replace(\".\",\" . \"))\n\n                # Sometimes words list don't perfectly match the text: \"'attrito della ruota\", words: [\"attrito\",\"della\", \"ruota'\"]\n                # But \"p\u00f2\" in words and \"po'\" in text match, TODO Fix with T2K \n                if word[\"word\"] not in segment[\"text\"] and not (word[\"word\"] == \"p\u00f2\" and \"po'\" in segment[\"text\"]) :\n                    longest_substring = ''\n                    text = segment[\"text\"]\n                    word_ = word[\"word\"]\n                    for ch_l in range(len(word_)):\n                        for ch_r in range(ch_l + 1, len(word_) + 1):\n                            if word_[ch_l:ch_r] in text and len(word_[ch_l:ch_r]) &gt; len(longest_substring):\n                                longest_substring = word_[ch_l:ch_r]\n                    # Replace the word with the longest substring\n                    word[\"word\"] = longest_substring\n\n                # Random points in words\n                if len(word[\"word\"]) &gt; 1 and word[\"word\"].endswith(\".\"):\n                    new_word = word.copy()\n                    new_word[\"word\"] = \".\"\n                    word[\"word\"] = word[\"word\"][:-1]\n                    segment[\"words\"].insert(j+1, new_word)\n\n\n                # Case \"22%\" -&gt; \"22\" \"%\"\n                elif any(re.findall(number_regex+'%', word[\"word\"])):\n                    new_word = word.copy()\n                    word[\"word\"] = word[\"word\"][:-1]\n                    new_word[\"word\"] = \"%\"\n                    segment[\"words\"].insert(j+1, new_word)\n\n                if any(re.findall(number_regex+'%', segment[\"text\"])):\n                    segment[\"text\"] = re.sub(r\"%([?,.!;:])\",r\"% \\1\", segment[\"text\"])\n                    segment[\"text\"] = re.sub(r\"([0-9])%\",r\"\\1 %\", segment[\"text\"])\n\n\n                # Match with \"dell'SiO2\" -&gt; split into tokens \"dell'\" and \"SiO2\"\n                if len(word[\"word\"].split(\"'\")) &gt; 1 and len(word[\"word\"].split(\"'\")[1]) &gt; 0:\n                    before_apos, after_apos = word[\"word\"].split(\"'\")\n                    new_word = word.copy()\n                    new_word[\"word\"] = after_apos\n                    segment[\"words\"].insert(j+1, new_word)\n                    word[\"word\"] = before_apos+\"'\"\n\n            for indx in reversed(to_remove_words):\n                del segment[\"words\"][indx]\n\n            segment[\"text\"] = segment[\"text\"].replace(\"  \", \" \")\n            out_sentences.append(segment)\n\n        return out_sentences\n\n    def _apply_english_fixes(self,timed_sentences:list):\n        \"\"\"\n        Apply English-specific fixes to the transcription.\n\n        Parameters\n        ----------\n        timed_sentences : list\n            List of timed sentences.\n\n        Returns\n        -------\n        list\n            List of fixed timed sentences.\n        \"\"\"\n        for sent_id, sentence in enumerate(timed_sentences):\n            sentence = {\"text\":sentence[\"text\"].strip(),\n                        \"start\":sentence[\"start\"],\n                        \"end\":sentence[\"end\"],\n                        \"words\":sentence[\"words\"]}\n            timed_sentences[sent_id] = sentence \n            for word_id, word in enumerate(sentence[\"words\"]):\n                word.pop(\"tokens\",None)\n                word[\"word\"] = word[\"word\"].strip()\n                if any([word[\"word\"].endswith(punct) for punct in [\",\",\".\",\"!\",\"?\"]]) and len(word[\"word\"]) &gt; 1:\n                    new_word = word.copy()\n                    new_word[\"word\"] = new_word[\"word\"][-1]\n                    sentence[\"words\"].insert(word_id+1, new_word)\n                    word[\"word\"] = word[\"word\"][:-1]\n        return timed_sentences\n\n\n\n    def _restore_italian_fixes(timed_transcript:list):\n        \"\"\"\n        Restores Italian-specific fixes in the transcription.\n\n        Parameters\n        ----------\n        timed_transcript : list\n            List of timed sentences.\n\n        Returns\n        -------\n        list\n            List of restored timed sentences.\n        \"\"\"\n        for sentence in timed_transcript:\n            sentence[\"text\"] = re.sub(r'\\s+%', '%', sentence[\"text\"])                           # replace \" %\" with \"%\"\n            sentence[\"text\"] = re.sub(r'%\\s([?,.!;:])', r'%\\1', sentence[\"text\"])               # replace \"% ,\" with \"%,\"\n            sentence[\"text\"] = re.sub(r\"\\s([a-zA-Z])\\s([?,.!;:])\",r\" \\1\\2\", sentence[\"text\"])   # replace \" A .\" with \" A.\"\n            match_ = re.findall(r'(\\s-\\s)?(\\d+)(\\s.\\s\\d{3})+',sentence[\"text\"])                 # find any big number spaced \" - 24 . 000\" (meno 24 mila)\n            if any(match_):\n                sentence[\"text\"] = sentence[\"text\"].replace(\"\".join(match_[0]), \"\".join(match_[0]).replace(\" \",\"\"))\n        return timed_transcript\n\n    def request_tagged_transcript(self, video_id:str, timed_transcript:list):\n        \"\"\"\n        Request POS-tagged transcript for the given video.\n\n        Parameters\n        ----------\n        video_id : str\n            ID of the video.\n        timed_transcript : list\n            List of timed sentences.\n\n        Returns\n        -------\n        tuple\n            Document ID and list of timed sentences.\n        \"\"\"\n        timed_transcript = self._group_short_sentences(timed_transcript)\n        if self._lang == \"it\":\n            timed_transcript = self._apply_italian_fixes(timed_transcript)\n            string_transcript = \" \".join(timed_sentence[\"text\"] for timed_sentence in timed_transcript if not \"[\" in timed_sentence['text'])\n            timed_transcript = self._restore_italian_fixes(timed_transcript)\n\n            api_obj = ItaliaNLAPI()\n            doc_id = api_obj.upload_document(string_transcript, language=language, async_call=False)\n\n            tagged_sentences = api_obj.wait_for_pos_tagging(doc_id)\n\n            tagged_transcript = {\"full_text\":\"\", \"words\":[]}\n            for sentence in tagged_sentences:\n                tagged_transcript[\"full_text\"] += sentence[\"sentence\"]+\" \"\n                for word in sentence[\"words\"]:\n                    # append the word from NLPTranscript but remove \"-\" from for example \"inviar-\", \"li\"\n                    word = {\"word\":     word[\"word\"] if len(word[\"word\"]) == 1 or (len(word[\"word\"]) &gt; 1 and not word[\"word\"].endswith(\"-\")) else word[\"word\"][:-1], \n                            \"lemma\":    word[\"lemma\"], \n                            \"pos\":      word[\"pos\"], \n                            \"gen\":      word[\"gen\"], \n                            \"cpos\":     word[\"cpos\"],\n                            \"num\":      word[\"num\"]}\n                    tagged_transcript[\"words\"].append(word)\n\n            words_cursor = 0\n            tagged_transcript_words = tagged_transcript[\"words\"]\n            is_first_part_of_word = True\n            is_misaligned = False\n            for sentence in timed_transcript:\n                for word_indx, word in enumerate(sentence[\"words\"]):\n                    if word[\"word\"] == tagged_transcript_words[words_cursor][\"word\"] or \\\n                      (word[\"word\"] == \"po'\" and tagged_transcript_words[words_cursor][\"word\"] == \"p\\u00f2\"):\n                        transcript_word = tagged_transcript_words[words_cursor]\n                        word[\"gen\"] = transcript_word[\"gen\"] if transcript_word[\"gen\"] is not None else \"\"\n                        word[\"lemma\"] = transcript_word[\"lemma\"]\n                        word[\"pos\"] = transcript_word[\"pos\"]\n                        word[\"cpos\"] = transcript_word[\"cpos\"]\n                        word[\"num\"] = transcript_word[\"num\"] if transcript_word[\"num\"] is not None else \"\"\n\n                    # Can be for example in Whisper transcript \"inviarli\" a single word but ItaliaNLP gives \"inviar\", \"li\"\n                    elif tagged_transcript_words[words_cursor][\"word\"] in word[\"word\"]:\n                        if is_first_part_of_word:\n                            new_word = word.copy()\n                        transcript_word = tagged_transcript_words[words_cursor]\n                        word[\"word\"] = transcript_word[\"word\"]\n                        word[\"gen\"] = transcript_word[\"gen\"] if transcript_word[\"gen\"] is not None else \"\"\n                        word[\"lemma\"] = transcript_word[\"lemma\"]\n                        word[\"pos\"] = transcript_word[\"pos\"]\n                        word[\"cpos\"] = transcript_word[\"cpos\"]\n                        word[\"num\"] = transcript_word[\"num\"] if transcript_word[\"num\"] is not None else \"\"\n                        if is_first_part_of_word:\n                            word[\"end\"] = 0.8*(word[\"end\"]-word[\"start\"]) + word[\"start\"]\n                        else:\n                            word[\"start\"] = sentence[\"words\"][word_indx-1][\"end\"]\n                        if is_first_part_of_word:\n                            sentence[\"words\"].insert(word_indx+1, new_word) \n                            is_first_part_of_word = False\n                        else:\n                            is_first_part_of_word = True\n\n                    # match is partial so it's wrong, print a message on backend but continue (TODO but may break)\n                    elif word[\"word\"] in tagged_transcript_words[words_cursor][\"word\"] and \\\n                      ((len(sentence[\"words\"]) &gt; word_indx+1 and sentence[\"words\"][word_indx+1][\"word\"] in tagged_transcript_words[words_cursor][\"word\"]) or \\\n                      is_misaligned) :\n                        tagged_word = tagged_transcript_words[words_cursor]\n                        word[\"gen\"] = tagged_word[\"gen\"]\n                        word[\"lemma\"] = tagged_word[\"lemma\"]\n                        word[\"pos\"] = tagged_word[\"pos\"]\n                        word[\"cpos\"] = tagged_word[\"cpos\"] \n                        word[\"num\"] = tagged_word[\"gen\"]\n                        is_misaligned = not tagged_word[\"word\"].endswith(word[\"word\"])\n                        print(f\"Error in matching tagged and timed transcript, for video: {self.data['video_id']}\")\n                        print(f\"word \\\"{word['word']}\\\" is not \\\"{tagged_transcript_words[words_cursor]['word']}\\\"\")\n                        if is_misaligned:\n                            words_cursor -= 1\n                        else:\n                            print(\"Realigned successfully!\")\n                    words_cursor += 1\n        elif self._lang == \"en\":\n            conll = conll_gen(video_id,SemanticText(\" \".join(timed_sentence[\"text\"] for timed_sentence in timed_transcript if not \"[\" in timed_sentence['text']), self._lang), self._lang)\n            conll_words = []\n            for sentence in conll:\n                for token in sentence:\n                        conll_words.append({ \"id\":token[\"id\"],\n                                             \"word\":token[\"form\"],\n                                             \"lemma\":token[\"lemma\"],\n                                             \"cpos\":token[\"upos\"],\n                                             \"pos\":token[\"xpos\"],\n                                             \"num\":token[\"feats\"].get(\"Number\",\"\").replace(\"Sing\",\"s\").replace(\"Plur\",\"p\") if token[\"feats\"] else \"\",\n                                             \"gen\":\"\" # token[\"feats\"].get(\"Gen\",\"\") # There is no gen in udpipe\n                                             })\n            timed_transcript = self._apply_english_fixes(timed_transcript)\n            word_indx = 0\n            skip_next = False\n            for sent_id, sentence in enumerate(timed_transcript):\n                for word_id, word in enumerate(sentence[\"words\"]):\n                    conll_word = conll_words[word_indx]\n                    if (\"id\" in conll_word.keys() and type(conll_word[\"id\"]) != int) or (conll_word[\"word\"] in word[\"word\"] and conll_word[\"word\"] != word[\"word\"]):\n                        if (\"id\" in conll_word.keys() and type(conll_word[\"id\"]) != int):\n                            word_indx += 1\n                        conll_word = conll_words[word_indx]\n                        conll_word.pop(\"id\",None)\n                        sentence[\"words\"][word_id] = conll_word\n                        conll_word[\"start\"] = word[\"start\"]\n                        conll_word[\"end\"] = word[\"start\"] + .8*(word[\"end\"]-word[\"start\"])\n                        conll_word[\"probability\"] = word[\"probability\"]\n\n                        word_indx += 1\n                        conll_word = conll_words[word_indx]\n                        conll_word.pop(\"id\",None)\n                        sentence[\"words\"].insert(word_id+1,conll_word)\n                        conll_word[\"start\"] = word[\"start\"] + .8*(word[\"end\"]-word[\"start\"])\n                        conll_word[\"end\"] = word[\"end\"]\n                        conll_word[\"probability\"] = word[\"probability\"]\n                        skip_next = True\n                    elif skip_next:\n                        word_indx -= 1\n                        skip_next = False\n\n                    elif word[\"word\"] == conll_word[\"word\"]:\n                        sentence[\"words\"][word_id] = conll_word\n                        conll_word[\"start\"] = word[\"start\"]\n                        conll_word[\"end\"] = word[\"end\"]\n                        conll_word[\"probability\"] = word[\"probability\"]\n                    else:\n                        raise Exception(\"Error parsing! required fix\")\n                    conll_word.pop(\"id\",None)\n                    word_indx += 1\n            doc_id = None\n        return doc_id, timed_transcript\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.WhisperToPosTagged.__init__","title":"<code>__init__(language)</code>","text":"<p>Initialize the WhisperToPosTagged object with language.</p> <p>Parameters:</p> Name Type Description Default <code>language</code> <code>str</code> <p>Language of the transcription.</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def __init__(self, language: str) -&gt; None:\n    \"\"\"\n    Initialize the WhisperToPosTagged object with language.\n\n    Parameters\n    ----------\n    language : str\n        Language of the transcription.\n    \"\"\"\n    self._lang = language\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.WhisperToPosTagged.request_tagged_transcript","title":"<code>request_tagged_transcript(video_id, timed_transcript)</code>","text":"<p>Request POS-tagged transcript for the given video.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>ID of the video.</p> required <code>timed_transcript</code> <code>list</code> <p>List of timed sentences.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Document ID and list of timed sentences.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def request_tagged_transcript(self, video_id:str, timed_transcript:list):\n    \"\"\"\n    Request POS-tagged transcript for the given video.\n\n    Parameters\n    ----------\n    video_id : str\n        ID of the video.\n    timed_transcript : list\n        List of timed sentences.\n\n    Returns\n    -------\n    tuple\n        Document ID and list of timed sentences.\n    \"\"\"\n    timed_transcript = self._group_short_sentences(timed_transcript)\n    if self._lang == \"it\":\n        timed_transcript = self._apply_italian_fixes(timed_transcript)\n        string_transcript = \" \".join(timed_sentence[\"text\"] for timed_sentence in timed_transcript if not \"[\" in timed_sentence['text'])\n        timed_transcript = self._restore_italian_fixes(timed_transcript)\n\n        api_obj = ItaliaNLAPI()\n        doc_id = api_obj.upload_document(string_transcript, language=language, async_call=False)\n\n        tagged_sentences = api_obj.wait_for_pos_tagging(doc_id)\n\n        tagged_transcript = {\"full_text\":\"\", \"words\":[]}\n        for sentence in tagged_sentences:\n            tagged_transcript[\"full_text\"] += sentence[\"sentence\"]+\" \"\n            for word in sentence[\"words\"]:\n                # append the word from NLPTranscript but remove \"-\" from for example \"inviar-\", \"li\"\n                word = {\"word\":     word[\"word\"] if len(word[\"word\"]) == 1 or (len(word[\"word\"]) &gt; 1 and not word[\"word\"].endswith(\"-\")) else word[\"word\"][:-1], \n                        \"lemma\":    word[\"lemma\"], \n                        \"pos\":      word[\"pos\"], \n                        \"gen\":      word[\"gen\"], \n                        \"cpos\":     word[\"cpos\"],\n                        \"num\":      word[\"num\"]}\n                tagged_transcript[\"words\"].append(word)\n\n        words_cursor = 0\n        tagged_transcript_words = tagged_transcript[\"words\"]\n        is_first_part_of_word = True\n        is_misaligned = False\n        for sentence in timed_transcript:\n            for word_indx, word in enumerate(sentence[\"words\"]):\n                if word[\"word\"] == tagged_transcript_words[words_cursor][\"word\"] or \\\n                  (word[\"word\"] == \"po'\" and tagged_transcript_words[words_cursor][\"word\"] == \"p\\u00f2\"):\n                    transcript_word = tagged_transcript_words[words_cursor]\n                    word[\"gen\"] = transcript_word[\"gen\"] if transcript_word[\"gen\"] is not None else \"\"\n                    word[\"lemma\"] = transcript_word[\"lemma\"]\n                    word[\"pos\"] = transcript_word[\"pos\"]\n                    word[\"cpos\"] = transcript_word[\"cpos\"]\n                    word[\"num\"] = transcript_word[\"num\"] if transcript_word[\"num\"] is not None else \"\"\n\n                # Can be for example in Whisper transcript \"inviarli\" a single word but ItaliaNLP gives \"inviar\", \"li\"\n                elif tagged_transcript_words[words_cursor][\"word\"] in word[\"word\"]:\n                    if is_first_part_of_word:\n                        new_word = word.copy()\n                    transcript_word = tagged_transcript_words[words_cursor]\n                    word[\"word\"] = transcript_word[\"word\"]\n                    word[\"gen\"] = transcript_word[\"gen\"] if transcript_word[\"gen\"] is not None else \"\"\n                    word[\"lemma\"] = transcript_word[\"lemma\"]\n                    word[\"pos\"] = transcript_word[\"pos\"]\n                    word[\"cpos\"] = transcript_word[\"cpos\"]\n                    word[\"num\"] = transcript_word[\"num\"] if transcript_word[\"num\"] is not None else \"\"\n                    if is_first_part_of_word:\n                        word[\"end\"] = 0.8*(word[\"end\"]-word[\"start\"]) + word[\"start\"]\n                    else:\n                        word[\"start\"] = sentence[\"words\"][word_indx-1][\"end\"]\n                    if is_first_part_of_word:\n                        sentence[\"words\"].insert(word_indx+1, new_word) \n                        is_first_part_of_word = False\n                    else:\n                        is_first_part_of_word = True\n\n                # match is partial so it's wrong, print a message on backend but continue (TODO but may break)\n                elif word[\"word\"] in tagged_transcript_words[words_cursor][\"word\"] and \\\n                  ((len(sentence[\"words\"]) &gt; word_indx+1 and sentence[\"words\"][word_indx+1][\"word\"] in tagged_transcript_words[words_cursor][\"word\"]) or \\\n                  is_misaligned) :\n                    tagged_word = tagged_transcript_words[words_cursor]\n                    word[\"gen\"] = tagged_word[\"gen\"]\n                    word[\"lemma\"] = tagged_word[\"lemma\"]\n                    word[\"pos\"] = tagged_word[\"pos\"]\n                    word[\"cpos\"] = tagged_word[\"cpos\"] \n                    word[\"num\"] = tagged_word[\"gen\"]\n                    is_misaligned = not tagged_word[\"word\"].endswith(word[\"word\"])\n                    print(f\"Error in matching tagged and timed transcript, for video: {self.data['video_id']}\")\n                    print(f\"word \\\"{word['word']}\\\" is not \\\"{tagged_transcript_words[words_cursor]['word']}\\\"\")\n                    if is_misaligned:\n                        words_cursor -= 1\n                    else:\n                        print(\"Realigned successfully!\")\n                words_cursor += 1\n    elif self._lang == \"en\":\n        conll = conll_gen(video_id,SemanticText(\" \".join(timed_sentence[\"text\"] for timed_sentence in timed_transcript if not \"[\" in timed_sentence['text']), self._lang), self._lang)\n        conll_words = []\n        for sentence in conll:\n            for token in sentence:\n                    conll_words.append({ \"id\":token[\"id\"],\n                                         \"word\":token[\"form\"],\n                                         \"lemma\":token[\"lemma\"],\n                                         \"cpos\":token[\"upos\"],\n                                         \"pos\":token[\"xpos\"],\n                                         \"num\":token[\"feats\"].get(\"Number\",\"\").replace(\"Sing\",\"s\").replace(\"Plur\",\"p\") if token[\"feats\"] else \"\",\n                                         \"gen\":\"\" # token[\"feats\"].get(\"Gen\",\"\") # There is no gen in udpipe\n                                         })\n        timed_transcript = self._apply_english_fixes(timed_transcript)\n        word_indx = 0\n        skip_next = False\n        for sent_id, sentence in enumerate(timed_transcript):\n            for word_id, word in enumerate(sentence[\"words\"]):\n                conll_word = conll_words[word_indx]\n                if (\"id\" in conll_word.keys() and type(conll_word[\"id\"]) != int) or (conll_word[\"word\"] in word[\"word\"] and conll_word[\"word\"] != word[\"word\"]):\n                    if (\"id\" in conll_word.keys() and type(conll_word[\"id\"]) != int):\n                        word_indx += 1\n                    conll_word = conll_words[word_indx]\n                    conll_word.pop(\"id\",None)\n                    sentence[\"words\"][word_id] = conll_word\n                    conll_word[\"start\"] = word[\"start\"]\n                    conll_word[\"end\"] = word[\"start\"] + .8*(word[\"end\"]-word[\"start\"])\n                    conll_word[\"probability\"] = word[\"probability\"]\n\n                    word_indx += 1\n                    conll_word = conll_words[word_indx]\n                    conll_word.pop(\"id\",None)\n                    sentence[\"words\"].insert(word_id+1,conll_word)\n                    conll_word[\"start\"] = word[\"start\"] + .8*(word[\"end\"]-word[\"start\"])\n                    conll_word[\"end\"] = word[\"end\"]\n                    conll_word[\"probability\"] = word[\"probability\"]\n                    skip_next = True\n                elif skip_next:\n                    word_indx -= 1\n                    skip_next = False\n\n                elif word[\"word\"] == conll_word[\"word\"]:\n                    sentence[\"words\"][word_id] = conll_word\n                    conll_word[\"start\"] = word[\"start\"]\n                    conll_word[\"end\"] = word[\"end\"]\n                    conll_word[\"probability\"] = word[\"probability\"]\n                else:\n                    raise Exception(\"Error parsing! required fix\")\n                conll_word.pop(\"id\",None)\n                word_indx += 1\n        doc_id = None\n    return doc_id, timed_transcript\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.extract_keywords_LEGACY","title":"<code>extract_keywords_LEGACY(text, maxWords=3, minFrequency=1)</code>","text":"<p>Extracts keywords from text using the Rake algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to extract keywords from.</p> required <code>maxWords</code> <code>int</code> <p>Maximum number of words in a keyword, by default 3.</p> <code>3</code> <code>minFrequency</code> <code>int</code> <p>Minimum frequency of a keyword to be included, by default 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing extracted keywords with their frequency and domain relevance.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def extract_keywords_LEGACY(text: str, maxWords=3, minFrequency=1):\n    \"\"\"\n    Extracts keywords from text using the Rake algorithm.\n\n    Parameters\n    ----------\n    text : str\n        Text to extract keywords from.\n    maxWords : int, optional\n        Maximum number of words in a keyword, by default 3.\n    minFrequency : int, optional\n        Minimum frequency of a keyword to be included, by default 1.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame containing extracted keywords with their frequency and domain relevance.\n    \"\"\"\n    r = Rake(max_length=maxWords)\n    r.extract_keywords_from_text(text)\n    ranks_and_keywords = r.get_ranked_phrases_with_scores()\n    concepts_stored = set()\n    nlp = NLPSingleton()\n    words_lemmas = {}\n    for i, (score, term) in enumerate(ranks_and_keywords):\n        lemma = \" \".join([token.lemma_ for token in nlp.lemmatize(term, \"en\")])\n        if not lemma in words_lemmas.keys():\n            words_lemmas[lemma] = True\n        else:\n            ranks_and_keywords[i] = (score, lemma)\n\n    counts = dict(Counter(list(map(lambda x: x[1], ranks_and_keywords))))\n    out_concepts = [{\"term\": key, \"frequency\": value, \"domain_relevance\": 100} for key, value in counts.items() if value &gt; minFrequency]\n    for concept in out_concepts:\n        concepts_stored.add(concept[\"term\"])\n    for (score, key) in ranks_and_keywords[:15]:\n        if key not in concepts_stored:\n            out_concepts.append({\"term\": key, \"frequency\": counts[key], \"domain_relevance\": 100})\n            concepts_stored.add(key)\n\n    return DataFrame(out_concepts)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.get_real_keywords","title":"<code>get_real_keywords(video_id, annotator_id=None)</code>","text":"<p>Retrieves real keywords for a video.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>ID of the video.</p> required <code>annotator_id</code> <code>str</code> <p>ID of the annotator, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, List[str]]</code> <p>Title of the video and list of keywords.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_real_keywords(video_id, annotator_id=None):\n    \"\"\"\n    Retrieves real keywords for a video.\n\n    Parameters\n    ----------\n    video_id : str\n        ID of the video.\n    annotator_id : str, optional\n        ID of the annotator, by default None.\n\n    Returns\n    -------\n    Tuple[str, List[str]]\n        Title of the video and list of keywords.\n    \"\"\"\n    graphs = mongo.get_graphs_info(video_id)\n    if graphs is None:\n        video_doc = mongo.get_video_data(video_id)\n        return video_doc['title'], [term[\"term\"] for term in video_doc['transcript_data'][\"terms\"]]\n\n    indx_annotator = 0\n    if annotator_id is not None:\n        annotators = graphs['annotators']\n        for i, annot in enumerate(annotators):\n            if annot['id'] == annotator_id:\n                indx_annotator = i\n                break\n    annotator_id = graphs[\"annotators\"][indx_annotator]['id']\n    concept_map_annotator = mongo.get_concept_map(annotator_id, video_id)\n    definitions = mongo.get_definitions(annotator_id, video_id)\n    keywords = []\n\n    for d in definitions:\n        if d[\"concept\"] not in keywords:\n            keywords.append(d[\"concept\"])\n\n    for rel in concept_map_annotator:\n        if rel[\"prerequisite\"] not in keywords:\n            keywords.append(rel[\"prerequisite\"])\n        if rel[\"target\"] not in keywords:\n            keywords.append(rel[\"target\"])\n\n    return graphs[\"title\"], keywords\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.get_timed_sentences","title":"<code>get_timed_sentences(subtitles, sentences)</code>","text":"<p>For each sentence, add its start and end time obtained from the subtitles.</p> <p>Parameters:</p> Name Type Description Default <code>subtitles</code> <code>List[Dict[str, Any]]</code> <p>List of subtitle dictionaries with 'start', 'end', and 'text' keys.</p> required <code>sentences</code> <code>List[str]</code> <p>List of sentences to time.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of timed sentences with 'text', 'start', and 'end' keys.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def get_timed_sentences(subtitles, sentences: List[str]):\n    \"\"\"\n    For each sentence, add its start and end time obtained from the subtitles.\n\n    Parameters\n    ----------\n    subtitles : List[Dict[str, Any]]\n        List of subtitle dictionaries with 'start', 'end', and 'text' keys.\n    sentences : List[str]\n        List of sentences to time.\n\n    Returns\n    -------\n    List[Dict[str, Any]]\n        List of timed sentences with 'text', 'start', and 'end' keys.\n    \"\"\"\n    num_words_sentence = []\n    num_words_sub = []\n\n    for s in sentences:\n        num_words_sentence.append(len(s.split()))\n    for s in subtitles:\n        num_words_sub.append(len(s[\"text\"].split()))\n\n    timed_sentences = [{\"text\": sentences[0], \"start\": subtitles[0][\"start\"]}]\n\n    i = 0\n    j = 0\n\n    while i &lt; len(num_words_sentence) and j &lt; len(num_words_sub):\n        if num_words_sentence[i] &gt; num_words_sub[j]:\n            num_words_sentence[i] = num_words_sentence[i] - num_words_sub[j]\n            j += 1\n\n        elif num_words_sentence[i] &lt; num_words_sub[j]:\n            timed_sentences[i][\"end\"] = subtitles[j][\"end\"]\n            num_words_sub[j] = num_words_sub[j] - num_words_sentence[i]\n            i += 1\n            if i &lt; len(num_words_sentence) and j &lt; len(num_words_sub):\n                timed_sentences.append({\"text\": sentences[i], \"start\": subtitles[j][\"start\"]})\n        else:\n            timed_sentences[i][\"end\"] = subtitles[j][\"end\"]\n            num_words_sentence[i] = 0\n            num_words_sub[j] = 0\n            i += 1\n            j += 1\n            if i &lt; len(num_words_sentence) and j &lt; len(num_words_sub):\n                timed_sentences.append({\"text\": sentences[i], \"start\": subtitles[j][\"start\"]})\n\n    timed_sentences[len(timed_sentences) - 1][\"end\"] = subtitles[len(subtitles) - 1][\"end\"]\n\n    return timed_sentences\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/text_processor/words/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.text_processor.words.lemmatize","title":"<code>lemmatize(lemmas)</code>","text":"<p>Lemmatizes a list of concepts.</p> <p>Parameters:</p> Name Type Description Default <code>lemmas</code> <code>List[str]</code> <p>List of concepts to lemmatize.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of lemmatized concepts.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/text_processor/words.py</code> <pre><code>def lemmatize(lemmas):\n    \"\"\"\n    Lemmatizes a list of concepts.\n\n    Parameters\n    ----------\n    lemmas : List[str]\n        List of concepts to lemmatize.\n\n    Returns\n    -------\n    List[str]\n        List of lemmatized concepts.\n    \"\"\"\n    concepts_lemmatized = []\n\n    lemmatizer = WordNetLemmatizer()\n\n    for concept in lemmas:\n        lemmatized = \"\"\n\n        for word in concept.split(\" \"):\n            lemmatized += lemmatizer.lemmatize(word.lower()) + \" \"\n\n        lemmatized = lemmatized.rstrip()\n\n        if lemmatized not in concepts_lemmatized:\n            concepts_lemmatized.append(lemmatized)\n\n    return concepts_lemmatized\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/utils/itertools/","title":"itertools","text":""},{"location":"codebase/EKEELVideoAnnotation/utils/itertools/#itertools","title":"Itertools","text":""},{"location":"codebase/EKEELVideoAnnotation/utils/itertools/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.utils.itertools.double_iterator","title":"<code>double_iterator(iterable, enumerated=False)</code>","text":"<p>Generates all possible pairs of elements from an iterable.</p> <p>Creates pairs representing both upper and lower triangular matrix elements, excluding diagonal (self-pairs).</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>The input iterable to process</p> required <code>enumerated</code> <code>bool</code> <p>If True, yields (index1, index2, value1, value2) instead of (value1, value2)</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator</code> <p>If enumerated=False: yields (value1, value2) tuples If enumerated=True: yields (index1, index2, value1, value2) tuples</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(double_iterator([1,2,3]))\n[(1,2), (1,3), (2,1), (2,3), (3,1), (3,2)]\n</code></pre> Source code in <code>EVA_apps/EKEELVideoAnnotation/utils/itertools.py</code> <pre><code>def double_iterator(iterable, enumerated:bool=False) -&gt; 'Generator[Iterable,Iterable] or Generator[int,int,Iterable,Iterable]':\n    \"\"\"\n    Generates all possible pairs of elements from an iterable.\n\n    Creates pairs representing both upper and lower triangular matrix elements,\n    excluding diagonal (self-pairs).\n\n    Parameters\n    ----------\n    iterable : Iterable\n        The input iterable to process\n    enumerated : bool, default=False\n        If True, yields (index1, index2, value1, value2) instead of (value1, value2)\n\n    Returns\n    -------\n    Generator\n        If enumerated=False: yields (value1, value2) tuples\n        If enumerated=True: yields (index1, index2, value1, value2) tuples\n\n    Examples\n    --------\n    &gt;&gt;&gt; list(double_iterator([1,2,3]))\n    [(1,2), (1,3), (2,1), (2,3), (3,1), (3,2)]\n    \"\"\"\n    if not enumerated:\n        for x in iter(iterable):\n            for y in iter(iterable):\n                if x is not y:\n                    yield (x,y)\n    else:\n        for i, x in enumerate(iter(iterable)):\n            for j, y in enumerate(iter(iterable)):\n                if x is not y:\n                    yield (i, j, x, y)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/utils/itertools/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.utils.itertools.pairwise","title":"<code>pairwise(iterable, None_tail=True, reversed=False)</code>","text":"<p>Generator of linked element pairs from an iterable.</p> <p>Creates pairs of successive elements from the input iterable, optionally including None as the final pair element.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>The input iterable to process</p> required <code>None_tail</code> <code>bool</code> <p>If True, includes (last_element, None) as final pair</p> <code>True</code> <code>reversed</code> <code>bool</code> <p>If True, yields pairs in reverse order</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator</code> <p>Yields tuples of paired elements</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/utils/itertools.py</code> <pre><code>def pairwise(iterable, None_tail=True, reversed=False) -&gt; Generator:\n    \"\"\"\n    Generator of linked element pairs from an iterable.\n\n    Creates pairs of successive elements from the input iterable,\n    optionally including None as the final pair element.\n\n    Parameters\n    ----------\n    iterable : Iterable\n        The input iterable to process\n    None_tail : bool, default=True\n        If True, includes (last_element, None) as final pair\n    reversed : bool, default=False\n        If True, yields pairs in reverse order\n\n    Returns\n    -------\n    Generator\n        Yields tuples of paired elements\n    \"\"\"\n    if not reversed:\n        a,b = tee(iterable)\n        next(b,None)\n        return zip_longest(a,b, fillvalue=None) if None_tail else zip(a,b)\n    else: return _pairwise_linked_reversed(iterable,None_tail)\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/utils/itertools/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.utils.itertools.pairwise_iterator","title":"<code>pairwise_iterator(iterable, None_tail=True, reversed=False)</code>","text":"<p>Generator that yields non-overlapping pairs from an iterable.</p> <p>Unlike pairwise(), this generates distinct pairs without overlap between successive pairs.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>The input iterable to process</p> required <code>None_tail</code> <code>bool</code> <p>If True, yields (last_element, None) for odd-length iterables</p> <code>True</code> <code>reversed</code> <code>bool</code> <p>If True, yields pairs in reverse order</p> <code>False</code> <p>Returns:</p> Type Description <code>Generator</code> <p>Yields tuples of paired elements</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/utils/itertools.py</code> <pre><code>def pairwise_iterator(iterable, None_tail=True, reversed=False) -&gt; 'Generator[Iterable, Iterable or None]':\n    \"\"\"\n    Generator that yields non-overlapping pairs from an iterable.\n\n    Unlike pairwise(), this generates distinct pairs without overlap\n    between successive pairs.\n\n    Parameters\n    ----------\n    iterable : Iterable\n        The input iterable to process\n    None_tail : bool, default=True\n        If True, yields (last_element, None) for odd-length iterables\n    reversed : bool, default=False\n        If True, yields pairs in reverse order\n\n    Returns\n    -------\n    Generator\n        Yields tuples of paired elements\n    \"\"\"\n    if not reversed:\n        iterable = iter(iterable)\n        while True:\n            try: a = next(iterable)\n            except StopIteration: return\n\n            try: yield a,next(iterable)\n            except StopIteration: \n                if None_tail: \n                    yield a, None \n                return\n    else:\n        curr_index = len(iterable)-1\n        while curr_index &gt; 1:\n            yield iterable[curr_index], iterable[curr_index-1]\n            curr_index -= 2\n        if None_tail and curr_index == 0:\n            yield iterable[0], None\n        return\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/utils/structures/","title":"structures","text":""},{"location":"codebase/EKEELVideoAnnotation/utils/structures/#structures","title":"Structures","text":""},{"location":"codebase/EKEELVideoAnnotation/utils/structures/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.utils.structures.LiFoStack","title":"<code>LiFoStack</code>","text":"<p>Last-In-First-Out (LIFO) stack implementation using linked nodes.</p> <p>A stack data structure that follows the LIFO principle where elements are added and removed from the same end.</p> <p>Attributes:</p> Name Type Description <code>_tail</code> <code>Node</code> <p>The tail node of the stack.</p> <code>_cursor</code> <code>Node</code> <p>Current position for iteration.</p> <code>_len</code> <code>int</code> <p>Number of elements in the stack.</p> <p>Methods:</p> Name Description <code>push</code> <p>Pushes an element onto the top of the stack.</p> <code>get</code> <p>Returns but does not remove the top element.</p> <code>pop</code> <p>Removes and returns the top element.</p> <code>is_head</code> <p>Checks if the stack is at the head position.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/utils/structures.py</code> <pre><code>class LiFoStack:\n    \"\"\"\n    Last-In-First-Out (LIFO) stack implementation using linked nodes.\n\n    A stack data structure that follows the LIFO principle where elements\n    are added and removed from the same end.\n\n    Attributes\n    ----------\n    _tail : Node\n        The tail node of the stack.\n    _cursor : Node\n        Current position for iteration.\n    _len : int\n        Number of elements in the stack.\n\n    Methods\n    -------\n    push(elem)\n        Pushes an element onto the top of the stack.\n    get(indx=None, raise_exception=False)\n        Returns but does not remove the top element.\n    pop(raise_exception=False)\n        Removes and returns the top element.\n    is_head(self)\n        Checks if the stack is at the head position.\n    \"\"\"\n    _tail = Node()\n    _cursor = _tail\n    _len = 0\n\n    def __init__(self,from_list:'list'= None) -&gt; None:\n        if from_list is not None: i=0; [ self.push(elem) for elem in from_list ]\n\n    def __str__(self):\n        cur = self._tail.get_prev()\n        out = \"\"\n        max_len = 0\n        # can be improved but for now is just for debug purposes\n        while cur is not None:\n            max_len = max(max_len, len(str(cur.value)))\n            out += \"| \" + str(cur.value) + \" |\\n\"\n            cur = cur.get_prev()\n        if out==\"\":\n            out= \"empty\"\n        return out + \"\u23bf\"+\"\u23bd\"*max_len+\"\u23bd\u23bd\u23bd\u23bd\"+\"\u23cc \"\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        node = self._cursor.get_prev()\n        if node is not None:\n            self._cursor = node\n            return node.value\n        else:\n            self._cursor = self._tail\n            raise StopIteration()\n\n    def __len__(self):\n        return self._len\n\n    def is_head(self):\n        return self._tail.get_prev() is None\n\n    def push(self,elem):\n        \"\"\"\n        Pushes an element onto the top of the stack.\n\n        Parameters\n        ----------\n        elem : any\n            The element to be pushed onto the stack.\n        \"\"\"\n        curr_tail = self._tail\n        curr_tail.value = elem\n        self._tail = Node().set_prev(curr_tail)\n        self._cursor = self._tail\n        self._len += 1\n\n    def get(self,indx=None,raise_exception=False):\n        \"\"\"\n        Returns but does not remove the top element of the stack.\n\n        Parameters\n        ----------\n        indx : int, optional\n            Currently unused parameter.\n        raise_exception : bool, default=False\n            If True, raises an exception when stack is empty.\n\n        Returns\n        -------\n        any or None\n            The top element of the stack, or None if stack is empty\n            and raise_exception is False.\n\n        Raises\n        ------\n        Exception\n            If the stack is empty and raise_exception is True.\n        \"\"\"\n        tail = self._tail\n        if tail.get_prev() is None:\n            if raise_exception:\n                raise Exception(\"Popping from an empty stack\")\n            else:\n                return None\n        return tail.get_prev().value\n\n    def pop(self,raise_exception=False):\n        \"\"\"\n        Removes and returns the top element of the stack.\n\n        Parameters\n        ----------\n        raise_exception : bool, default=False\n            If True, raises an exception when stack is empty.\n\n        Returns\n        -------\n        any or None\n            The top element of the stack, or None if stack is empty\n            and raise_exception is False.\n        \"\"\"\n        value = self.get(raise_exception)\n        if value is not None:\n            self._tail = self._tail.get_prev()\n            self._len -= 1\n        return value\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/utils/structures/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.utils.structures.LiFoStack.get","title":"<code>get(indx=None, raise_exception=False)</code>","text":"<p>Returns but does not remove the top element of the stack.</p> <p>Parameters:</p> Name Type Description Default <code>indx</code> <code>int</code> <p>Currently unused parameter.</p> <code>None</code> <code>raise_exception</code> <code>bool</code> <p>If True, raises an exception when stack is empty.</p> <code>False</code> <p>Returns:</p> Type Description <code>any or None</code> <p>The top element of the stack, or None if stack is empty and raise_exception is False.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the stack is empty and raise_exception is True.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/utils/structures.py</code> <pre><code>def get(self,indx=None,raise_exception=False):\n    \"\"\"\n    Returns but does not remove the top element of the stack.\n\n    Parameters\n    ----------\n    indx : int, optional\n        Currently unused parameter.\n    raise_exception : bool, default=False\n        If True, raises an exception when stack is empty.\n\n    Returns\n    -------\n    any or None\n        The top element of the stack, or None if stack is empty\n        and raise_exception is False.\n\n    Raises\n    ------\n    Exception\n        If the stack is empty and raise_exception is True.\n    \"\"\"\n    tail = self._tail\n    if tail.get_prev() is None:\n        if raise_exception:\n            raise Exception(\"Popping from an empty stack\")\n        else:\n            return None\n    return tail.get_prev().value\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/utils/structures/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.utils.structures.LiFoStack.pop","title":"<code>pop(raise_exception=False)</code>","text":"<p>Removes and returns the top element of the stack.</p> <p>Parameters:</p> Name Type Description Default <code>raise_exception</code> <code>bool</code> <p>If True, raises an exception when stack is empty.</p> <code>False</code> <p>Returns:</p> Type Description <code>any or None</code> <p>The top element of the stack, or None if stack is empty and raise_exception is False.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/utils/structures.py</code> <pre><code>def pop(self,raise_exception=False):\n    \"\"\"\n    Removes and returns the top element of the stack.\n\n    Parameters\n    ----------\n    raise_exception : bool, default=False\n        If True, raises an exception when stack is empty.\n\n    Returns\n    -------\n    any or None\n        The top element of the stack, or None if stack is empty\n        and raise_exception is False.\n    \"\"\"\n    value = self.get(raise_exception)\n    if value is not None:\n        self._tail = self._tail.get_prev()\n        self._len -= 1\n    return value\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/utils/structures/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.utils.structures.LiFoStack.push","title":"<code>push(elem)</code>","text":"<p>Pushes an element onto the top of the stack.</p> <p>Parameters:</p> Name Type Description Default <code>elem</code> <code>any</code> <p>The element to be pushed onto the stack.</p> required Source code in <code>EVA_apps/EKEELVideoAnnotation/utils/structures.py</code> <pre><code>def push(self,elem):\n    \"\"\"\n    Pushes an element onto the top of the stack.\n\n    Parameters\n    ----------\n    elem : any\n        The element to be pushed onto the stack.\n    \"\"\"\n    curr_tail = self._tail\n    curr_tail.value = elem\n    self._tail = Node().set_prev(curr_tail)\n    self._cursor = self._tail\n    self._len += 1\n</code></pre>"},{"location":"codebase/EKEELVideoAnnotation/utils/structures/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAnnotation.utils.structures.Node","title":"<code>Node</code>","text":"<p>A node class for implementing linked data structures.</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>any</code> <p>The value stored in the node.</p> <code>_prev</code> <code>Node or None</code> <p>Reference to the previous node in the structure.</p> <p>Methods:</p> Name Description <code>set_prev</code> <p>Sets the previous node reference and returns self.</p> <code>get_prev</code> <p>Returns the previous node reference.</p> Source code in <code>EVA_apps/EKEELVideoAnnotation/utils/structures.py</code> <pre><code>class Node:\n    \"\"\"\n    A node class for implementing linked data structures.\n\n    Attributes\n    ----------\n    value : any\n        The value stored in the node.\n    _prev : Node or None\n        Reference to the previous node in the structure.\n\n    Methods\n    -------\n    set_prev(prev)\n        Sets the previous node reference and returns self.\n    get_prev()\n        Returns the previous node reference.\n    \"\"\"\n    def __init__(self,value=None):\n        self.value = value\n        self._prev:Node or None = None\n\n    def set_prev(self,prev) -&gt; 'Node':\n        self._prev = prev \n        return self\n\n    def get_prev(self) -&gt; 'Node or None':\n        return self._prev\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/","title":"data","text":""},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#data","title":"Data","text":"<p>Data handling module for the Flask server.</p> <p>This module provides functions to interact with the MongoDB database, retrieve and process data related to video annotations and concepts.</p> <p>Functions:</p> Name Description <code>load_db</code> <p>Connects to the MongoDB database and returns the database object.</p> <code>delete_graphs</code> <p>Deletes all graph documents associated with the given email.</p> <code>get_conll</code> <p>Retrieves the CoNLL data for the specified video ID.</p> <code>get_sentences</code> <p>Extracts sentences from the parsed CoNLL data between the specified start and end IDs.</p> <code>format_datetime</code> <p>Formats a datetime string by removing the type annotation.</p> <code>get_concept_list</code> <p>Retrieves a list of concepts for the specified annotator and video ID.</p> <code>get_concept_map</code> <p>Retrieves a concept map for the specified annotator and video ID.</p> <code>get_concept_vocabulary</code> <p>Retrieves the concept vocabulary for the specified annotator and video ID.</p> <code>get_concept_instants</code> <p>Retrieves concept instants for the specified annotator and video ID.</p> <code>get_concept_targets</code> <p>Retrieves the target concepts for the specified concept ID.</p> <code>get_concept_prerequisites</code> <p>Retrieves the prerequisite concepts for the specified concept ID.</p> <code>build_concept_without_sub_graph</code> <p>Builds a concept object without subgraph information.</p> <code>build_concept_sub_graph_without_target_recursively</code> <p>Builds a subgraph for a concept recursively without target information.</p> <code>build_concept_sub_graph</code> <p>Builds a subgraph for a concept including target information.</p> <code>retrieve_primary_notions</code> <p>Retrieves primary notions from a concept instance.</p> <code>build_array</code> <p>Builds an array of concepts for the specified annotator and video ID.</p>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.build_array","title":"<code>build_array(annotator, video_id)</code>","text":"<p>Builds an array of concepts for the specified annotator and video ID.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>The ID of the annotator.</p> required <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of concepts.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def build_array(annotator,video_id):\n    \"\"\"\n    Builds an array of concepts for the specified annotator and video ID.\n\n    Parameters\n    ----------\n    annotator : str\n        The ID of the annotator.\n    video_id : str\n        The ID of the video.\n\n    Returns\n    -------\n    list\n        A list of concepts.\n    \"\"\"\n    concept_map = get_concept_map(annotator,video_id)\n    concept_instants = get_concept_instants(annotator,video_id)\n    primary_concept_list = get_concept_list(annotator,video_id)\n    parsed_conll = parse(get_conll(video_id))\n    conceptsList = []\n    for c in primary_concept_list:\n        conceptsList.append(build_concept_without_sub_graph(concept_instants,c[\"id\"]))\n    for c in conceptsList:\n        c[\"subgraph\"] =  build_concept_sub_graph(concept_map, concept_instants, c[\"conceptName\"])\n        c[\"subgraph\"][\"primary_notions\"] = retrieve_primary_notions(c)\n        for c_i in concept_instants:\n            if c_i[\"concept_id\"] == c[\"conceptName\"]:\n                c[\"description\"] = get_sentences(parsed_conll, c_i[\"start_sent_id\"], c_i[\"end_sent_id\"])\n\n    return conceptsList\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.build_concept_sub_graph","title":"<code>build_concept_sub_graph(concept_map, concept_instants, concept_id)</code>","text":"<p>Builds a subgraph for a concept including target information.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map</code> <code>list</code> <p>The concept map.</p> required <code>concept_instants</code> <code>list</code> <p>The list of concept instants.</p> required <code>concept_id</code> <code>str</code> <p>The ID of the concept.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the subgraph.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def build_concept_sub_graph(concept_map, concept_instants, concept_id):\n    \"\"\"\n    Builds a subgraph for a concept including target information.\n\n    Parameters\n    ----------\n    concept_map : list\n        The concept map.\n    concept_instants : list\n        The list of concept instants.\n    concept_id : str\n        The ID of the concept.\n\n    Returns\n    -------\n    dict\n        A dictionary representing the subgraph.\n    \"\"\"\n    sub_graph = {\"targets\": [], \"prerequisites\": [], \"primary_notions\": [] }\n    primary_targets = get_concept_targets(concept_map, concept_id)\n    for c in primary_targets:\n        sub_graph[\"targets\"].append(build_concept_without_sub_graph(concept_instants, c))\n\n    prerequisites = get_concept_prerequisites(concept_map, concept_id)\n    prerequisites_concept = []\n    for c in prerequisites:\n        c = build_concept_without_sub_graph(concept_instants, c)\n        prerequisites_concept.append(c)\n    sub_graph[\"prerequisites\"] = prerequisites_concept\n    for concept in sub_graph[\"prerequisites\"]:\n        concept[\"subgraph\"] = build_concept_sub_graph_without_target_recursively(concept_map,concept_instants, c[\"conceptName\"])\n\n    sub_graph[\"relations\"] = concept_map\n    return sub_graph\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.build_concept_sub_graph_without_target_recursively","title":"<code>build_concept_sub_graph_without_target_recursively(concept_map, concept_instants, concept_id)</code>","text":"<p>Builds a subgraph for a concept recursively without target information.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map</code> <code>list</code> <p>The concept map.</p> required <code>concept_instants</code> <code>list</code> <p>The list of concept instants.</p> required <code>concept_id</code> <code>str</code> <p>The ID of the concept.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the subgraph.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def build_concept_sub_graph_without_target_recursively(concept_map, concept_instants, concept_id):\n    \"\"\"\n    Builds a subgraph for a concept recursively without target information.\n\n    Parameters\n    ----------\n    concept_map : list\n        The concept map.\n    concept_instants : list\n        The list of concept instants.\n    concept_id : str\n        The ID of the concept.\n\n    Returns\n    -------\n    dict\n        A dictionary representing the subgraph.\n    \"\"\"\n    sub_graph = {\"targets\": [], \"prerequisites\": [], \"primary_notions\": []}\n    prerequisites = get_concept_prerequisites(concept_map, concept_id)\n    prerequisites_concept = []\n    for c in prerequisites:\n        concept = build_concept_without_sub_graph(concept_instants, c)\n        if concept not in prerequisites_concept:\n            prerequisites_concept.append(concept)\n    sub_graph[\"prerequisites\"] = prerequisites_concept\n    for c in sub_graph[\"prerequisites\"]:\n        c[\"subgraph\"] =  build_concept_sub_graph_without_target_recursively(concept_map,concept_instants, c[\"conceptName\"])\n    return sub_graph\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.build_concept_without_sub_graph","title":"<code>build_concept_without_sub_graph(concept_instants, concept_id)</code>","text":"<p>Builds a concept object without subgraph information.</p> <p>Parameters:</p> Name Type Description Default <code>concept_instants</code> <code>list</code> <p>The list of concept instants.</p> required <code>concept_id</code> <code>str</code> <p>The ID of the concept.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary representing the concept.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def build_concept_without_sub_graph(concept_instants,concept_id):\n    \"\"\"\n    Builds a concept object without subgraph information.\n\n    Parameters\n    ----------\n    concept_instants : list\n        The list of concept instants.\n    concept_id : str\n        The ID of the concept.\n\n    Returns\n    -------\n    dict\n        A dictionary representing the concept.\n    \"\"\"\n    concept = {\"conceptName\": \"\", \n                        \"type\": \"\", \n                        \"description\": \"\", \n                        \"startTimestamp\": \"\",\n                        \"endTimestamp\": \"\",\n                        \"image\": \"\",\n                        \"subgraph\": []}\n    concept[\"conceptName\"] = concept_id\n\n    for c in concept_instants:\n        if c[\"concept_id\"] == concept_id:\n            concept[\"startTimestamp\"] = c[\"start_time\"]\n            concept[\"endTimestamp\"] = c[\"end_time\"]\n    return concept\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.delete_graphs","title":"<code>delete_graphs(email)</code>","text":"<p>Deletes all graph documents associated with the given email.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>The email address associated with the graph documents to delete.</p> required <p>Returns:</p> Type Description <code>DeleteResult</code> <p>The result of the delete operation.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def delete_graphs(email):\n    \"\"\"\n    Deletes all graph documents associated with the given email.\n\n    Parameters\n    ----------\n    email : str\n        The email address associated with the graph documents to delete.\n\n    Returns\n    -------\n    pymongo.results.DeleteResult\n        The result of the delete operation.\n    \"\"\"\n    collection = db.graphs\n    if collection.find({\"email\":email}) is not None:\n        return collection.delete_many({\"email\":email})\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.format_datetime","title":"<code>format_datetime(str)</code>","text":"<p>Formats a datetime string by removing the type annotation.</p> <p>Parameters:</p> Name Type Description Default <code>str</code> <code>str</code> <p>The datetime string to format.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted datetime string.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def format_datetime(str):\n    \"\"\"\n    Formats a datetime string by removing the type annotation.\n\n    Parameters\n    ----------\n    str : str\n        The datetime string to format.\n\n    Returns\n    -------\n    str\n        The formatted datetime string.\n    \"\"\"\n    s = str.split(\"^^\")\n    return s[0]\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.get_concept_instants","title":"<code>get_concept_instants(annotator, video_id)</code>","text":"<p>Retrieves concept instants for the specified annotator and video ID.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>The ID of the annotator.</p> required <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of concept instants.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def get_concept_instants(annotator, video_id):\n    \"\"\"\n    Retrieves concept instants for the specified annotator and video ID.\n\n    Parameters\n    ----------\n    annotator : str\n        The ID of the annotator.\n    video_id : str\n        The ID of the video.\n\n    Returns\n    -------\n    list\n        A list of concept instants.\n    \"\"\"\n    pipeline = [\n        {\"$unwind\": \"$graph.@graph\"},\n        {\n            \"$match\":\n                {\n                    \"video_id\": str(video_id),\n                    \"annotator_id\": str(annotator),\n                    \"graph.@graph.type\": \"oa:annotation\",\n                    \"graph.@graph.motivation\": \"describing\",\n                }\n\n        },\n\n        {\"$project\":\n            {\n                \"concept_id\": \"$graph.@graph.body\",\n                \"start_time\":\"$graph.@graph.target.selector.startSelector.value\",\n                \"end_time\": \"$graph.@graph.target.selector.endSelector.value\",\n                \"start_sent_id\": \"$graph.@graph.target.selector.startSelector.edu:conllSentId\",\n                \"end_sent_id\":  \"$graph.@graph.target.selector.endSelector.edu:conllSentId\",\n            }\n        },\n\n\n        {\"$sort\": {\"time\": 1}}]\n\n    collection = db.graphs\n    aggregation = collection.aggregate(pipeline)\n    concept_instants = list(aggregation)\n    for c in concept_instants:\n        c[\"start_time\"] = format_datetime(c[\"start_time\"])\n        c[\"end_time\"] = format_datetime(c[\"end_time\"])\n        c[\"concept_id\"] = c[\"concept_id\"].replace(\"edu:\",\"\").replace(\"_\",\" \")\n    return concept_instants\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.get_concept_list","title":"<code>get_concept_list(annotator, video_id)</code>","text":"<p>Retrieves a list of concepts for the specified annotator and video ID.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>The ID of the annotator.</p> required <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of concepts.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def get_concept_list(annotator, video_id):\n    \"\"\"\n    Retrieves a list of concepts for the specified annotator and video ID.\n\n    Parameters\n    ----------\n    annotator : str\n        The ID of the annotator.\n    video_id : str\n        The ID of the video.\n\n    Returns\n    -------\n    list\n        A list of concepts.\n    \"\"\"\n    collection = db.graphs\n    pipeline = [\n        {\"$unwind\": \"$graph.@graph\"},\n        {\n            \"$match\":\n                {\n                    \"video_id\": str(video_id),\n                    \"annotator_id\": str(annotator),\n                    \"graph.@graph.type\": \"skos:Concept\",\n                }\n\n        },\n\n        {\"$project\":\n            {\n                \"id\": \"$graph.@graph.id\",\n                \"name\": \"$graph.@graph.id\"\n            }\n        },\n\n        {\"$sort\": {\"time\": 1}}\n\n    ]\n\n    aggregation = collection.aggregate(pipeline)\n    concept_list = list(aggregation)\n    for c in concept_list:\n        c[\"id\"] = c[\"id\"].replace(\"edu:\",\"\").replace(\"_\",\" \")\n\n    #get_concept_vocabulary(annotator, video_id)\n\n    return concept_list\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.get_concept_map","title":"<code>get_concept_map(annotator, video_id)</code>","text":"<p>Retrieves a concept map for the specified annotator and video ID.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>The ID of the annotator.</p> required <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list representing the concept map.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def get_concept_map(annotator,video_id):\n    \"\"\"\n    Retrieves a concept map for the specified annotator and video ID.\n\n    Parameters\n    ----------\n    annotator : str\n        The ID of the annotator.\n    video_id : str\n        The ID of the video.\n\n    Returns\n    -------\n    list\n        A list representing the concept map.\n    \"\"\"\n    collection = db.graphs\n\n    pipeline = [\n       {\"$unwind\": \"$graph.@graph\"},\n       {\n           \"$match\":\n               {\n                   \"video_id\": str(video_id),\n                   \"annotator_id\": str(annotator),\n                   \"graph.@graph.type\": \"oa:annotation\",\n                   \"graph.@graph.motivation\": \"edu:linkingPrerequisite\",\n               }\n       },\n\n        {\"$project\":\n            {\n                \"prerequisite\": \"$graph.@graph.body\",\n                \"target\": \"$graph.@graph.target.dcterms:subject.id\",\n                \"weight\": \"$graph.@graph.skos:note\",\n                \"time\": \"$graph.@graph.target.selector.value\",\n                \"sent_id\": \"$graph.@graph.target.selector.edu:conllSentId\",\n                \"word_id\": \"$graph.@graph.target.selector.edu:conllWordId\",\n                \"xywh\": \"$graph.@graph.target.selector.edu:hasMediaFrag\",\n                \"creator\": \"$graph.@graph.dcterms:creator\",\n                \"_id\": 0\n            }\n        },\n\n        {\"$sort\": {\"time\": 1}}\n\n    ]\n\n    aggregation = collection.aggregate(pipeline)\n    concept_map = list(aggregation)\n\n    for rel in concept_map:\n        rel[\"prerequisite\"] = rel[\"prerequisite\"].replace(\"edu:\",\"\").replace(\"_\",\" \")\n        rel[\"target\"] = rel[\"target\"].replace(\"edu:\",\"\").replace(\"_\",\" \")\n        rel[\"weight\"] = rel[\"weight\"].replace(\"Prerequisite\",\"\")\n        rel[\"time\"] = rel[\"time\"].replace(\"^^xsd:dateTime\",\"\")\n        if \"xywh\" not in rel:\n            rel[\"xywh\"] = \"None\"\n\n    return concept_map\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.get_concept_prerequisites","title":"<code>get_concept_prerequisites(concept_map, concept_id)</code>","text":"<p>Retrieves the prerequisite concepts for the specified concept ID.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map</code> <code>list</code> <p>The concept map.</p> required <code>concept_id</code> <code>str</code> <p>The ID of the concept.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of prerequisite concepts.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def get_concept_prerequisites(concept_map, concept_id):\n    \"\"\"\n    Retrieves the prerequisite concepts for the specified concept ID.\n\n    Parameters\n    ----------\n    concept_map : list\n        The concept map.\n    concept_id : str\n        The ID of the concept.\n\n    Returns\n    -------\n    list\n        A list of prerequisite concepts.\n    \"\"\"\n    prerequisites = []\n    for relation in concept_map:\n        if relation[\"target\"] == concept_id:\n            prerequisites.append(relation[\"prerequisite\"])\n    return prerequisites\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.get_concept_targets","title":"<code>get_concept_targets(concept_map, concept_id)</code>","text":"<p>Retrieves the target concepts for the specified concept ID.</p> <p>Parameters:</p> Name Type Description Default <code>concept_map</code> <code>list</code> <p>The concept map.</p> required <code>concept_id</code> <code>str</code> <p>The ID of the concept.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of target concepts.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def get_concept_targets(concept_map, concept_id):\n    \"\"\"\n    Retrieves the target concepts for the specified concept ID.\n\n    Parameters\n    ----------\n    concept_map : list\n        The concept map.\n    concept_id : str\n        The ID of the concept.\n\n    Returns\n    -------\n    list\n        A list of target concepts.\n    \"\"\"\n    targets = []\n    for relation in concept_map:\n        if relation[\"prerequisite\"] == concept_id:\n            targets.append(relation[\"target\"])\n    return targets\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.get_concept_vocabulary","title":"<code>get_concept_vocabulary(annotator, video_id)</code>","text":"<p>Retrieves the concept vocabulary for the specified annotator and video ID.</p> <p>Parameters:</p> Name Type Description Default <code>annotator</code> <code>str</code> <p>The ID of the annotator.</p> required <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <p>Returns:</p> Type Description <code>dict or None</code> <p>A dictionary representing the concept vocabulary, or None if not found.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def get_concept_vocabulary(annotator, video_id):\n    \"\"\"\n    Retrieves the concept vocabulary for the specified annotator and video ID.\n\n    Parameters\n    ----------\n    annotator : str\n        The ID of the annotator.\n    video_id : str\n        The ID of the video.\n\n    Returns\n    -------\n    dict or None\n        A dictionary representing the concept vocabulary, or None if not found.\n    \"\"\"\n    collection = db.graphs\n\n    pipeline = [\n        {\"$unwind\": \"$conceptVocabulary.@graph\"},\n        {\n            \"$match\":\n                {\n                    \"video_id\": str(video_id),\n                    \"annotator_id\": str(annotator),\n                    \"conceptVocabulary.@graph.type\": \"skos:Concept\"\n                }\n        },\n\n        {\"$project\":\n            {\n                \"prefLabel\": \"$conceptVocabulary.@graph.skos:prefLabel.@value\",\n                \"altLabel\": \"$conceptVocabulary.@graph.skos:altLabel.@value\",\n                \"_id\": 0\n            }\n        }\n\n    ]\n\n    aggregation = collection.aggregate(pipeline)\n    results = list(aggregation)\n\n    # define new concept vocabulary\n    conceptVocabulary = {}\n\n    # if there is none on DB\n    if len(results) == 0:\n        print(conceptVocabulary)\n        return None\n\n    # iterate for each concept and build the vocabulary basing on the number of synonyms\n    for concept in results: \n\n        if \"altLabel\" in concept :\n            if isinstance(concept[\"altLabel\"], list):\n                conceptVocabulary[concept[\"prefLabel\"]] = concept[\"altLabel\"]\n            else:\n                conceptVocabulary[concept[\"prefLabel\"]] = [concept[\"altLabel\"]]\n        else:\n            conceptVocabulary[concept[\"prefLabel\"]]=[]\n\n    #print(conceptVocabulary)\n\n    return conceptVocabulary\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.get_conll","title":"<code>get_conll(video_id)</code>","text":"<p>Retrieves the CoNLL data for the specified video ID.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <p>Returns:</p> Type Description <code>str or None</code> <p>The CoNLL data if found, otherwise None.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def get_conll(video_id):\n    \"\"\"\n    Retrieves the CoNLL data for the specified video ID.\n\n    Parameters\n    ----------\n    video_id : str\n        The ID of the video.\n\n    Returns\n    -------\n    str or None\n        The CoNLL data if found, otherwise None.\n    \"\"\"\n    collection = db.conlls\n    if collection.find_one({\"video_id\":video_id}) is not None:\n        return collection.find_one({\"video_id\":video_id})[\"conll\"]\n    else:\n        return None\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.get_sentences","title":"<code>get_sentences(parsed_conll, start_id, end_id)</code>","text":"<p>Extracts sentences from the parsed CoNLL data between the specified start and end IDs.</p> <p>Parameters:</p> Name Type Description Default <code>parsed_conll</code> <code>list</code> <p>The parsed CoNLL data.</p> required <code>start_id</code> <code>int</code> <p>The starting sentence ID.</p> required <code>end_id</code> <code>int</code> <p>The ending sentence ID.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted sentences.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def get_sentences(parsed_conll,start_id,end_id):\n    \"\"\"\n    Extracts sentences from the parsed CoNLL data between the specified start and end IDs.\n\n    Parameters\n    ----------\n    parsed_conll : list\n        The parsed CoNLL data.\n    start_id : int\n        The starting sentence ID.\n    end_id : int\n        The ending sentence ID.\n\n    Returns\n    -------\n    str\n        The extracted sentences.\n    \"\"\"\n    sentences = \"\"\n    start_id = int(start_id)\n    end_id = int(end_id)\n    # print(start_id)\n    # print(end_id)\n    for i in range(start_id, end_id):\n        for k in range(0,len(parsed_conll[i])):\n            sentences += parsed_conll[i][k][\"lemma\"] +\" \"\n    return sentences\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.load_db","title":"<code>load_db()</code>","text":"<p>Connects to the MongoDB database and returns the database object.</p> <p>Returns:</p> Name Type Description <code>db</code> <code>Database</code> <p>The MongoDB database object.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def load_db():\n    \"\"\"\n    Connects to the MongoDB database and returns the database object.\n\n    Returns\n    -------\n    db : pymongo.database.Database\n        The MongoDB database object.\n    \"\"\"\n    client = pymongo.MongoClient(\"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/edurell?retryWrites=true&amp;w=majority\")\n    db = client.edurell\n    return db\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.data.retrieve_primary_notions","title":"<code>retrieve_primary_notions(concept_instance)</code>","text":"<p>Retrieves primary notions from a concept instance.</p> <p>Parameters:</p> Name Type Description Default <code>concept_instance</code> <code>dict</code> <p>The concept instance.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of primary notions.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/data.py</code> <pre><code>def retrieve_primary_notions(concept_instance):\n    \"\"\"\n    Retrieves primary notions from a concept instance.\n\n    Parameters\n    ----------\n    concept_instance : dict\n        The concept instance.\n\n    Returns\n    -------\n    list\n        A list of primary notions.\n    \"\"\"\n    primary_notions = []\n    for c in concept_instance[\"subgraph\"][\"prerequisites\"]:\n        if c[\"subgraph\"][\"prerequisites\"] == []:\n            primary_notions.append(c)\n        else:\n            primary_notions = primary_notions + retrieve_primary_notions(c)\n    return primary_notions\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/environment/","title":"environment","text":""},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/environment/#environment","title":"Environment","text":"<p>Environment configuration module.</p> <p>This module loads environment variables from a <code>.env</code> file and provides access to these variables.</p> <p>Attributes:</p> Name Type Description <code>MONGO_CLUSTER_USERNAME</code> <code>str</code> <p>Username for the MongoDB cluster.</p> <code>MONGO_CLUSTER_PASSWORD</code> <code>str</code> <p>Password for the MongoDB cluster.</p> <code>EMAIL_ACCOUNT</code> <code>str</code> <p>Email account used for sending emails.</p> <code>EMAIL_PASSWORD</code> <code>str</code> <p>Password for the email account.</p> <code>APP_SECRET_KEY</code> <code>str</code> <p>Secret key used for application security.</p> <code>APP_SECURITY_PASSWORD_SALT</code> <code>str</code> <p>Salt used for password hashing.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the <code>.env</code> file is missing in the EVA_apps directory.</p>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/handle_data/","title":"handle_data","text":""},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/handle_data/#handle-data","title":"Handle Data","text":"<p>Handle data module for the Flask server.</p> <p>This module provides functions to interact with the MongoDB database and retrieve data related to video annotations.</p> <p>Functions:</p> Name Description <code>get_graphs</code> <p>Retrieves all graphs associated with the given video ID.</p> <code>check_graphs</code> <p>Checks if there are any graphs associated with the given video ID and email.</p> <code>get_definitions_fragments</code> <p>Retrieves definitions for the specified email, video ID, and fragments.</p>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/handle_data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.handle_data.check_graphs","title":"<code>check_graphs(video_id, email)</code>","text":"<p>Checks if there are any graphs associated with the given video ID and email.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <code>email</code> <code>str</code> <p>The email address associated with the graphs.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing annotator ID and video ID.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/handle_data.py</code> <pre><code>def check_graphs(video_id, email):\n    \"\"\"\n    Checks if there are any graphs associated with the given video ID and email.\n\n    Parameters\n    ----------\n    video_id : str\n        The ID of the video.\n    email : str\n        The email address associated with the graphs.\n\n    Returns\n    -------\n    list\n        A list of dictionaries containing annotator ID and video ID.\n    \"\"\"\n    db = data.load_db()\n    collection = db.graphs\n    q = collection.find({\"video_id\":video_id,\"email\":email})\n    res = []\n    for graph in q:\n        res.append({\"annotator_id\": graph[\"annotator_id\"], \"video_id\": video_id})\n    return res\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/handle_data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.handle_data.get_definitions_fragments","title":"<code>get_definitions_fragments(email, video_id, fragments)</code>","text":"<p>Retrieves definitions for the specified email, video ID, and fragments.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>The email address associated with the definitions.</p> required <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <code>fragments</code> <code>list</code> <p>A list of fragments with start and end times.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of definitions for the specified fragments.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/handle_data.py</code> <pre><code>def get_definitions_fragments(email, video_id, fragments):\n    \"\"\"\n    Retrieves definitions for the specified email, video ID, and fragments.\n\n    Parameters\n    ----------\n    email : str\n        The email address associated with the definitions.\n    video_id : str\n        The ID of the video.\n    fragments : list\n        A list of fragments with start and end times.\n\n    Returns\n    -------\n    list\n        A list of definitions for the specified fragments.\n    \"\"\"\n    db = data.load_db()\n    collection = db.graphs\n    print(email, video_id)\n    defs = []\n\n\n\n    pipeline = [\n        {\"$unwind\": \"$graph.@graph\"},\n        {\n            \"$match\":\n                {\n                    \"video_id\": str(video_id),\n                    \"email\": str(email),\n                    \"graph.@graph.type\": \"oa:annotation\",\n                    \"graph.@graph.motivation\": \"describing\",\n                    \"graph.@graph.skos:note\": \"Definition\",\n\n\n                }\n\n        },\n\n        {\"$project\":\n            {\n                \"concept\": \"$graph.@graph.body\",\n                \"start\": \"$graph.@graph.target.selector.startSelector.value\",\n                \"end\": \"$graph.@graph.target.selector.endSelector.value\",\n                \"_id\": 0\n            }\n        },\n\n        {\"$sort\": {\"start\": 1}}\n\n    ]\n\n    aggregation = collection.aggregate(pipeline)\n    definitions = list(aggregation)\n\n\n\n    for d in definitions:\n        d[\"concept\"] = d[\"concept\"].replace(\"edu:\", \"\").replace(\"_\", \" \") \n        d[\"end\"] = d[\"end\"].replace(\"^^xsd:dateTime\",\"\")\n        d[\"start\"] = d[\"start\"].replace(\"^^xsd:dateTime\", \"\")\n\n\n\n    if fragments is not None:\n        for f in fragments:\n\n            start_time = f['start']\n            end_time = f['end']\n\n            concepts = \"\"\n            added = []\n\n            for d in definitions:   \n                if d[\"start\"] &lt; end_time and d[\"start\"] &gt; start_time and d[\"concept\"] not in added:\n                    concepts += d[\"concept\"] + \",\"\n                    added.append(d[\"concept\"])\n\n\n            defs.append(concepts[0:-1])\n\n\n    return defs\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/handle_data/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.handle_data.get_graphs","title":"<code>get_graphs(video_id)</code>","text":"<p>Retrieves all graphs associated with the given video ID.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>The ID of the video.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries containing annotator ID and video ID.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/handle_data.py</code> <pre><code>def get_graphs(video_id):\n    \"\"\"\n    Retrieves all graphs associated with the given video ID.\n\n    Parameters\n    ----------\n    video_id : str\n        The ID of the video.\n\n    Returns\n    -------\n    list\n        A list of dictionaries containing annotator ID and video ID.\n    \"\"\"\n    db = data.load_db()\n    collection = db.graphs\n    q = collection.find({\"video_id\":video_id})\n    res = []\n    for graph in q:\n        res.append({\"annotator_id\": graph[\"annotator_id\"], \"video_id\": video_id})\n    return res\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/","title":"main","text":""},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#main","title":"Main","text":""},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.Fragment","title":"<code>Fragment</code>","text":"<p>               Bases: <code>EmbeddedDocument</code></p> <p>Embedded document for video fragments.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the fragment</p> <code>start</code> <code>str</code> <p>Start time of the fragment</p> <code>end</code> <code>str</code> <p>End time of the fragment</p> <code>progress</code> <code>int</code> <p>Progress of the fragment</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class Fragment(db.EmbeddedDocument):\n    \"\"\"\n    Embedded document for video fragments.\n\n    Attributes\n    ----------\n    name : str\n        Name of the fragment\n    start : str\n        Start time of the fragment\n    end : str\n        End time of the fragment\n    progress : int\n        Progress of the fragment\n    \"\"\"\n    name = db.StringField()\n    start = db.StringField()\n    end =  db.StringField()\n    progress = db.IntField()\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.Graphs","title":"<code>Graphs</code>","text":"<p>               Bases: <code>Document</code></p> <p>Document for graph data.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>str</code> <p>Identifier for the video</p> <code>annotator_name</code> <code>str</code> <p>Name of the annotator</p> <code>annotator_id</code> <code>str</code> <p>Identifier for the annotator</p> <code>email</code> <code>str</code> <p>Email of the annotator</p> <code>graph</code> <code>dict</code> <p>Graph data</p> <code>conceptVocabulary</code> <code>dict</code> <p>Concept vocabulary data</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class Graphs(db.Document):\n    \"\"\"\n    Document for graph data.\n\n    Attributes\n    ----------\n    video_id : str\n        Identifier for the video\n    annotator_name : str\n        Name of the annotator\n    annotator_id : str\n        Identifier for the annotator\n    email : str\n        Email of the annotator\n    graph : dict\n        Graph data\n    conceptVocabulary : dict\n        Concept vocabulary data\n    \"\"\"\n    video_id = db.StringField()\n    annotator_name = db.StringField()\n    annotator_id = db.StringField()\n    email = db.StringField()\n    graph = db.DynamicField()\n    conceptVocabulary = db.DynamicField()\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.HistoryLog","title":"<code>HistoryLog</code>","text":"<p>               Bases: <code>EmbeddedDocument</code></p> <p>Embedded document for history logs.</p> <p>Attributes:</p> Name Type Description <code>date</code> <code>datetime</code> <p>Date of the log entry</p> <code>request</code> <code>str</code> <p>Request content</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class HistoryLog(db.EmbeddedDocument):\n    \"\"\"\n    Embedded document for history logs.\n\n    Attributes\n    ----------\n    date : datetime\n        Date of the log entry\n    request : str\n        Request content\n    \"\"\"\n    date = db.DateTimeField()\n    request = db.StringField()\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.Student","title":"<code>Student</code>","text":"<p>               Bases: <code>Document</code></p> <p>Document for student data.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>First name of the student</p> <code>surname</code> <code>str</code> <p>Last name of the student</p> <code>email</code> <code>str</code> <p>Email address of the student</p> <code>password_hash</code> <code>str</code> <p>Hashed password of the student</p> <code>code_reset_password</code> <code>str</code> <p>Code for resetting password</p> <code>nb_try_code_reset_password</code> <code>int</code> <p>Number of attempts for resetting password</p> <code>code_delete_account</code> <code>str</code> <p>Code for deleting account</p> <code>nb_try_code_delete_account</code> <code>int</code> <p>Number of attempts for deleting account</p> <code>video_history_list</code> <code>list</code> <p>List of VideoHistory embedded documents</p> <p>Methods:</p> Name Description <code>hash_password</code> <p>Hash the student's password</p> <code>verify_password</code> <p>Verify the student's password</p> <code>hash_code_reset_password</code> <p>Hash the code for resetting password</p> <code>verify_code_reset_password</code> <p>Verify the code for resetting password</p> <code>hash_code_delete_account</code> <p>Hash the code for deleting account</p> <code>verify_code_delete_account</code> <p>Verify the code for deleting account</p> <code>generate_auth_token</code> <p>Generate an authentication token</p> <code>verify_auth_token</code> <p>Verify an authentication token</p> <code>get_sorted_history</code> <p>Get sorted video history</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class Student(db.Document):\n    \"\"\"\n    Document for student data.\n\n    Attributes\n    ----------\n    name : str\n        First name of the student\n    surname : str\n        Last name of the student\n    email : str\n        Email address of the student\n    password_hash : str\n        Hashed password of the student\n    code_reset_password : str\n        Code for resetting password\n    nb_try_code_reset_password : int\n        Number of attempts for resetting password\n    code_delete_account : str\n        Code for deleting account\n    nb_try_code_delete_account : int\n        Number of attempts for deleting account\n    video_history_list : list\n        List of VideoHistory embedded documents\n\n    Methods\n    -------\n    hash_password(password)\n        Hash the student's password\n    verify_password(password)\n        Verify the student's password\n    hash_code_reset_password(code)\n        Hash the code for resetting password\n    verify_code_reset_password(code)\n        Verify the code for resetting password\n    hash_code_delete_account(code)\n        Hash the code for deleting account\n    verify_code_delete_account(code)\n        Verify the code for deleting account\n    generate_auth_token(expires_in)\n        Generate an authentication token\n    verify_auth_token(token)\n        Verify an authentication token\n    get_sorted_history(by)\n        Get sorted video history\n    \"\"\"\n    name = db.StringField()\n    surname = db.StringField()\n    email = db.StringField()\n    password_hash = db.StringField()\n\n    code_reset_password = db.StringField()\n    nb_try_code_reset_password = db.IntField()\n\n    code_delete_account = db.StringField()\n    nb_try_code_delete_account = db.IntField()\n\n    video_history_list:list = db.EmbeddedDocumentListField(VideoHistory)\n\n    def hash_password(self, password):\n        hashed_pwd = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n        self.password_hash = hashed_pwd.decode('utf8')\n        self.save()\n\n    def verify_password(self, password):\n        return bcrypt.checkpw(password.encode('utf-8'), self.password_hash.encode('utf-8'))\n\n\n\n    def hash_code_reset_password(self, code):\n        hashed_code = bcrypt.hashpw(code.encode('utf-8'), bcrypt.gensalt())\n        self.code_reset_password = hashed_code.decode('utf8')\n        self.save()\n\n    def verify_code_reset_password(self, code):\n        nb_try_code_reset_password_to_increment = self.nb_try_code_reset_password\n        if nb_try_code_reset_password_to_increment is None:\n            nb_try_code_reset_password_to_increment=0\n        self.nb_try_code_reset_password = nb_try_code_reset_password_to_increment+ 1\n        self.save()\n        return bcrypt.checkpw(code.encode('utf-8'), self.code_reset_password.encode('utf-8'))\n\n\n\n    def hash_code_delete_account(self, code):\n        hashed_code = bcrypt.hashpw(code.encode('utf-8'), bcrypt.gensalt())\n        self.code_delete_account = hashed_code.decode('utf8')\n        self.save()\n\n    def verify_code_delete_account(self, code):\n        nb_try_code_delete_account_to_increment = self.nb_try_code_delete_account\n        if nb_try_code_delete_account_to_increment is None:\n            nb_try_code_delete_account_to_increment=0\n        self.nb_try_code_delete_account = nb_try_code_delete_account_to_increment + 1\n        self.save()\n        return bcrypt.checkpw(code.encode('utf-8'), self.code_delete_account.encode('utf-8'))\n\n\n\n    def generate_auth_token(self, expires_in=600):\n        #return jwt.encode(\n        #    {'email': self.email, 'exp': time.time() + expires_in},\n        #    app.config['SECRET_KEY'], algorithm='HS256')\n        return jwt.encode(\n            {'email': self.email},\n            app.config['SECRET_KEY'], algorithm='HS256')\n\n\n    @staticmethod\n    def verify_auth_token(token):\n        try:\n            data = jwt.decode(token, app.config['SECRET_KEY'],\n                              algorithms=['HS256'])\n        except:\n            return\n        return Student.objects(email=data['email']).first()\n\n\n    def get_sorted_history(self,by:str):\n        docs = self.video_history_list.copy()\n\n        if by == \"title\":\n            comp_funct = lambda video: get_video_title_from_url(video.video_url.split(\"watch?v=\")[1])\n        elif by == \"lastChange\":\n            comp_funct = lambda video: video.lastChangesDate\n\n        n = len(docs)\n        # optimize code, so if the array is already sorted, it doesn't need\n        # to go through the entire process\n        # Traverse through all array elements\n        for i in range(n):\n\n            # range(n) also work but outer loop will\n            # repeat one time more than needed.\n            # Last i elements are already in place\n            swapped = False\n            for j in range(0, n-i-1):\n\n                # traverse the array from 0 to n-i-1\n                # Swap if the element found is greater\n                # than the next element\n                if comp_funct(docs[j]) &gt; comp_funct(docs[j + 1]):\n                    swapped = True\n                    docs[j], docs[j + 1] = docs[j + 1], docs[j]\n\n            if not swapped:\n                # if we haven't needed to make a single swap, we\n                # can just exit the main loop.\n                return docs[::-1] if by == \"lastChange\" else docs\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.UnverifiedStudent","title":"<code>UnverifiedStudent</code>","text":"<p>               Bases: <code>Document</code></p> <p>Document for unverified student data.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>First name of the student</p> <code>surname</code> <code>str</code> <p>Last name of the student</p> <code>email</code> <code>str</code> <p>Email address of the student</p> <code>password_hash</code> <code>str</code> <p>Hashed password of the student</p> <code>code_on_creation_hash</code> <code>str</code> <p>Code for account creation</p> <code>nb_try_code_on_creation</code> <code>int</code> <p>Number of attempts for account creation</p> <p>Methods:</p> Name Description <code>hash_password</code> <p>Hash the student's password</p> <code>hash_code_on_creation</code> <p>Hash the code for account creation</p> <code>verify_code_on_creation</code> <p>Verify the code for account creation</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class UnverifiedStudent(db.Document):\n    \"\"\"\n    Document for unverified student data.\n\n    Attributes\n    ----------\n    name : str\n        First name of the student\n    surname : str\n        Last name of the student\n    email : str\n        Email address of the student\n    password_hash : str\n        Hashed password of the student\n    code_on_creation_hash : str\n        Code for account creation\n    nb_try_code_on_creation : int\n        Number of attempts for account creation\n\n    Methods\n    -------\n    hash_password(password)\n        Hash the student's password\n    hash_code_on_creation(code)\n        Hash the code for account creation\n    verify_code_on_creation(code)\n        Verify the code for account creation\n    \"\"\"\n    name = db.StringField()\n    surname = db.StringField()\n    email = db.StringField()\n    password_hash = db.StringField()\n\n    code_on_creation_hash = db.StringField()\n    nb_try_code_on_creation = db.IntField()\n\n\n    def hash_password(self, password):\n        hashed_pwd = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n        self.password_hash = hashed_pwd.decode('utf8')\n        self.save()\n\n    def hash_code_on_creation(self, code):\n        hashed_code = bcrypt.hashpw(code.encode('utf-8'), bcrypt.gensalt())\n        self.code_on_creation_hash = hashed_code.decode('utf8')\n        self.save()\n\n    def verify_code_on_creation(self, code):\n        nb_try_code_on_creation_to_increment = self.nb_try_code_on_creation\n        if nb_try_code_on_creation_to_increment is None:\n            nb_try_code_on_creation_to_increment=0\n        self.nb_try_code_on_creation = nb_try_code_on_creation_to_increment+ 1\n        self.save()\n        return bcrypt.checkpw(code.encode('utf-8'), self.code_on_creation_hash.encode('utf-8'))\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.VTStitles","title":"<code>VTStitles</code>","text":"<p>               Bases: <code>EmbeddedDocument</code></p> <p>Embedded document for video text segmentation titles.</p> <p>Attributes:</p> Name Type Description <code>start_end_seconds</code> <code>list</code> <p>List of start and end times in seconds</p> <code>text</code> <code>str</code> <p>Text content of the title</p> <code>xywh_normalized</code> <code>list</code> <p>List of normalized bounding box coordinates</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class VTStitles(db.EmbeddedDocument):\n    \"\"\"\n    Embedded document for video text segmentation titles.\n\n    Attributes\n    ----------\n    start_end_seconds : list\n        List of start and end times in seconds\n    text : str\n        Text content of the title\n    xywh_normalized : list\n        List of normalized bounding box coordinates\n    \"\"\"\n    start_end_seconds = db.ListField(db.DecimalField())\n    text = db.StringField()\n    xywh_normalized = db.ListField(db.DecimalField())\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.VideoHistory","title":"<code>VideoHistory</code>","text":"<p>               Bases: <code>EmbeddedDocument</code></p> <p>Embedded document for video history.</p> <p>Attributes:</p> Name Type Description <code>video_url</code> <code>str</code> <p>URL of the video</p> <code>video_watchtime</code> <code>int</code> <p>Watch time of the video</p> <code>fragment_clicks</code> <code>int</code> <p>Number of fragment clicks</p> <code>node_clicks</code> <code>int</code> <p>Number of node clicks</p> <code>transcript_clicks</code> <code>int</code> <p>Number of transcript clicks</p> <code>searchbar_clicks</code> <code>int</code> <p>Number of search bar clicks</p> <code>notes</code> <code>str</code> <p>Notes for the video</p> <code>lastChangesDate</code> <code>datetime</code> <p>Date of the last changes</p> <code>fragments_progress</code> <code>list</code> <p>List of Fragment embedded documents</p> <code>logs</code> <code>list</code> <p>List of HistoryLog embedded documents</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class VideoHistory(db.EmbeddedDocument):\n    \"\"\"\n    Embedded document for video history.\n\n    Attributes\n    ----------\n    video_url : str\n        URL of the video\n    video_watchtime : int\n        Watch time of the video\n    fragment_clicks : int\n        Number of fragment clicks\n    node_clicks : int\n        Number of node clicks\n    transcript_clicks : int\n        Number of transcript clicks\n    searchbar_clicks : int\n        Number of search bar clicks\n    notes : str\n        Notes for the video\n    lastChangesDate : datetime\n        Date of the last changes\n    fragments_progress : list\n        List of Fragment embedded documents\n    logs : list\n        List of HistoryLog embedded documents\n    \"\"\"\n    video_url = db.StringField()\n    video_watchtime = db.IntField()\n    fragment_clicks = db.IntField()\n    node_clicks = db.IntField()\n    transcript_clicks = db.IntField()\n    searchbar_clicks = db.IntField()\n    notes = db.StringField()\n    lastChangesDate = db.DateTimeField()\n    fragments_progress = db.EmbeddedDocumentListField(Fragment)\n    logs = db.EmbeddedDocumentListField(HistoryLog)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.VideoStatistics","title":"<code>VideoStatistics</code>","text":"<p>               Bases: <code>Document</code></p> <p>Document for video statistics.</p> <p>Attributes:</p> Name Type Description <code>video_url</code> <code>str</code> <p>URL of the video</p> <code>amountViewers</code> <code>int</code> <p>Number of viewers</p> <code>total_fragment_clicks</code> <code>int</code> <p>Total number of fragment clicks</p> <code>total_node_clicks</code> <code>int</code> <p>Total number of node clicks</p> <code>total_transcript_clicks</code> <code>int</code> <p>Total number of transcript clicks</p> <code>total_searchbar_clicks</code> <code>int</code> <p>Total number of search bar clicks</p> <code>viewersList</code> <code>list</code> <p>List of viewer emails</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class VideoStatistics(db.Document):\n    \"\"\"\n    Document for video statistics.\n\n    Attributes\n    ----------\n    video_url : str\n        URL of the video\n    amountViewers : int\n        Number of viewers\n    total_fragment_clicks : int\n        Total number of fragment clicks\n    total_node_clicks : int\n        Total number of node clicks\n    total_transcript_clicks : int\n        Total number of transcript clicks\n    total_searchbar_clicks : int\n        Total number of search bar clicks\n    viewersList : list\n        List of viewer emails\n    \"\"\"\n    video_url = db.StringField()\n    amountViewers = db.IntField()\n    total_fragment_clicks = db.IntField()\n    total_node_clicks = db.IntField()\n    total_transcript_clicks = db.IntField()\n    total_searchbar_clicks = db.IntField()\n    viewersList = db.ListField(db.StringField())\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.VideoTextSegmentation","title":"<code>VideoTextSegmentation</code>","text":"<p>               Bases: <code>Document</code></p> <p>Document for video text segmentation data.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>str</code> <p>Identifier for the video</p> <code>slides_percentage</code> <code>float</code> <p>Percentage of slides in the video</p> <code>slide_titles</code> <code>list</code> <p>List of VTStitles embedded documents</p> <code>slide_startends</code> <code>list</code> <p>List of start and end times for slides</p> <code>slidish_frames_startend</code> <code>list</code> <p>List of start and end frames for slides</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class VideoTextSegmentation(db.Document):\n    \"\"\"\n    Document for video text segmentation data.\n\n    Attributes\n    ----------\n    video_id : str\n        Identifier for the video\n    slides_percentage : float\n        Percentage of slides in the video\n    slide_titles : list\n        List of VTStitles embedded documents\n    slide_startends : list\n        List of start and end times for slides\n    slidish_frames_startend : list\n        List of start and end frames for slides\n    \"\"\"\n    video_id = db.StringField()\n    slides_percentage = db.DecimalField()\n    slide_titles = db.EmbeddedDocumentListField(VTStitles)\n    slide_startends = db.ListField(db.ListField(db.DecimalField()))\n    slidish_frames_startend = db.ListField(db.ListField(db.IntField()))\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.Videos","title":"<code>Videos</code>","text":"<p>               Bases: <code>Document</code></p> <p>Document for video metadata.</p> <p>Attributes:</p> Name Type Description <code>video_id</code> <code>str</code> <p>Identifier for the video</p> <code>title</code> <code>str</code> <p>Title of the video</p> <code>creator</code> <code>str</code> <p>Creator of the video</p> <code>duration</code> <code>str</code> <p>Duration of the video</p> <code>segment_starts</code> <code>list</code> <p>List of segment start times</p> <code>segment_ends</code> <code>list</code> <p>List of segment end times</p> <code>extracted_keywords</code> <code>list</code> <p>List of extracted keywords</p> <code>language</code> <code>str</code> <p>Language of the video</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>class Videos(db.Document):\n    \"\"\"\n    Document for video metadata.\n\n    Attributes\n    ----------\n    video_id : str\n        Identifier for the video\n    title : str\n        Title of the video\n    creator : str\n        Creator of the video\n    duration : str\n        Duration of the video\n    segment_starts : list\n        List of segment start times\n    segment_ends : list\n        List of segment end times\n    extracted_keywords : list\n        List of extracted keywords\n    language : str\n        Language of the video\n    \"\"\"\n    video_id = db.StringField()\n    title = db.StringField()\n    creator = db.StringField()\n    duration = db.StringField()\n    segment_starts = db.ListField(db.StringField())\n    segment_ends = db.ListField(db.StringField())\n    extracted_keywords = db.ListField(db.StringField())\n    language = db.StringField()\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.ConceptVideoData","title":"<code>ConceptVideoData(video_id_list, concept_searched)</code>","text":"<p>Get concept video data for a list of videos.</p> <p>Parameters:</p> Name Type Description Default <code>video_id_list</code> <code>str</code> <p>Comma-separated list of video IDs</p> required <code>concept_searched</code> <code>str</code> <p>Concept to search for</p> required <p>Returns:</p> Type Description <code>str</code> <p>JSON string containing concept video data</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/ConceptVideoData/&lt;video_id_list&gt;/&lt;concept_searched&gt;')\n@auth.login_required\ndef ConceptVideoData(video_id_list, concept_searched):\n    \"\"\"\n    Get concept video data for a list of videos.\n\n    Parameters\n    ----------\n    video_id_list : str\n        Comma-separated list of video IDs\n    concept_searched : str\n        Concept to search for\n\n    Returns\n    -------\n    str\n        JSON string containing concept video data\n    \"\"\"\n    #pymongo db config for query sparql\n    client = pymongo.MongoClient(\n            \"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/edurell?retryWrites=true&amp;w=majority\")\n    dbsparql = client.edurell\n    # retrieve from mongodb collection=graphs the all elements with the value of video_id\n    collection = dbsparql.graphs\n\n    videoList = video_id_list.split(',')\n    result_list=[]\n    for i in range(0,len(videoList)):\n        video_id = videoList[i]\n        # initialize the dicitonary where we save our results\n        result = {\n            'video_id':video_id,\n            'concept_starttime':[],\n            'concept_endtime':[],\n            'explain':[],\n            'list_preconcept': [],\n            'list_prenotes':[],\n            'list_postnotes':[],\n            'list_derivatedconcept':[],\n            'derivatedconcept_starttime':[],\n            'derivatedconcept_endtime':[]\n\n        }\n\n        #print(\"?????????????????????????????START??????????????????????????????????\")\n        #print(\"start query: \",video_id,\" \",concept_searched)\n        #print(\"result: \",result)\n\n        # two options:\n        ## 1. search newest annotation\n        ### 2. combine every annotation of a video into a unique graph and make the query of that\n        cursor = collection.find({\"video_id\":video_id})\n        optiongraph = 2\n        gr=Graph()\n\n\n        if optiongraph == 1:\n            ## 1. search newest annotation\n            ## search for the newest in term of timestamp and get the idx of the newest document\n            ## maybe it's pointless if the lastest is always the newest\n            ## but in case the order changes in the future, this piece of code will make the ode work right regardless\n            lastest = \"1970-01-01T00:00:00.000000Z\"\n            lastest_idx = 0\n            for idx,document in enumerate(cursor):\n                if document is None:\n                    continue\n                gr=Graph().parse(data=json.dumps(document[\"graph\"]), format='json-ld')\n                qr = \"\"\"\n                        PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                        PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                        PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                        PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n\n                        SELECT DISTINCT ?timestamp\n                        WHERE {\n                                ?who dcterms:created ?timestamp\n                            }\n\n                    \"\"\"\n                qres = gr.query(qr)\n                #print(\"query result: \",qres)\n                for row in qres:\n                #   print(\"current_timestamp: \",lastest,\" / selected_timestamp: \",row['timestamp'])\n                    if row['timestamp'] &gt; lastest:\n                        lastest = row['timestamp']\n                        lastest_idx = idx\n            #print(\"lastest_timestamp: \",lastest,\" lastest_idx: \",lastest_idx)\n            #print(\"TESTTTTTTTTT       \",cursor)\n\n\n            # select the newest document\n            document = collection.find({\"video_id\":video_id})[lastest_idx]\n\n\n            # Query the concept timeline and duration\n            gr.parse(data=json.dumps(document[\"graph\"]), format='json-ld')\n\n            # initialize conceptVocabulary for the query\n            gr.parse(data=json.dumps(document[\"conceptVocabulary\"]), format='json-ld')\n\n        else:\n            ### 2. combine all annotation of a video together\n            ### in mongodb we collect all annotation of a single video\n            ### made by different people or the same\n            ### this mode combine from the oldest to the newest into a single graph\n\n            #print(\"start cursor for: \",video_id)\n            for idx,document in enumerate(cursor):\n            #   print(\"document \",video_id,\" idx: \",idx,\" \\n\\nGRAPH: \",document[\"graph\"],\" \\n\\nCONCEPT:  \",document[\"conceptVocabulary\"],\" \\n\\n\")\n            #  print(\"\\n\")\n                # Query the concept timeline and duration\n                if document[\"graph\"]==\"\":\n            #     print(\"\\nIGNORE GRAPH: \")\n                    continue\n                if document[\"conceptVocabulary\"] == \"\":\n                #    print(\"\\nIGNORE CONCEPT: \")\n\n                    continue\n                gr.parse(data=json.dumps(document[\"graph\"]), format='json-ld')\n\n                # initialize conceptVocabulary for the query\n\n                gr.parse(data=json.dumps(document[\"conceptVocabulary\"]), format='json-ld')\n\n\n\n\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        ## ---------------------------------------------------------QUERYCREATED-----------------------------------------------------------------\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        qr = \"\"\"\n                    PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                    PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                    PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                    PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n                    PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\n\n                    SELECT DISTINCT ?created\n                    WHERE{\n                            ?who oa:motivatedBy oa:describing.\n                            ?who dcterms:created ?created.\n                            ?who a oa:Annotation.\n                            ?who oa:hasBody ?c_id.\n                            ?c_id skos:prefLabel ?c_selected.\n                    }\n        \"\"\"\n\n\n        qres = gr.query(qr, initBindings = {\"c_selected\":Literal(concept_searched, lang=\"en\")})\n        #print(\"query created\")\n        #print(\"qres: \",len(qres))\n        for row in qres:\n        #   print(\"_________________________________________\")\n        #  print(row['created'])\n            result['created']=row['created']\n\n        #print(result)\n\n\n\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        ## ---------------------------------------------------QUERYCONCEPTTIMELINE&amp;SKOSNOTE------------------------------------------------------\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        qr = \"\"\"\n                    PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                    PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                    PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                    PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n                    PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\n                    SELECT ?concept_starttime ?concept_endtime ?explain\n                    WHERE {\n\n                            ?who oa:hasBody ?c_id.\n                            ?c_id skos:prefLabel ?c_selected.\n                            ?who oa:motivatedBy oa:describing.\n                            ?who oa:hasTarget ?target.\n                            ?target oa:hasSelector ?selector.\n                            ?selector oa:hasStartSelector ?startselector.\n                            ?startselector rdf:value ?concept_starttime.\n                            ?selector oa:hasEndSelector ?endselector.\n                            ?endselector rdf:value ?concept_endtime.\n                            ?who skos:note ?explain               \n                        }\n\n\n                \"\"\"\n\n        qres = gr.query(qr, initBindings = {\"c_selected\":Literal(concept_searched, lang=\"en\")})\n\n        #print(\"qres: \",len(qres))\n        for row in qres:\n            #print(\"result: \",row['concept_starttime'],\" \",row['concept_endtime'])\n        #   print(\"_________________________________________\")\n        #  print(row['concept_starttime'],\"\\n\",row['concept_endtime'])\n\n            result['concept_starttime'].append(row['concept_starttime'])\n            result['concept_endtime'].append(row['concept_endtime'])\n            result['explain'].append(row['explain'])\n\n\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        ## ---------------------------------------------------------QUERYPRECONCEPTS-------------------------------------------------------------\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        ## preconcepts that aren't explained: no conceptDefinition nor conceptExpansion\n\n        qr = \"\"\"\n                    PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                    PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                    PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                    PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n                    PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\n\n                    SELECT DISTINCT ?preconcept ?prenote\n                    WHERE{\n                            ?who oa:hasBody ?preconceptIRI.\n                            ?c_id skos:prefLabel ?c_selected.\n                            ?who oa:motivatedBy edu:linkingPrerequisite.\n                            ?who oa:hasTarget ?target.\n                            ?target dcterms:subject ?c_id.\n                            ?preconceptIRI skos:prefLabel ?preconcept.\n                            ?who skos:note ?prenote.\n\n                    }\n\n\n        \"\"\"\n\n\n        qres = gr.query(qr, initBindings = {\"c_selected\":Literal(concept_searched, lang=\"en\")})\n        #print(\"preconcept\")\n        #print(\"qres: \",len(qres))\n        for row in qres:\n        #   print(\"a_________________________________________\")\n        #  print(row['preconcept'])\n            result['list_preconcept'].append(row['preconcept'])\n            result['list_prenotes'].append(row['prenote'])\n\n        #print(result)\n\n\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        ## ---------------------------------------------------------QUERYCONCEPTDERIVATED--------------------------------------------------------\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        qr = \"\"\"\n                    PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                    PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                    PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                    PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n                    PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\n\n                    SELECT DISTINCT ?c_derivated ?postnote\n                    WHERE{\n                            ?who oa:hasBody ?c_id.\n                            ?c_id skos:prefLabel ?c_selected.\n                            ?who oa:motivatedBy edu:linkingPrerequisite.\n                            ?who oa:hasTarget ?target.\n                            ?target dcterms:subject ?c_derivatedIRI.\n                            ?c_derivatedIRI skos:prefLabel ?c_derivated.\n                            ?who skos:note ?postnote\n                    }\n\n\n        \"\"\"\n        \"\"\"\n            'list_derivatedconcept':[],\n            'derivatedconcept_starttime':[],\n            'derivatedconcept_endtime':[]\n        \"\"\"\n        qres = gr.query(qr, initBindings = {\"c_selected\":Literal(concept_searched, lang=\"en\")})\n        #print(\"c_derivated\")\n        #print(\"qres: \",len(qres))\n        for row in qres:\n        #   print(\"b_________________________________________\")\n        #  print(row['c_derivated'])\n\n            result['list_derivatedconcept'].append(row['c_derivated'])\n            result['list_postnotes'].append(row['postnote'])\n\n\n\n\n\n\n        for dc in result['list_derivatedconcept']:\n            qr = \"\"\"\n                    PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                    PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                    PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                    PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n                    PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\n                    SELECT ?dc_starttime ?dc_endtime\n                    WHERE {\n\n                            ?who oa:hasBody ?c_id.\n                            ?c_id skos:prefLabel ?c_selected.\n                            ?who oa:motivatedBy oa:describing.\n                            ?who oa:hasTarget ?target.\n                            ?target oa:hasSelector ?selector.\n                            ?selector oa:hasStartSelector ?startselector.\n                            ?startselector rdf:value ?dc_starttime.\n                            ?selector oa:hasEndSelector ?endselector.\n                            ?endselector rdf:value ?dc_endtime.               \n                        }\n\n\n                \"\"\"\n\n            qres = gr.query(qr, initBindings = {\"c_selected\":Literal(dc, lang=\"en\")})\n            #print(\"qres: \",len(qres))\n            for row in qres:\n                #print(\"result: \",row['concept_starttime'],\" \",row['concept_endtime'])\n            #   print(\"_________________________________________\")\n            #  print(row['dc_starttime'],\"\\n\",row['dc_endtime'])\n                result['derivatedconcept_starttime'].append(row['dc_starttime'])\n                result['derivatedconcept_endtime'].append(row['dc_endtime'])\n\n\n\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        ## ---------------------------------------------------------SLIDISHNESS------------------------------------------------------------------\n        ## --------------------------------------------------------------------------------------------------------------------------------------\n        VTS=VideoTextSegmentation.objects(video_id = video_id)\n        for VTSdoc in VTS:\n        # print(VTSdoc.video_id)\n        # print(VTSdoc.slides_percentage)\n            result['slides_percentage'] = str(VTSdoc.slides_percentage)\n        result_list.append(result)\n\n\n    return json.dumps(result_list)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.GetVideoTypeAndPrerequisite","title":"<code>GetVideoTypeAndPrerequisite()</code>","text":"<p>Get video type and prerequisite information.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing video type and prerequisite information</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/GetVideoTypeAndPrerequisite')\n@auth.login_required\ndef GetVideoTypeAndPrerequisite():\n    \"\"\"\n    Get video type and prerequisite information.\n\n    Returns\n    -------\n    dict\n        Dictionary containing video type and prerequisite information\n    \"\"\"\n    client = pymongo.MongoClient(\n        \"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/edurell?retryWrites=true&amp;w=majority\")\n\n    # retrieve from mongodb collection=graphs the all elements with the value of video_id\n    db = client.edurell\n    collection = db.graphs\n\n    videos = Videos.objects()\n    result={}\n    for v in videos:\n        cursor = collection.find({\"video_id\":v.video_id})\n\n\n\n        for document in cursor:\n            #print(\"per ogni document in cursor: \",document[\"_id\"],\" \",document[\"conceptVocabulary\"])\n            #print(\"\\n asd \\n\")\n            if document[\"conceptVocabulary\"] == \"\":\n                print(\"NONE\")\n                continue\n\n            gr=Graph()\n            print(\"Generato grafo: \")\n            # Query the concept timeline and duration\n            gr.parse(data=json.dumps(document[\"graph\"]), format='json-ld')\n\n            # initialize conceptVocabulary for the query\n            gr.parse(data=json.dumps(document[\"conceptVocabulary\"]), format='json-ld')\n\n\n        qr=\"\"\"\n                PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n                PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt;\n\n                SELECT DISTINCT ?video_id ?prerequisite ?typedef\n                WHERE{\n                    ?id_descr oa:motivatedBy oa:describing.\n                    ?id_descr oa:hasTarget ?target_descr.\n                    ?target_descr oa:hasSource ?video_id.\n                    ?id_descr skos:note  ?typedef.\n\n                    ?id_pre oa:motivatedBy edu:linkingPrerequisite.\n                    ?id_pre oa:hasTarget ?target_link.\n                    ?target_link oa:hasSource ?video_id.\n                    ?id_pre oa:hasBody ?prerequisiteIRI.\n                    ?prerequisiteIRI skos:prefLabel ?prerequisite.\n\n\n                }\n\n        \"\"\"\n        print(\"applico la query\")\n        qres = gr.query(qr)\n        print(\"qres: \",len(qres),\" \",qr)\n        for row in qres:\n            #print(\"_________________________________________\")\n            #print(row['video_id'],\" \",row['prerequisite'],\" \",row['typedef'])\n            if v.video_id in result:\n                if \"prerequisite\" in result[v.video_id]:\n                    result[v.video_id][\"prerequisite\"].append(row['prerequisite'])\n                else:\n                    result[v.video_id][\"prerequisite\"] = []\n                    result[v.video_id][\"prerequisite\"].append(row['prerequisite'])\n\n                if \"typedef\" in result[v.video_id]:\n                    result[v.video_id][\"typedef\"].append(row['typedef'])\n                else:\n                    result[v.video_id][\"typedef\"] = []\n                    result[v.video_id][\"typedef\"].append(row['typedef'])\n            else:\n                result[v.video_id] = {}\n                if \"prerequisite\" in result[v.video_id]:\n                    result[v.video_id][\"prerequisite\"].append(row['prerequisite'])\n                else:\n                    result[v.video_id][\"prerequisite\"] = []\n                    result[v.video_id][\"prerequisite\"].append(row['prerequisite'])\n\n                if \"typedef\" in result[v.video_id]:\n                    result[v.video_id][\"typedef\"].append(row['typedef'])\n                else:\n                    result[v.video_id][\"typedef\"] = []\n                    result[v.video_id][\"typedef\"].append(row['typedef'])\n\n        VTS=VideoTextSegmentation.objects(video_id = v.video_id)\n        for VTSdoc in VTS:\n            print(VTSdoc.video_id)\n            print(VTSdoc.slides_percentage)\n            if v.video_id in result:\n                result[v.video_id]['slides_percentage'] = VTSdoc.slides_percentage\n            else:\n                result[v.video_id] = {}\n                result[v.video_id]['slides_percentage'] = VTSdoc.slides_percentage\n    return result\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.add_history","title":"<code>add_history()</code>","text":"<p>Update user history for a specific video.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/update_user_history',  methods=['POST'])\n@auth.login_required\ndef add_history():\n    \"\"\"\n    Update user history for a specific video.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email\n    \"\"\"\n    student= g.student\n    #get data from request\n    video_url_input = request.json.get('url')\n    video_watchtime_input = request.json.get('video_watchtime')\n    fragment_clicks_input = request.json.get('fragment_clicks')\n    node_clicks_input = request.json.get('node_clicks')\n    transcript_clicks_input = request.json.get('transcript_clicks')\n    searchbar_clicks_input = request.json.get('searchbar_clicks')\n    notes_input = request.json.get('notes')\n    fragments_input = request.json.get('fragments')\n\n    if video_url_input is None:\n        abort(400, \"The video url is missing\")    # missing arguments\n\n    #set None numbers to zero to avoid errors in the sums\n    if fragment_clicks_input is None:\n        fragment_clicks_input=0\n    if node_clicks_input is None:\n        node_clicks_input=0\n    if transcript_clicks_input is None:\n        transcript_clicks_input=0\n    if searchbar_clicks_input is None:\n        searchbar_clicks_input=0\n\n\n    videoHistory=None\n    try :\n        #case where the student already watched the video and the history exsists for this video\n        videoHistory = student.video_history_list.get(video_url=video_url_input)\n\n        if video_watchtime_input is not None:\n            videoHistory.video_watchtime= video_watchtime_input\n\n        if notes_input is not None:\n            videoHistory.notes= notes_input\n\n        if fragments_input is not None:\n            if videoHistory.fragments_progress is not None:\n                videoHistory.fragments_progress.delete()\n            for item in fragments_input:\n                new_fragment = Fragment(name = item['name'], start= item['start'], end= item['end'], progress=item['progress'])\n                videoHistory.fragments_progress.append(new_fragment)\n\n\n        videoHistory.fragment_clicks = videoHistory.fragment_clicks + fragment_clicks_input\n        videoHistory.node_clicks = videoHistory.node_clicks + node_clicks_input\n        videoHistory.transcript_clicks = videoHistory.transcript_clicks + transcript_clicks_input\n        videoHistory.searchbar_clicks = videoHistory.searchbar_clicks + searchbar_clicks_input\n        videoHistory.lastChangesDate = datetime.now()\n\n        new_log = HistoryLog(date = datetime.now(), request = str(request.json) )\n        videoHistory.logs.append(new_log)\n\n    except:\n\n        #case where the student have not already watched the video, we need to create a VideoHistory object for this video\n\n        if video_watchtime_input is None:\n            video_watchtime_input=0\n\n        if notes_input is None:\n            notes_input=''\n\n\n\n        videoHistory = VideoHistory(video_url = video_url_input, video_watchtime = video_watchtime_input, fragment_clicks = fragment_clicks_input, node_clicks = node_clicks_input, transcript_clicks = transcript_clicks_input,  searchbar_clicks = searchbar_clicks_input, notes = notes_input, lastChangesDate = datetime.now())\n\n        new_log = HistoryLog(date = datetime.now(), request = str(request.json) )\n        videoHistory.logs.append(new_log)\n\n        if fragments_input is not None:\n            for item in fragments_input:\n                new_fragment = Fragment(name = item['name'], start= item['start'], end= item['end'], progress=item['progress'])\n                videoHistory.fragments_progress.append(new_fragment)\n\n        student.video_history_list.append(videoHistory)\n\n    student.save()\n\n    #store global video statistics\n    videoStatistics = VideoStatistics.objects(video_url = video_url_input).first()\n\n    if videoStatistics is None :\n        videoStatistics = VideoStatistics(video_url = video_url_input, total_fragment_clicks = fragment_clicks_input, total_node_clicks = node_clicks_input, total_transcript_clicks = transcript_clicks_input,  total_searchbar_clicks = searchbar_clicks_input, amountViewers = 1)\n        videoStatistics.viewersList.append(student.email)\n\n    else:\n        videoStatistics.total_fragment_clicks = videoStatistics.total_fragment_clicks + fragment_clicks_input\n        videoStatistics.total_node_clicks = videoStatistics.total_node_clicks + node_clicks_input\n        videoStatistics.total_transcript_clicks = videoStatistics.total_transcript_clicks + transcript_clicks_input\n        videoStatistics.total_searchbar_clicks = videoStatistics.total_searchbar_clicks + searchbar_clicks_input\n\n        if videoStatistics.viewersList.count(student.email)==0 :\n            videoStatistics.viewersList.append(student.email)\n            videoStatistics.amountViewers = videoStatistics.amountViewers + 1\n\n\n    videoStatistics.save()\n    return (jsonify({'email': student.email}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.change_name_and_surname","title":"<code>change_name_and_surname()</code>","text":"<p>Change student's name and surname.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email, new name, and new surname</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/change_name_and_surname', methods=['POST'])\n@auth.login_required\n#the above line require that the credentials (mail/password or token) are sent in the request to login th user\ndef change_name_and_surname():\n    \"\"\"\n    Change student's name and surname.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email, new name, and new surname\n    \"\"\"\n    name_input = request.json.get('name')\n    surname_input = request.json.get('surname')\n    if name_input is None or surname_input is None:\n        abort(400, \"The new name or/and the new surname is/are missing\")    # missing arguments\n    student= g.student\n    student.name = name_input\n    student.surname = surname_input\n    student.save()\n    return (jsonify({'email': student.email, 'newName': name_input, 'newSurname': surname_input}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.generate_code","title":"<code>generate_code(N=6)</code>","text":"<p>Generates a random string of length N composed of lowercase letters and numbers.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>The length of the generated string (default is 6).</p> <code>6</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated random string.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>def generate_code(N=6):\n    \"\"\"\n    Generates a random string of length N composed of lowercase letters and numbers.\n\n    Parameters\n    ----------\n    N : int, optional\n        The length of the generated string (default is 6).\n\n    Returns\n    -------\n    str\n        The generated random string.\n    \"\"\"\n    return ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(N))\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_auth_token","title":"<code>get_auth_token()</code>","text":"<p>Get authentication token.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the token, duration, name, and surname</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/token')\n@auth.login_required\ndef get_auth_token():\n    \"\"\"\n    Get authentication token.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the token, duration, name, and surname\n    \"\"\"\n    token = g.student.generate_auth_token()\n    return jsonify({'token': token, 'duration': None, 'name': g.student.name, 'surname':g.student.surname})\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_catalog","title":"<code>get_catalog()</code>","text":"<p>Get list of all available videos in the database.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the video catalog</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/get_catalog')\n@auth.login_required\ndef get_catalog():\n    \"\"\"\n    Get list of all available videos in the database.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the video catalog\n    \"\"\"\n    graphs = Graphs.objects()\n    videos = Videos.objects()\n\n    catalog = []\n\n    for v in videos:\n        for gr in graphs:\n            if v.video_id == gr.video_id:\n                if v not in catalog:\n                    catalog.append(v)\n\n\n    return (jsonify({'catalog': catalog}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_fragments","title":"<code>get_fragments(video_id)</code>","text":"<p>Get video fragments and their progress.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email, fragments, and keywords</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/get_fragments/&lt;string:video_id&gt;')\n@auth.login_required\ndef get_fragments(video_id):\n    \"\"\"\n    Get video fragments and their progress.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email, fragments, and keywords\n    \"\"\"\n    student= g.student\n    video_url = \"https://www.youtube.com/watch?v=%s\" % video_id\n    video_fragment_progress = None\n    for i in student.video_history_list:\n        if video_url == i.video_url:\n            '''print(i.fragments_progress)\n            for e in i.fragments_progress:\n                print(e.name)'''\n\n            video_fragment_progress = i.fragments_progress\n    graph_object = Graphs.objects(video_id = video_id, email = student.email).first()\n\n    #if user first time on this video open first available video\n    if(graph_object is None):\n        graph_object = Graphs.objects(video_id = video_id).first()\n\n    #if then graph still not exist show msg\n    if graph_object is None:\n        abort(409, \"Unexisting graph for this video id\")    # the video doesn't exist in the graphs collection\n\n    keywords = handle_data.get_definitions_fragments(graph_object.email, video_id, video_fragment_progress)\n\n    if(video_fragment_progress is None or not len(video_fragment_progress)):\n\n        video = Videos.objects(video_id = video_id).first()\n\n        #if then video not exist show msg\n        if video is None:\n            abort(409, \"video not in the catalog\")    # the video is not in the catalog\n\n        segment_amount = len(video.segment_starts)\n        video_fragment_progress = []\n\n        '''if video_id ==\"sXLhYStO0m8\":\n            names = [\"Sex determination Pelvis Sciatic notch\", \"Pre-auricular sulcus Ventral arc Iliopubic ramus\",\"Skull\",\"Eye sockets\", \"Forehead Mastoid process Posterior zygomatich arch\", \"Nuchal crest\", \"Femur\"]\n            for i in range(segment_amount):\n                video_fragment_progress.append({'name': names[i], 'start' : time.strftime('%H:%M:%S', time.gmtime(int(round(video.segment_starts[i])))),  'end' : time.strftime('%H:%M:%S', time.gmtime(int(round(video.segment_ends[i])))), 'progress':0 })\n\n        else:'''\n\n        for i in range(segment_amount):\n            video_fragment_progress.append({'name': 'Part %s'%(i+1), 'start' : time.strftime('%H:%M:%S', time.gmtime(int(round(video.segment_starts[i])))),  'end' : time.strftime('%H:%M:%S', time.gmtime(int(round(video.segment_ends[i])))), 'progress':0 })\n\n\n\n    return (jsonify({'email': student.email, 'fragments' : video_fragment_progress, 'keywords':keywords}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_graph","title":"<code>get_graph(video_id=None)</code>","text":"<p>Get complete JSON graph for a video.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> <code>None</code> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email, graph, and concept vocabulary</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/get_graph/&lt;string:video_id&gt;')\n@auth.login_required\ndef get_graph(video_id=None):\n    \"\"\"\n    Get complete JSON graph for a video.\n\n    Parameters\n    ----------\n    video_id : str, optional\n        Identifier for the video\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email, graph, and concept vocabulary\n    \"\"\"\n    if video_id is None:\n        abort(400, \"The video id is missing\")    # missing arguments\n    student= g.student\n    graph_object = Graphs.objects(video_id = video_id, email=student.email).first()\n\n    #if user first time on this video open first available video\n    if(graph_object is None):\n        graph_object = Graphs.objects(video_id = video_id).first()\n\n    #if then graph still not exist show msg\n    if graph_object is None:\n        abort(409, \"Unexisting graph for this video id\")    # the video doesn't exist in the graphs collection\n\n    if graph_object.conceptVocabulary is None:\n        graph_object.conceptVocabulary = False\n\n    return (jsonify({'email': student.email, 'graph' : graph_object.graph, 'conceptVocabulary': graph_object.conceptVocabulary}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_history","title":"<code>get_history()</code>","text":"<p>Get user's video history.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email, video history, and video titles</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/get_user_history')\n@auth.login_required\ndef get_history():\n    \"\"\"\n    Get user's video history.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email, video history, and video titles\n    \"\"\"\n    student:Student = g.student\n    video_title_list = []\n\n    # Removing videos that have been removed from videos collection\n    for video_in_history in reversed(student.video_history_list):\n        if not Videos.objects(video_id = video_in_history.video_url.split(\"watch?v=\")[1]):\n            student.video_history_list.remove(video_in_history)\n\n    history_list = student.get_sorted_history(by=\"lastChange\")\n\n    for video in history_list:\n        video_title_list.append(get_video_title_from_url(video.video_url.split(\"watch?v=\")[1]))\n        #print(video,\" \",get_video_title_from_url(video.video_url.split(\"watch?v=\")[1]))\n\n    return (jsonify({'email': student.email, 'videoHistory' : history_list, 'videoHistoryTitles': video_title_list}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_image","title":"<code>get_image(video_id=None, fragment_index=None)</code>","text":"<p>Get image of a video fragment.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> <code>None</code> <code>fragment_index</code> <code>int</code> <p>Index of the fragment</p> <code>None</code> <p>Returns:</p> Type Description <code>Response</code> <p>Image file response</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route(\"/api/get_image/&lt;string:video_id&gt;/&lt;int:fragment_index&gt;\")\ndef get_image(video_id=None, fragment_index=None ):\n    \"\"\"\n    Get image of a video fragment.\n\n    Parameters\n    ----------\n    video_id : str, optional\n        Identifier for the video\n    fragment_index : int, optional\n        Index of the fragment\n\n    Returns\n    -------\n    flask.Response\n        Image file response\n    \"\"\"\n    if video_id is None or fragment_index is None:\n        abort(400, \"The video id or the image name is missing\")    # missing arguments\n\n    #get the starting time of the segment to know the image file name\n    video = Videos.objects(video_id = video_id).first()\n    if video is None:\n        abort(409, \"video not in the catalog\")    # the video doesn't exist\n\n    #segment_starting_time = None\n    #try :\n    #    segment_starting_time = video.segment_starts[fragment_index]\n    #except IndexError:\n    #    abort(409, \"fragment index out of range \")\n\n    #image_name = str(segment_starting_time).replace('.','_')+'.jpg'\n    image_name = str(fragment_index).replace('.','_')+'.jpg'\n    try:\n        return send_from_directory(app.config[\"CLIENT_IMAGES\"]+sep+video_id, image_name, as_attachment=True)\n    except FileNotFoundError:\n        abort\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_integer_part","title":"<code>get_integer_part(number)</code>","text":"<p>Get the integer part of a number.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>float</code> <p>Input number</p> required <p>Returns:</p> Type Description <code>float</code> <p>Integer part of the number</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>def get_integer_part(number):\n    \"\"\"\n    Get the integer part of a number.\n\n    Parameters\n    ----------\n    number : float\n        Input number\n\n    Returns\n    -------\n    float\n        Integer part of the number\n    \"\"\"\n    split_number = math.modf(number)\n    return split_number[1]\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_student","title":"<code>get_student()</code>","text":"<p>Get student's name, surname, and email.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's name, surname, and email</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/get_user_infos')\n@auth.login_required\ndef get_student():\n    \"\"\"\n    Get student's name, surname, and email.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's name, surname, and email\n    \"\"\"\n    return jsonify({'name': g.student.name, 'surname':g.student.surname, 'email': g.student.email})\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.get_video_title_from_url","title":"<code>get_video_title_from_url(video_id)</code>","text":"<p>Get YouTube video title based on video ID.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>YouTube video ID</p> required <p>Returns:</p> Type Description <code>str</code> <p>Video title</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>def get_video_title_from_url(video_id):\n    \"\"\"\n    Get YouTube video title based on video ID.\n\n    Parameters\n    ----------\n    video_id : str\n        YouTube video ID\n\n    Returns\n    -------\n    str\n        Video title\n    \"\"\"\n    #print(\"GET VIDEO: \",video_id)\n    params = {\"format\": \"json\", \"url\": \"https://www.youtube.com/watch?v=%s\" % video_id}\n    url = \"https://www.youtube.com/oembed\"\n    query_string = urllib.parse.urlencode(params)\n    url = url + \"?\" + query_string\n    #print(\"url \",url)\n\n    #print(\"dopo\")\n\n    with urllib.request.urlopen(url) as response:\n        response_text = response.read()\n        data = json.loads(response_text.decode())\n    return data['title']\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.graph","title":"<code>graph(annotator_id, video_id)</code>","text":"<p>Get graph for a given video and annotator.</p> <p>Parameters:</p> Name Type Description Default <code>annotator_id</code> <code>str</code> <p>Identifier for the annotator</p> required <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing concepts list and concept vocabulary</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/graph/&lt;annotator_id&gt;/&lt;video_id&gt;')\n@auth.login_required\ndef graph(annotator_id, video_id):\n    \"\"\"\n    Get graph for a given video and annotator.\n\n    Parameters\n    ----------\n    annotator_id : str\n        Identifier for the annotator\n    video_id : str\n        Identifier for the video\n\n    Returns\n    -------\n    dict\n        Dictionary containing concepts list and concept vocabulary\n    \"\"\"\n    concept_graph = data.build_array(annotator_id, video_id)\n\n    # old: return {\"conceptsList\": concept_graph }\n\n    conceptVocabulary = data.get_concept_vocabulary(annotator_id, video_id)\n\n    # If the concept vocabulary is new (empty) in DB then initialize it to empty synonyms\n    if(conceptVocabulary == None) :\n        conceptVocabulary = {}\n        concept_list = data.get_concept_list(annotator_id, video_id)\n        for c in concept_list:\n            conceptVocabulary[c[\"name\"][4:].replace(\"_\", \" \")] = []    \n\n    return {\"conceptsList\": concept_graph, \"conceptVocabulary\": conceptVocabulary}\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.graph_id","title":"<code>graph_id(video_id)</code>","text":"<p>Get graph user IDs for a given video.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing list of graph IDs</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/graph_id/&lt;video_id&gt;')\n@auth.login_required\ndef graph_id(video_id):\n    \"\"\"\n    Get graph user IDs for a given video.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video\n\n    Returns\n    -------\n    dict\n        Dictionary containing list of graph IDs\n    \"\"\"\n    print(\"***** EKEEL - Video Augmentation: main.py::graph_id(): Inizio ******\")\n    student= g.student\n    graph_list = handle_data.check_graphs(video_id,student.email)\n\n    # if user first time on this video, select all available graph ids\n    if not len(graph_list):\n        graph_list = handle_data.get_graphs(video_id)\n    print(graph_list)\n    print(\"***** EKEEL - Video Augmentation: main.py::graph_id(): Fine ******\")\n\n    return {\"graphs_id_list\": graph_list }\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.new_student","title":"<code>new_student()</code>","text":"<p>Register a new student account.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/register', methods=['POST'])\ndef new_student():\n    \"\"\"\n    Register a new student account.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email\n    \"\"\"\n    email_input = request.json.get('email')\n    password_input = request.json.get('password')\n    name_input = request.json.get('name')\n    surname_input = request.json.get('surname')\n    if email_input is None or password_input is None or name_input is None or surname_input is None:\n        abort(400)    # missing arguments\n    if Student.objects(email=email_input).first() is not None:\n        abort(409, \"An account have already been created with this mail\")    # existing student\n    existingUnverifiedStudent = UnverifiedStudent.objects(email=email_input).first()\n    if existingUnverifiedStudent is not None:\n        existingUnverifiedStudent.delete()\n\n    generated_code_creation = generate_code()\n    # create an UnverifiedStudent and save it in the db\n    unverifiedStudent = UnverifiedStudent(email=email_input, name= name_input, surname = surname_input,nb_try_code_on_creation = 0)\n    unverifiedStudent.hash_password(password_input)\n    unverifiedStudent.hash_code_on_creation(generated_code_creation)\n    unverifiedStudent.save()\n\n    #send a mail with the verification code\n    msg = Message('EKEEL- Code to finalize your account creation', recipients = [email_input])\n    msg.body = \"Hello, welcome to EKEEL ! \\n Your validation code to finalize your account creation : \"+ generated_code_creation\n    try:\n        mail.send(msg)\n    except SMTPRecipientsRefused:\n        abort(406,\"invalid mail\")\n    except :\n        abort(503, \"error returned by the email sending service\")\n    return (jsonify({'email': unverifiedStudent.email}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.send_code_to_delete_account_by_mail","title":"<code>send_code_to_delete_account_by_mail()</code>","text":"<p>Send code to delete account via email.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/delete_account', methods=['POST'])\n@auth.login_required\ndef send_code_to_delete_account_by_mail():\n    \"\"\"\n    Send code to delete account via email.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email\n    \"\"\"\n    email_input = request.json.get('email')\n    if email_input is None:\n        abort(400)    # missing arguments\n    student = Student.objects(email=email_input).first()\n    if student is None:\n        abort(409, \"No account associated with this mail\")    # no existing student\n    generated_code_delete_account = generate_code()\n    student.hash_code_delete_account(generated_code_delete_account)\n    student.save()\n\n    #send a mail with the verification code\n    msg = Message('EKEEL- Code to delete your account', recipients = [email_input])\n    msg.body = \"Hello ! \\n Here is your validation code to delete your account : \"+ generated_code_delete_account+\"\\n Be carefull, account deletion can't be undone\"\n    mail.send(msg)\n    return (jsonify({'email': student.email}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.send_code_to_reset_password_by_mail","title":"<code>send_code_to_reset_password_by_mail()</code>","text":"<p>Send code to reset password via email.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/retrieve_password', methods=['POST'])\ndef send_code_to_reset_password_by_mail():\n    \"\"\"\n    Send code to reset password via email.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email\n    \"\"\"\n    email_input = request.json.get('email')\n    if email_input is None:\n        abort(400)    # missing arguments\n    student = Student.objects(email=email_input).first()\n    if student is None:\n        abort(409, \"No account associated with this mail\")    # no existing student\n    generated_code_reset_password = generate_code()\n    student.hash_code_reset_password(generated_code_reset_password)\n    student.save()\n\n    #send a mail with the verification code\n    msg = Message('EKEEL- Code to change your password', recipients = [email_input])\n    msg.body = \"Hello ! \\n Here is your validation code to change your password : \"+ generated_code_reset_password\n    mail.send(msg)\n    return (jsonify({'email': student.email}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.sparql_query_concepts","title":"<code>sparql_query_concepts(video_id, annotator_id)</code>","text":"<p>Perform SPARQL query for concepts.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <code>annotator_id</code> <code>str</code> <p>Identifier for the annotator</p> required Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/sparql_query_concepts/&lt;video_id&gt;')\n@auth.login_required\ndef sparql_query_concepts(video_id, annotator_id):\n    \"\"\"\n    Perform SPARQL query for concepts.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video\n    annotator_id : str\n        Identifier for the annotator\n    \"\"\"\n    client = pymongo.MongoClient(\n        \"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/edurell?retryWrites=true&amp;w=majority\")\n\n    db = client.edurell\n    collection = db.graphs\n\n    query = {\n        \"annotator_id\": annotator_id,\n        \"video_id\": video_id\n    }\n\n    general_query = {\n        \"video_id\": video_id\n    }\n\n    #se trova un grafo qualunque con quel video_id, salva il risultato della query\n    if collection.find_one(general_query) is not None:\n        risultato = collection.find_one(general_query)[\"conceptVocabulary\"]\n    #ma se trova il grafo con quel video_id e proprio con quell'annotator_id, sovrascrive risultato\n    if collection.find_one(query) is not None:\n        risultato = collection.find_one(query)[\"conceptVocabulary\"]    \n\n    #se almeno una delle due query \u00e8 andata a buon fine\n    if risultato is not None:\n        gr = Graph()\\\n        .parse(data=json.dumps(risultato), format='json-ld')\n\n        tic = time.time()\n\n        # Seleziona tutti i nodi concetto ed eventuali sinonimi\n        query3 = \"\"\"\n            PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n            PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n            PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n\n            SELECT ?concept ?synonym\n            WHERE {\n                    ?concetto skos:prefLabel ?concept.\n                    OPTIONAL {\n                        ?concetto skos:altLabel ?synonym\n                    }\n                    FILTER (!BOUND(?altLabel))\n                }\"\"\"\n\n        qres = gr.query(query3)\n\n        toc = time.time()\n\n        #print per i concetti\n        print(\"Results: - obtained in %.4f seconds\" % (toc - tic))\n        for row in qres: \n            print(f\"Concept: {row.concept} || Synonym: {row.synonym}\")\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.sparql_query_definitions","title":"<code>sparql_query_definitions(video_id, annotator_id)</code>","text":"<p>Perform SPARQL query for definitions.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <code>annotator_id</code> <code>str</code> <p>Identifier for the annotator</p> required Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>def sparql_query_definitions(video_id, annotator_id):\n    \"\"\"\n    Perform SPARQL query for definitions.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video\n    annotator_id : str\n        Identifier for the annotator\n    \"\"\"\n    client = pymongo.MongoClient(\n        \"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/edurell?retryWrites=true&amp;w=majority\")\n\n    db = client.edurell\n    collection = db.graphs\n\n    query = {\n        \"annotator_id\": annotator_id,\n        \"video_id\": video_id\n    }\n\n    general_query = {\n        \"video_id\": video_id\n    }\n\n    #se trova un grafo qualunque con quel video_id, salva il risultato della query\n    if collection.find_one(general_query) is not None:\n        risultato = collection.find_one(general_query)[\"graph\"]\n    #ma se trova il grafo con quel video_id e proprio con quell'annotator_id, sovrascrive risultato\n    if collection.find_one(query) is not None:\n        risultato = collection.find_one(query)[\"graph\"]    \n\n    #se almeno una delle due query \u00e8 andata a buon fine\n    if risultato is not None:\n        gr = Graph()\\\n        .parse(data=json.dumps(risultato), format='json-ld')\n\n        tic = time.time()\n\n        # Seleziona i concetti che sono spiegati nel video\n        query1 = \"\"\"\n            PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n            PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n            PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n            PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\n\n            SELECT ?explained_concept ?created ?creator ?description_type\n            WHERE {\n                    ?concept_definition oa:motivatedBy oa:describing.\n                    ?concept_definition dcterms:created ?created.\n                    ?concept_definition dcterms:creator ?creator.\n                    ?concept_definition oa:hasBody ?explained_concept.\n                    ?concept_definition skos:note ?description_type.\n                    ?concept_definition oa:hasTarget ?target.\n                    ?target oa:hasSelector ?selector.\n                    ?selector oa:hasStartSelector ?startSelector\n\n                }\"\"\"\n\n\n        qres = gr.query(query1)\n\n        toc = time.time()\n\n        #print per le definizioni\n        print(\"Results: - obtained in %.4f seconds\" % (toc - tic))\n        for row in qres:\n            print(f\"Explained concept: {row.explained_concept}\")\n            print(f\"Created: {row.created}\")\n            print(f\"Creator: {row.creator}\")\n            print(f\"Description Type: {row.description_type}\")\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.sparql_query_prerequisite","title":"<code>sparql_query_prerequisite(video_id, annotator_id)</code>","text":"<p>Perform SPARQL query for prerequisites.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <code>annotator_id</code> <code>str</code> <p>Identifier for the annotator</p> required Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>def sparql_query_prerequisite(video_id, annotator_id):\n    \"\"\"\n    Perform SPARQL query for prerequisites.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video\n    annotator_id : str\n        Identifier for the annotator\n    \"\"\"\n    client = pymongo.MongoClient(\n        \"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/edurell?retryWrites=true&amp;w=majority\")\n\n    db = client.edurell\n    collection = db.graphs\n\n    query = {\n        \"annotator_id\": annotator_id,\n        \"video_id\": video_id\n    }\n\n    general_query = {\n        \"video_id\": video_id\n    }\n\n    #se trova un grafo qualunque con quel video_id, salva il risultato della query\n    if collection.find_one(general_query) is not None:\n        risultato = collection.find_one(general_query)[\"graph\"]\n    #ma se trova il grafo con quel video_id e proprio con quell'annotator_id, sovrascrive risultato\n    if collection.find_one(query) is not None:\n        risultato = collection.find_one(query)[\"graph\"]    \n\n    #se almeno una delle due query \u00e8 andata a buon fine\n    if risultato is not None:\n        gr = Graph()\\\n        .parse(data=json.dumps(risultato), format='json-ld')\n\n        tic = time.time()\n\n        #query per i prerequisiti\n        query2 = \"\"\"\n            PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n            PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n\n            SELECT ?prerequisite_concept ?created ?creator ?prerequisite_type ?target_concept\n            WHERE {\n                    ?concept_prerequisite oa:motivatedBy edu:linkingPrerequisite.\n                    ?concept_prerequisite dcterms:created ?created.\n                    ?concept_prerequisite dcterms:creator ?creator.\n                    ?concept_prerequisite oa:hasBody ?prerequisite_concept.\n                    ?concept_prerequisite skos:note ?prerequisite_type.\n                    ?concept_prerequisite oa:hasTarget ?target.\n                    ?target dcterms:subject ?target_concept.\n                }\"\"\"\n\n\n        qres = gr.query(query2)\n\n        toc = time.time()\n\n        #print per i prerequisiti\n        print(\"Results: - obtained in %.4f seconds\" % (toc - tic))\n        for row in qres: \n            print(f\"Prerequisite concept: {row.prerequisite_concept}\")\n            print(f\"Created: {row.created}\")\n            print(f\"Creator: {row.creator}\")\n            print(f\"Prerequisite Type: {row.prerequisite_type}\")\n            print(f\"Target concept: {row.target_concept}\")\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.speech_from_youtube","title":"<code>speech_from_youtube()</code>","text":"<p>Retrieves the speech transcript from a YouTube video URL.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> required <p>Returns:</p> Type Description <code>Response</code> <p>A JSON response containing the speech transcript and video metadata.</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/youtube_transcript', methods=[\"GET\", \"POST\"])\ndef speech_from_youtube():\n    \"\"\"\n    Retrieves the speech transcript from a YouTube video URL.\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    flask.Response\n        A JSON response containing the speech transcript and video metadata.\n    \"\"\"\n    req = request.get_json(force=True)\n    videoId=req.get('video_id')\n\n    collection = pymongo.MongoClient(\n        \"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/edurell?retryWrites=true&amp;w=majority\").edurell.videos\n\n    video_metadata = collection.find_one({\"video_id\": videoId})\n    languages = []\n    if video_metadata is not None and \"language\" in video_metadata.keys():\n        languages.append(video_metadata[\"language\"])\n    languages.append(\"en\")\n    transcript = YouTubeTranscriptApi.get_transcript(videoId, languages=languages)\n    subs_dict = []\n    for sub in transcript:\n        start = get_integer_part(sub[\"start\"])\n        duration = get_integer_part(sub[\"duration\"])\n        subs_dict.append(\n            {\"text\": sub[\"text\"],\n             \"start\": start,\n             \"end\": start + duration}\n        )\n    result = jsonify(subs_dict)\n    return result\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.testm","title":"<code>testm(video_id)</code>","text":"<p>Test function for querying video data.</p> <p>Parameters:</p> Name Type Description Default <code>video_id</code> <code>str</code> <p>Identifier for the video</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing query results</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/testm/&lt;video_id&gt;')\n@auth.login_required\ndef testm(video_id):\n    \"\"\"\n    Test function for querying video data.\n\n    Parameters\n    ----------\n    video_id : str\n        Identifier for the video\n\n    Returns\n    -------\n    dict\n        Dictionary containing query results\n    \"\"\"\n    client = pymongo.MongoClient(\n        \"mongodb+srv://\"+MONGO_CLUSTER_USERNAME+\":\"+MONGO_CLUSTER_PASSWORD+\"@clusteredurell.z8aeh.mongodb.net/edurell?retryWrites=true&amp;w=majority\")\n\n    db = client.edurell\n    collection = db.graphs\n\n    cursor = collection.find({\"video_id\": video_id})\n    gr = Graph()\n    retval={}\n    for document in cursor:\n        print(\"asd\")\n        if document is not None:\n\n            gr.parse(data=json.dumps(document[\"graph\"]), format='json-ld')\n\n\n            qr = \"\"\"\n                PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n\n                SELECT ?sub ?pre ?obj\n                WHERE {\n                        ?sub ?pre ?obj\n\n                    }\n\n            \"\"\"\n\n            qr2=\"\"\"\n                PREFIX oa: &lt;http://www.w3.org/ns/oa#&gt;\n                PREFIX edu: &lt;https://teldh.github.io/edurell#&gt;\n                PREFIX dctypes: &lt;http://purl.org/dc/dcmitype/&gt;\n                PREFIX dcterms: &lt;http://purl.org/dc/terms/&gt;\n\n                SELECT ?sub  ?obj\n                WHERE {\n                        ?sub dcterms:created ?obj\n\n                    }\n\n            \"\"\"\n            qres = gr.query(qr)\n            qres2=gr.query(qr2)\n            resulto={}\n            resulto2={}\n            print(\"STARTOOOOOO \",qres)\n            for idx,row in enumerate(qres):\n                resulto[idx]={\"subject\":row['sub'],\"predicate\":row['pre'],\"object\":row['obj']}\n            print(\"ENDOOOOOOOO\")\n            for idx,row in enumerate(qres2):\n                resulto2[idx]={\"subject\":row['sub'],\"object\":row['obj']}\n            retval[\"res\"]=resulto\n    return retval\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.verify_code_and_change_password","title":"<code>verify_code_and_change_password()</code>","text":"<p>Verify code and change password.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/retrieve_password/verify', methods=['POST'])\ndef verify_code_and_change_password():\n    \"\"\"\n    Verify code and change password.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email\n    \"\"\"\n    email_input = request.json.get('email')\n    code_input = request.json.get('code')\n    new_password_input = request.json.get('password')\n    if email_input is None or code_input is None:\n        abort(400, \"The code or the email is missing\")    # missing arguments\n    student = Student.objects(email=email_input).first()\n    if student is None:\n        abort(409, \"No account exists with this mail\")    # no existing student\n    if not student.verify_code_reset_password(code_input):\n        if student.nb_try_code_reset_password is not None and student.nb_try_code_reset_password&gt;5:\n            student.code_reset_password = None\n            student.nb_try_code_reset_password = None\n            student.save()\n            abort(403, \"Too many wrong validation codes, please restart the password retrieval procedure\")\n        abort(401, \"Wrong validation code\")\n\n    else:\n        student.hash_password(new_password_input)\n        student.code_reset_password = None\n        student.nb_try_code_reset_password = None\n        student.save()\n        return (jsonify({'email': student.email}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.verify_code_and_delete_account","title":"<code>verify_code_and_delete_account()</code>","text":"<p>Verify code and delete account.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/delete_account/verify', methods=['POST'])\n@auth.login_required\ndef verify_code_and_delete_account():\n    \"\"\"\n    Verify code and delete account.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email\n    \"\"\"\n    email_input = request.json.get('email')\n    code_input = request.json.get('code')\n    if email_input is None or code_input is None:\n        abort(400, \"The code or the email is missing\")    # missing arguments\n    student = Student.objects(email=email_input).first()\n    if student is None:\n        abort(409, \"No account exists with this mail\")    # no existing student\n    if not student.verify_code_delete_account(code_input):\n        if student.nb_try_code_delete_account is not None and student.nb_try_code_delete_account&gt;5:\n            student.code_delete_account = None\n            student.nb_try_code_delete_account = None\n            student.save()\n            abort(403, \"Too many wrong validation codes, please restart the account deletion procedure\")\n        abort(415, \"Wrong validation code\")\n\n    else:\n        data.delete_graphs(email_input)\n        student.delete()\n        return (jsonify({'email': student.email}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.verify_mail","title":"<code>verify_mail()</code>","text":"<p>Verify email and code for account creation.</p> <p>Returns:</p> Type Description <code>Response</code> <p>JSON response with the student's email</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@app.route('/api/register/verify', methods=['POST'])\ndef verify_mail():\n    \"\"\"\n    Verify email and code for account creation.\n\n    Returns\n    -------\n    flask.Response\n        JSON response with the student's email\n    \"\"\"\n    email_input = request.json.get('email')\n    code_input = request.json.get('code')\n    if email_input is None or code_input is None:\n        abort(400, \"The code or the email is missing\")    # missing arguments\n    if Student.objects(email=email_input).first() is not None:\n        abort(409,\"An account have already been created with this mail\")    # existing student\n    unverifiedStudent = UnverifiedStudent.objects(email=email_input).first()\n    if unverifiedStudent is None:\n        abort(404, \"No temporary student found, please restart register from the beginning\")    # not existing UnverifiedStudent\n    if not unverifiedStudent.verify_code_on_creation(code_input):\n        if unverifiedStudent.nb_try_code_on_creation is not None and unverifiedStudent.nb_try_code_on_creation&gt;5:\n            unverifiedStudent.delete()\n            abort(403, \"Too many wrong validation code, please restart register from the beginning\")\n        abort(401, \"Wrong validation code\")\n\n    #after the checks have passed, create a Student in the db and delete the UnverifiedStudent from the db\n    student = Student(email = unverifiedStudent.email, name = unverifiedStudent.name, surname = unverifiedStudent.surname, password_hash = unverifiedStudent.password_hash)\n    student.save()\n    unverifiedStudent.delete()\n    return (jsonify({'email': student.email}), 201)\n</code></pre>"},{"location":"codebase/EKEELVideoAugmentation/src/flask-server/main/#/home/runner/work/ekeel/ekeel/EVA_apps.EKEELVideoAugmentation.src.flask-server.main.verify_password","title":"<code>verify_password(email_or_token, password)</code>","text":"<p>Verify user password or token for authentication.</p> <p>Parameters:</p> Name Type Description Default <code>email_or_token</code> <code>str</code> <p>Email or authentication token</p> required <code>password</code> <code>str</code> <p>User password</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if authentication is successful, False otherwise</p> Source code in <code>EVA_apps/EKEELVideoAugmentation/src/flask-server/main.py</code> <pre><code>@auth.verify_password\ndef verify_password(email_or_token, password):\n    \"\"\"\n    Verify user password or token for authentication.\n\n    Parameters\n    ----------\n    email_or_token : str\n        Email or authentication token\n    password : str\n        User password\n\n    Returns\n    -------\n    bool\n        True if authentication is successful, False otherwise\n    \"\"\"\n    # first try to authenticate by token\n    student = Student.verify_auth_token(email_or_token)\n    if not student:\n        # try to authenticate with email/password\n        student = Student.objects(email=email_or_token).first()\n        if not student or not student.verify_password(password):\n            return False\n    g.student = student\n    return True\n</code></pre>"},{"location":"platforms/annotator/deploy/","title":"Annotator","text":""},{"location":"platforms/annotator/deploy/#deploy-ekeel-app-on-server","title":"Deploy Ekeel App on Server","text":""},{"location":"platforms/annotator/deploy/#update-and-setup-video-annotation-app","title":"Update and Setup Video Annotation App","text":"<p>Go inside EKEELVideoAnnotation app folder </p><pre><code>cd /var/www/ekeel/EVA_apps/EKEELVideoAnnotation/\n</code></pre> <p>Update prune (TODO not always works, sometimes happen that one has to delete environment and reinstall) and restart </p><pre><code>conda env update --file conda_environment.yml --prune\nsudo systemctl restart ekeel\n</code></pre>"},{"location":"platforms/annotator/deploy/#launch-gunicorn","title":"Launch gunicorn","text":"<p>(done automatically by \"systemctl ekeel\", use only for debugging) For infos about linux services used in this project look here </p><pre><code>cd /var/www/ekeel/EVA_apps/EkeelVideoAnnotation/\n/home/anaconda3/envs/myenv/bin/gunicorn --bind 127.0.0.1:5050 connector:app --timeout 180 --limit-request-line 0\n</code></pre>"},{"location":"platforms/annotator/deploy/#view-gunicorn-instances","title":"View gunicorn instances","text":"<pre><code>ps -ef|grep gunicorn\n</code></pre>"},{"location":"platforms/annotator/deploy/#run-manually-video-augmentation-app","title":"Run manually Video Augmentation App","text":"<p>(done automatically by \"systemctl ekeel\", use only for debugging) </p><pre><code>cd /var/www/ekeel/EVA_apps/EkeelVideoAugmentation/src/flask-server\nsudo /home/anaconda3/envs/env-wp3/bin/python ./main.py\n</code></pre>"},{"location":"platforms/annotator/deploy/#important-files","title":"Important Files","text":"<ul> <li>Github Repository folder: <code>/var/www/ekeel</code></li> <li>Video Annotation folder: <code>/var/www/ekeel/EVA_apps/EkeelVideoAnnotation/</code></li> <li>Video Augmentation folder: <code>/var/www/ekeel/EVA_apps/EkeelVideoAugmentation/</code></li> <li>Server run service Video Annotation: <code>/etc/systemd/system/ekeel.service</code></li> <li>Server run service Video Augmentation: <code>/etc/systemd/system/ekeel-wp3.service</code></li> <li>NGINX: <code>/etc/nginx/sites-enabled/ekeel-wp3</code></li> <li><code>location /</code> and <code>/api</code> are for react project of video augmentation</li> <li><code>location /annotator</code> is for annotation tool (prefix added for each route using reverseProxy class in config.py)</li> <li>Might be necessary to do <code>sudo systemctl daemon-reload</code> if editing these files</li> </ul>"},{"location":"platforms/annotator/install/","title":"Annotator","text":""},{"location":"platforms/annotator/install/#install-annotation-tool","title":"Install Annotation tool","text":"<p>The document provides instructions for setting up the EKEEL Video Annotation project locally on your pc.</p>"},{"location":"platforms/annotator/install/#first-installation","title":"First Installation","text":""},{"location":"platforms/annotator/install/#prerequisites-ffmpeg","title":"Prerequisites: ffmpeg","text":"<p>Open a Terminal and type: </p><pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install ffmpeg\nsudo apt install tesseract-ocr\nsudo apt install libtesseract-dev\n</code></pre>"},{"location":"platforms/annotator/install/#prerequisites-anaconda","title":"Prerequisites: Anaconda","text":"<p>Ensure Anaconda/Conda configured in terminal:</p> <p>To install follow this guide </p> <p></p> <p>Download the repository: </p><pre><code>git clone https://github.com/Teldh/ekeel.git\n</code></pre> <p>\"cd\" to the folder EKEELVideoAnnotation</p> <pre><code>&gt; cd {path to the folder EKEELVideoAnnotation}\n</code></pre> <p>conda create the python environment from a yml file </p><pre><code>conda env create -f conda_environment.yml\nconda activate ekeel_anno_env\n</code></pre> <p>(Facoltative) If you have a gpu, to improve performances: </p><pre><code>conda install m2w64-toolchain\nconda install libpython\n</code></pre> <p>Installation completed, with the environment activated launch the project with: </p><pre><code>python main.py\n</code></pre> <p></p>"},{"location":"platforms/annotator/install/#on-any-change-in-environment-packages","title":"On any Change in Environment Packages","text":"<p>To avoid inconsistency between local and server, yml file has been used to enforce same environment state</p> <p>open a terminal:</p> <pre><code>&gt; cd {inside folder EKEELVideoAnnotation}\n</code></pre> <p>overwrite the conda_environment.yml inside using </p><pre><code>conda env export --no-builds | grep -v \"^prefix: \" | grep -v \"en-core-web-lg\" | grep -v \"it-core-news-lg\" &gt; conda_environment.yml\n</code></pre> and push the modifications to the repo. (The spacy models end up in the final distribution but must be ignored otherwise cause errors) <p>Then to synchronize the packages change in the server pull updates from the repo and on the server terminal update dependencies on the server:</p> <p>The guide is here</p>"},{"location":"platforms/annotator/install/#run-locally","title":"Run locally","text":"<p>Start by running Anaconda/Conda terminal:</p> <p></p> <p>Activate the \"myenv\" virtual environment:</p> <pre><code>&gt; conda activate ekeel_anno_env\n</code></pre> <p>(Facoltative) Open VSCode with conda (if dev using VScode ide)</p> <pre><code>&gt; code\n</code></pre> <p>With the environment activated launch the app locally with:</p> <pre><code>&gt; python main.py\n</code></pre>"},{"location":"platforms/annotator/install/#notes","title":"Notes","text":"<ul> <li> <p>Email </p> <p>After some months the email sender could stop working and you can find errors on register or forgot password:</p> <ul> <li> <p>Login to the google account with this app credentials  (you can find those credentials on file .env) </p> </li> <li> <p>go to security settings -&gt; allow less secure app</p> </li> <li> <p>(More info -&gt; https://support.google.com/accounts/answer/6010255?hl=en)</p> </li> </ul> <p>If still not working:</p> <ul> <li> <p>after log in with the google account open this link:   https://accounts.google.com/DisplayUnlockCaptcha</p> </li> <li> <p>(More info -&gt; https://stackoverflow.com/questions/16512592/login-credentials-not-working-with-gmail-smtp)</p> </li> </ul> </li> </ul>"},{"location":"platforms/annotator/transcriber/deploy/","title":"Transcriber","text":""},{"location":"platforms/annotator/transcriber/deploy/#transcriber-deployment","title":"Transcriber deployment","text":"<p>The EKEEL Transcriber Service is a continuous transcription system that automates the process of converting YouTube videos into text transcripts using the Stable-Whisper model (large-v3), a state-of-the-art speech recognition model.</p> <p>The service operates as a background worker that continuously polls a MongoDB database for untranscribed video jobs. </p> <p>For each video, it:</p> <ul> <li>Downloads the video from YouTube.</li> <li>Converts it into WAV format.</li> <li>Transcribes the audio using Whisper, generating accurate, multilingual text segments.</li> <li>Updates the database with the transcription and cleans up temporary files.</li> </ul> <p>It's a standalone file that shouldn't be imported outside of this service to avoid unnecessary memory allocation.</p>"},{"location":"platforms/annotator/transcriber/deploy/#restart-the-transcriber-service","title":"Restart the Transcriber Service","text":"<p>To restart the <code>ekeel-transcriber</code> service, use the following commands:</p> <ol> <li> <p>Open a terminal on the server.</p> </li> <li> <p>Use <code>systemctl</code> to restart the service:</p> </li> </ol> <pre><code>sudo systemctl restart ekeel-transcriber\n</code></pre> <ol> <li>Verify the service status to ensure it restarted successfully:</li> </ol> <pre><code>sudo systemctl status ekeel-transcriber\n</code></pre> <p>If the service is running correctly, you should see an active (running) status.</p>"},{"location":"platforms/annotator/transcriber/deploy/#code","title":"Code","text":"<p>The code is available here</p>"},{"location":"platforms/augmentator/deploy/","title":"Augmentator","text":""},{"location":"platforms/augmentator/deploy/#deploy-augmentator-on-server","title":"Deploy Augmentator on Server","text":""},{"location":"platforms/augmentator/deploy/#update-and-setup-video-augmentation-app","title":"Update and Setup Video Augmentation App","text":"<p>Go inside EkeelVideoAugmentation app folder </p><pre><code>cd /var/www/ekeel/EVA_apps/EkeelVideoAugmentation/\n</code></pre> <p>Go inside flask-server folder and update dependencies from requirements.txt </p><pre><code>cd ./src/flask-server/\nsudo /home/anaconda3/envs/env-wp3/bin/pip install -r ./requirements.txt\n</code></pre> <p>Go inside react-app folder </p><pre><code>cd /var/www/ekeel/EVA_apps/EkeelVideoAugmentation/src/react-app\n</code></pre> <p>If you have a build folder, delete it </p><pre><code>sudo rm -r ./build\n</code></pre> <p>Update npm packages </p><pre><code>sudo npm install --legacy-peer-deps\n</code></pre> <p>Create new version of the react app build </p><pre><code>sudo -s\nnpm run build\n</code></pre> <p>Restart VideoAugmentation App </p><pre><code>sudo systemctl restart ekeel-wp3\n</code></pre>"},{"location":"platforms/augmentator/deploy/#in-case-of-reinstall-on-server","title":"In case of reinstall on server","text":"<p>Clone the repo </p><pre><code>git clone https://github.com/\n</code></pre> <p>Follow this guide to install flask backend and react frontend</p>"},{"location":"platforms/augmentator/install/","title":"Augmentator","text":""},{"location":"platforms/augmentator/install/#video-augmentation-and-graph-exploration","title":"Video augmentation and graph exploration","text":""},{"location":"platforms/augmentator/install/#short-description-of-the-project","title":"Short description of the project:","text":"<p>This project is a web application developped in React (JS) for the front-end and with Flask for the back-end (Python). The aim of the application is to help students learn through videos, contextual help and an interactive knowledge graph gathering all the concepts explainend in the video and the relationships with each other.</p> <p>Here is a Drive folder of demo of the features</p> <p></p>"},{"location":"platforms/augmentator/install/#installation","title":"Installation","text":""},{"location":"platforms/augmentator/install/#virtual-environment","title":"Virtual Environment","text":"<p>To organize the projecty it is better to create and use a virtual env as Annotator app, but you can skip this step.</p>"},{"location":"platforms/augmentator/install/#prerequisites-anaconda","title":"Prerequisites: Anaconda","text":"<p>Ensure Anaconda/Conda configured in terminal:</p> <p>To install Conda follow this guide </p>"},{"location":"platforms/augmentator/install/#back-end-flask","title":"Back-end (Flask)","text":"<ul> <li> <p>Make sure that you have Flask installed on your machine</p> </li> <li> <p>Go inside flask-server folder: </p><pre><code>cd src/flask-server\nconda env create -f conda_environment.yml\nconda activate ekeel_aug_env\n</code></pre> </li> </ul>"},{"location":"platforms/augmentator/install/#on-any-change-in-environment-packages","title":"On any Change in Environment Packages","text":"<p>To avoid inconsistency between local and server, yml file has been used to enforce same environment state</p> <p>open a terminal:</p> <p>cd {inside folder EKEELVideoAugmentation/src/flask-server}</p> <p>overwrite the conda_environment.yml inside using </p><pre><code>conda env export --no-builds | grep -v \"^prefix: \" | grep -v \"en-core-web-lg\" | grep -v \"it-core-news-lg\" &gt; conda_environment.yml\n</code></pre> and push the modifications to the repo. (The spacy models end up in the final distribution but must be ignored otherwise cause errors) <p>Then to synchronize the packages change in the server pull updates from the repo and on the server terminal update dependencies on the server:</p> <p>The guide is here</p> <ul> <li>If you need to connect to EKEEL\u2019s mail box:</li> <li>Go to Gmail\u2019s login interface</li> <li>Email address : Specified in the <code>.env</code> file as <code>EMAIL_ACCOUNT</code></li> <li>Password : Specified in the <code>.env</code> file as <code>EMAIL_PASSWORD</code></li> </ul>"},{"location":"platforms/augmentator/install/#front-end-reactjs","title":"Front-end (React.js)","text":"<ul> <li>Make sure that Node.js v16.20.2 (and npm v8.19.4) are installed on the machine</li> </ul> <p>Suggest install nvm to manage different environments </p><pre><code>nvm install 16\nnvm use 16\n</code></pre> <ul> <li> <p>Go inside react-app folder </p><pre><code>cd src/react-app  \n</code></pre> </li> <li> <p>Install the dependencies </p><pre><code>npm install --legacy-peer-deps\n</code></pre> </li> </ul>"},{"location":"platforms/augmentator/install/#notes","title":"Notes","text":"<ul> <li>Pymongo issues </li> </ul> <p>if pymongo certificate is invalid:     1. Download https://letsencrypt.org/certs/lets-encrypt-r3.pem      2. rename file .pem to .cer     3. double click and install   </p> <p></p> <ul> <li> <p>To make graphs the same in 1st and this app we commented the call to the function for removing transitivity.</p> </li> <li> <p>to reactivate transitivity go to line 452 of GraphKnowledge.js and decomment the line with the call.</p> </li> <li> <p>// this.removeTransitivity(this.state.graph)</p> </li> </ul> <p></p> <ul> <li>Change annotation to display/consider:</li> </ul> <p>To change annotations have a look on this part of the code:</p> <ul> <li> <p>data.py (get_concept_instants, get_concept_vocabulary, get_concept_map, get_concept_list)</p> </li> <li> <p>handle_data.py (get_definitions_fragments)</p> </li> <li> <p>main.py (get_fragments, class Graphs, get_graph, graph)</p> </li> </ul> <p>Atm on each of thoose functions the email or annotatorId fields/var are used as a filter to the DB.   Change user informations to update the filter and obtain other annotations as result.   This can be done also with burst or gold annotations.</p> <p></p> <ul> <li>Videos folder</li> </ul> <p>the videos folder inside main.py (line ~28) has to be arranged basing on the server folder structure.</p> <p></p><pre><code>  app.config[\"CLIENT_IMAGES\"] = \n  \"/var/www/edurell/EVA_apps/EKEELVideoAnnotation/static/videos\"\n</code></pre> <ul> <li> <p>Email </p> <p>After some months the email sender could stop working and you can find errors on register or forgot password:</p> <ul> <li> <p>Login to the google account with this app credentials  (you can find those credentials on file main.py line ~43) </p> </li> <li> <p>go to security settings -&gt; allow less secure app</p> </li> <li> <p>(More info -&gt; https://support.google.com/accounts/answer/6010255?hl=en)</p> </li> </ul> <p>If still not working:</p> <ul> <li> <p>after log in with the google account open this link:   https://accounts.google.com/DisplayUnlockCaptcha</p> </li> <li> <p>(More info -&gt; https://stackoverflow.com/questions/16512592/login-credentials-not-working-with-gmail-smtp)</p> </li> </ul> </li> </ul>"},{"location":"platforms/augmentator/install/#run-the-application","title":"Run the application","text":"<ul> <li> <p>Open 2 terminals</p> </li> <li> <p>1st one : go to flask-server folder and run main   </p><pre><code>cd src/flask-server\npython main.py  \n</code></pre> </li> <li> <p>2nd one : go to react-app folder and run the start script   </p><pre><code>cd src/react-app\nnpm start\n</code></pre> </li> </ul> <p>The app should start automatically in the default browser at this point.. (However the url to type in the browser is the following: http://localhost:3000/)</p>"},{"location":"platforms/augmentator/install/#deploy-and-run-on-server","title":"Deploy and run on server","text":"<p>Follow this guide: https://drive.google.com/file/d/1hta5qeYVr-2U9mcQdjT0-a_NacvhYUPC/view?usp=sharing</p>"},{"location":"platforms/augmentator/install/#credits","title":"Credits:","text":"<ul> <li>Thomas Neveux</li> <li>Julie Massari</li> </ul>"},{"location":"prerequisites/conda/","title":"Prerequisites","text":""},{"location":"prerequisites/conda/#prerequisites","title":"Prerequisites","text":""},{"location":"prerequisites/conda/#mandatory-files","title":"Mandatory files","text":"<p>You must put a <code>secrets.env</code> file when installed locally and must be moved inside folder under <code>\\EVA_apps\\sharedSecrets</code>, ask the project manager to access the file</p>"},{"location":"prerequisites/conda/#miniconda-installation-guide-for-linux","title":"Miniconda Installation Guide for Linux","text":""},{"location":"prerequisites/conda/#download-miniconda-installer","title":"Download Miniconda Installer","text":"<pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n</code></pre>"},{"location":"prerequisites/conda/#run-the-installer","title":"Run the Installer","text":"<pre><code>bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre>"},{"location":"prerequisites/conda/#installation-prompts","title":"Installation Prompts","text":"<ol> <li>Press <code>Enter</code> to view license</li> <li>Type <code>yes</code> to accept the license</li> <li>Press <code>Enter</code> to confirm installation location</li> <li>Type <code>yes</code> to initialize Miniconda3</li> </ol>"},{"location":"prerequisites/conda/#activate-installation","title":"Activate Installation","text":"<pre><code>source ~/.bashrc\n</code></pre>"},{"location":"prerequisites/conda/#verify-installation","title":"Verify Installation","text":"<pre><code>conda --version\n</code></pre>"},{"location":"prerequisites/conda/#clean-up-installer","title":"Clean Up Installer","text":"<pre><code>rm Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Note: Restart your terminal or run <code>source ~/.bashrc</code> after installation.</p>"},{"location":"prerequisites/linux-services/","title":"Prerequisites","text":""},{"location":"prerequisites/linux-services/#prerequisites","title":"Prerequisites","text":""},{"location":"prerequisites/linux-services/#commands-for-systemctl","title":"Commands for systemctl","text":"<p>These commands should be used for debugging or for big changes in the server structure.</p> <p>3 important modules of systemctl (use as <code>&lt;name&gt;</code> in commands): - <code>ekeel</code> (Video Annotation App) - <code>ekeel-wp3</code> (Video Augmentation App) - <code>nginx</code> (Global connection for the server)</p> <pre><code># Enable systemctl\nsudo systemctl enable &lt;name&gt;\n\n# View info about status\nsudo systemctl status &lt;name&gt;\n\n# Start the app\nsudo systemctl start &lt;name&gt;\n\n# Stop the app\nsudo systemctl stop &lt;name&gt;\n\n# Check app logs\nsudo journalctl -u &lt;name&gt;\n</code></pre>"},{"location":"prerequisites/linux-services/#open-router-ports","title":"Open router ports","text":"<p>Port 5000 must be opened to allow external access to the Flask server:</p> <ol> <li>Router Configuration</li> <li>Log into your router's admin interface</li> <li>Navigate to Port Forwarding/NAT settings</li> <li> <p>Add a new port forwarding rule:</p> <ul> <li>External Port: 5000</li> <li>Internal Port: 5000</li> <li>Protocol: TCP</li> <li>Internal IP: Your server's local IP address</li> </ul> </li> <li> <p>Firewall Configuration</p> </li> </ol> <pre><code>sudo ufw status | grep 5000\nsudo ufw allow 5000/tcp\nsudo netstat -tuln | grep 5000\n</code></pre>"},{"location":"prerequisites/mkdocs/","title":"Mkdocs","text":""},{"location":"prerequisites/mkdocs/#mkdocs","title":"Mkdocs","text":"<p>This project is documented with mkdocs</p> <p>Its update is fully automated with every code modification, as long as classes and functions are documented in the numpydoc format.</p> <p>To modify and view the live-updated content locally, install the packages from the <code>requirements.txt</code> file located in the <code>mkdocs</code> directory in a new environment:</p> <pre><code>pip install -r mkdocs/requirements.txt\nmkdocs serve\n</code></pre> <p>Warning</p> <p>Use a different environment from <code>ekeel_anno_env</code> and <code>ekeel_aug_env</code> (the two environments of the project) to avoid mixing packages and create possible conflicts</p>"},{"location":"remote/pull/","title":"Codebase Update","text":""},{"location":"remote/pull/#pulling-updates-from-the-github-repo","title":"Pulling updates from the github repo","text":""},{"location":"remote/pull/#connect-to-the-server","title":"Connect to the Server","text":"<pre><code>ssh torre@130.251.47.107\n# passw: &lt; ask to project administrator &gt;\n</code></pre>"},{"location":"remote/pull/#update-ekeel-app-files-from-the-repository","title":"Update Ekeel App Files from the Repository","text":"<p>Go inside app folder (/var/www/ekeel) </p><pre><code>cd /var/www/ekeel\n# Pull new versions of files from repository\nsudo git pull\n# username for github: Mirwe\n# password: &lt; ask to project administrator &gt;\n</code></pre> Note: If github credentials are not valid or you want to use your own credentials..."},{"location":"remote/pull/#bug-provided-github-credentials-are-not-valid","title":"Bug: Provided github credentials are not valid","text":"<p>You can use your own github credentials: </p><pre><code>$ git clone https://github.com/...\nUsername: your_username\nPassword: your_token # (how to create a personal github access token)\n</code></pre> A guide on how to do it is here"},{"location":"remote/pull/#in-case-of-reinstall","title":"In case of Reinstall","text":"<p>Go in static folder, then create folder videos and give permissions</p> <pre><code>cd /var/www/ekeel/EVA_apps/EKEELVideoAnnotation/static\n# create the folder if you don't have it already\nsudo mkdir videos\nsudo chmod 777 ./videos\n</code></pre>"}]}